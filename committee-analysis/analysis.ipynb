{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "154efe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from datetime import timedelta\n",
    "from ripser import ripser\n",
    "from persim import plot_diagrams\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_meeting_df(path):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if \"Meeting Date\" not in df.columns:\n",
    "            print(f\"Warning: 'Meeting Date' column not found in {path}. Returning raw DataFrame.\")\n",
    "            return df\n",
    "        df[\"meeting_date\"] = pd.to_datetime(df[\"Meeting Date\"], errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "        df.dropna(subset=[\"meeting_date\"], inplace=True)\n",
    "        df = df.drop_duplicates(subset=[\"Committee\", \"Meeting Title\", \"meeting_date\"])\\\n",
    "               .reset_index(drop=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Meeting file not found at {path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing meeting file {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def load_stock_transactions_df(path, years_to_include):\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "        if \"transaction_date\" not in df.columns or \"year\" not in df.columns:\n",
    "            print(f\"Warning: 'transaction_date' or 'year' column not found in {path}. Returning raw DataFrame.\")\n",
    "            return df\n",
    "            \n",
    "        df[\"transaction_date\"] = pd.to_datetime(df[\"transaction_date\"], errors='coerce').dt.strftime(\"%Y-%m-%d\")\n",
    "        df.dropna(subset=[\"transaction_date\"], inplace=True) \n",
    "\n",
    "        df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "        df.dropna(subset=['year'], inplace=True)\n",
    "        df['year'] = df['year'].astype(int)\n",
    "\n",
    "        df = df[df[\"year\"].isin(years_to_include)]\n",
    "        if \"sector\" in df.columns:\n",
    "            df[\"sector\"] = df[\"sector\"].fillna(\"Unspecified_Sector\")\n",
    "        else:\n",
    "            print(f\"Warning: 'sector' column not found in {path}. Sectors will not be processed for this DataFrame.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Stock transaction file not found at {path}\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing stock transaction file {path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    return df\n",
    "\n",
    "def filter_stock_transactions_by_members(df_stocks, member_ids_set): \n",
    "    if \"member_id\" not in df_stocks.columns:\n",
    "        print(\"Warning: 'member_id' column not found in stock DataFrame for filtering.\")\n",
    "        return pd.DataFrame()\n",
    "    # Ensure member_id in df_stocks is string for consistent comparison\n",
    "    df_stocks['member_id'] = df_stocks['member_id'].astype(str)\n",
    "    df_new = df_stocks[df_stocks[\"member_id\"].isin(member_ids_set)]\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4415657c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  transaction_date ticker                                  asset_description  \\\n",
      "0       2021-09-27     BP                                             BP plc   \n",
      "1       2021-09-13    XOM                            Exxon Mobil Corporation   \n",
      "2       2021-09-10   ILPT  Industrial Logistics Properties Trust - Common...   \n",
      "3       2021-09-28     PM                   Phillip Morris International Inc   \n",
      "4       2021-09-17    BLK                                      BlackRock Inc   \n",
      "\n",
      "           type  amount state  \\\n",
      "0      purchase    8000    NC   \n",
      "1      purchase    8000    NC   \n",
      "2      purchase   35000    NC   \n",
      "3      purchase   35000    NC   \n",
      "4  sale_partial    8000    CA   \n",
      "\n",
      "                                            ptr_link  \\\n",
      "0  https://disclosures-clerk.house.gov/public_dis...   \n",
      "1  https://disclosures-clerk.house.gov/public_dis...   \n",
      "2  https://disclosures-clerk.house.gov/public_dis...   \n",
      "3  https://disclosures-clerk.house.gov/public_dis...   \n",
      "4  https://disclosures-clerk.house.gov/public_dis...   \n",
      "\n",
      "                             industry                 sector party chamber  \\\n",
      "0            Integrated oil Companies                 Energy     R   House   \n",
      "1            Integrated oil Companies                 Energy     R   House   \n",
      "2       Real Estate Investment Trusts            Real Estate     R   House   \n",
      "3               Farming/Seeds/Milling  Consumer Non-Durables     R   House   \n",
      "4  Investment Bankers/Brokers/Service                Finance     D   House   \n",
      "\n",
      "              member  year  last_name member_id  \n",
      "0      Virginia Foxx  2021       foxx   F000450  \n",
      "1      Virginia Foxx  2021       foxx   F000450  \n",
      "2      Virginia Foxx  2021       foxx   F000450  \n",
      "3      Virginia Foxx  2021       foxx   F000450  \n",
      "4  Alan S. Lowenthal  2021  lowenthal   L000579  \n",
      "Unique sectors in loaded stock data: ['Energy' 'Real Estate' 'Consumer Non-Durables' 'Finance' 'Technology'\n",
      " 'Public Utilities' 'Consumer Staples' 'Unspecified_Sector' 'Health Care'\n",
      " 'Utilities' 'Consumer Discretionary' 'Consumer Services'\n",
      " 'Basic Materials' 'Industrials' 'Capital Goods' 'Consumer Durables'\n",
      " 'Miscellaneous' 'Telecommunications' 'Basic Industries' 'Transportation']\n",
      "\n",
      "Loading committee meeting data...\n",
      "\n",
      "Total committee meetings loaded: 2764\n",
      "Committees found in COMMITTEE_MEETINGS_DF: ['House Appropriations' 'House Judiciary, House Appropriations'\n",
      " 'House Energy and Commerce' 'House Financial Services'\n",
      " 'House Oversight and Reform'\n",
      " 'House Oversight and Reform, House Homeland Security'\n",
      " 'House Ways and Means' 'Senate Appropriations'\n",
      " 'Senate Foreign Relations, Senate Appropriations'\n",
      " 'Senate Banking, Housing, and Urban Affairs' 'Senate Finance'\n",
      " 'Senate Health, Education, Labor, and Pensions']\n"
     ]
    }
   ],
   "source": [
    "# --- Configuration Constants ---\n",
    "DATA_PATH = '../data' # Assuming this path is correct relative to your notebook\n",
    "YEARS_TO_ANALYZE = [2019, 2020, 2021, 2022] # Renamed for clarity\n",
    "# DAYS_BEFORE = 7 # Moved to relevant cell (bipartite meeting day analysis)\n",
    "# DAYS_AFTER = 3  # Moved to relevant cell\n",
    "# TOP_K = 10      # This seems specific to a particular analysis, not global\n",
    "\n",
    "# --- Load Stock Transactions ---\n",
    "\n",
    "CONGRESS_PERIOD_MAP = {\"116\" : [2019, 2020], \"117\" : [2021, 2022]}\n",
    "\n",
    "STOCK_TRANSACTIONS_DF = load_stock_transactions_df(f'{DATA_PATH}/cleaned/2014-2023/stocks.csv', YEARS_TO_ANALYZE)\n",
    "if STOCK_TRANSACTIONS_DF.empty:\n",
    "    print(\"CRITICAL: STOCK_TRANSACTIONS_DF is empty after loading. Halting execution.\")\n",
    "    exit()\n",
    "else:\n",
    "    print(STOCK_TRANSACTIONS_DF.head())\n",
    "\n",
    "if \"sector\" in STOCK_TRANSACTIONS_DF.columns:\n",
    "    print(\"Unique sectors in loaded stock data:\", STOCK_TRANSACTIONS_DF[\"sector\"].unique())\n",
    "else:\n",
    "    print(\"Warning: 'sector' column not found in STOCK_TRANSACTIONS_DF after loading.\")\n",
    "\n",
    "\n",
    "committee_meeting_files = {\n",
    "    \"House Appropriations\": \"house_appropriations.csv\",\n",
    "    \"House Energy and Commerce\": \"house_energy_and_commerce.csv\",\n",
    "    \"House Financial Services\": \"house_financial_services.csv\",\n",
    "    \"House Oversight and Reform\": \"house_oversight_and_reform.csv\",\n",
    "    \"House Ways and Means\": \"house_ways_and_means.csv\",\n",
    "    \"Senate Appropriations\": \"senate_appropriations.csv\",\n",
    "    \"Senate Banking, Housing, and Urban Affairs\": \"senate_banking.csv\", # Assuming key in COMMITTEE_MEMBERSHIP_MAP matches this\n",
    "    \"Senate Finance\": \"senate_finance.csv\",\n",
    "    \"Senate Health, Education, Labor, and Pensions\": \"senate_help.csv\" # Assuming key in COMMITTEE_MEMBERSHIP_MAP matches this\n",
    "}\n",
    "\n",
    "committee_meeting_dfs_list = []\n",
    "print(\"\\nLoading committee meeting data...\")\n",
    "for committee_key_name, filename in committee_meeting_files.items():\n",
    "    df_meeting = load_meeting_df(f'{DATA_PATH}/committees/meetings/2019-2022/downloaded/{filename}')\n",
    "    if not df_meeting.empty:\n",
    "        # Ensure the 'Committee' column in the meeting data matches the keys used elsewhere (e.g., COMMITTEE_MEMBERSHIP_MAP)\n",
    "        # If 'Committee' column doesn't exist or is inconsistent, this needs to be handled.\n",
    "        # For now, we assume the loaded CSV already has a 'Committee' column or it's added by load_meeting_df\n",
    "        # If load_meeting_df doesn't add it, we might need to assign it here:\n",
    "        if \"Committee\" not in df_meeting.columns and not df_meeting.empty:\n",
    "             df_meeting[\"Committee\"] = committee_key_name # Assign committee name if missing\n",
    "        committee_meeting_dfs_list.append(df_meeting)\n",
    "    else:\n",
    "        print(f\"Warning: No data loaded for {committee_key_name} meetings.\")\n",
    "\n",
    "if not committee_meeting_dfs_list:\n",
    "    print(\"CRITICAL: No committee meeting data loaded. Some analyses may fail.\")\n",
    "    COMMITTEE_MEETINGS_DF = pd.DataFrame() # Create empty DF\n",
    "else:\n",
    "    COMMITTEE_MEETINGS_DF = pd.concat(committee_meeting_dfs_list).reset_index(drop=True)\n",
    "    print(f\"\\nTotal committee meetings loaded: {len(COMMITTEE_MEETINGS_DF)}\")\n",
    "    if \"Committee\" in COMMITTEE_MEETINGS_DF.columns:\n",
    "        print(\"Committees found in COMMITTEE_MEETINGS_DF:\", COMMITTEE_MEETINGS_DF[\"Committee\"].unique())\n",
    "\n",
    "\n",
    "COMMITTEE_MEMBERSHIP_MAP = {\n",
    "    'House Energy and Commerce': {\n",
    "        '116': {\n",
    "            'Darren Soto', 'O000171', 'Jan Schakowsky', 'Tim Walberg', 'H. Morgan Griffith', 'B001284', 'Michael Doyle', 'Dave Loebsack', 'B001275', 'D000624', 'M001180', 'Nanette Barragán', 'C001066', 'P000034', 'Peter G. Olson', 'Bill Johnson', 'Eliot Engel', 'M001200', 'Jerry McNerney', 'Cathy McMorris Rodgers', 'Jeff Duncan', 'Greg Walden', 'Markwayne Mullin', 'P000608', 'Ben Ray Luján', 'U000031', 'Yvette D. Clarke', 'Robin Kelly', 'B001257', 'John Shimkus', 'B001248', 'Richard Hudson', 'Diana DeGette', 'Tony Cárdenas', 'Lisa Blunt Rochester', 'Paul Tonko', 'Steve Scalise', 'Brett Guthrie', 'Bobby Rush', 'Anna Eshoo', 'F000461', 'Annie Kuster', 'K000378', 'L000576', 'S001180', 'John Sarbanes', 'Raul Ruiz', 'G000584', 'G.K. Butterfield', 'Joseph Kennedy III', 'L000566', 'W000800', 'M001163', 'C001103', 'Marc Veasey'\n",
    "                }, \n",
    "        '117': {\n",
    "            'Darren Soto', 'O000171', 'Jan Schakowsky', 'H. Morgan Griffith', 'Tim Walberg', 'Michael Doyle', 'B001275', 'D000624', 'S001216', 'Nanette Barragán', 'M001180', 'C001066', 'P000034', 'D000628', 'C001114', 'Bill Johnson', 'C001120', 'M001200', 'Jerry McNerney', 'Kathleen Rice', 'Lizzie Pannill Fletcher', 'Cathy McMorris Rodgers', 'Jeff Duncan', 'Markwayne Mullin', 'P000608', 'Yvette D. Clarke', 'U000031', 'Robin Kelly', 'Angie Craig', 'T000482', 'Greg Pence', 'B001257', 'Gary Palmer', 'B001248', 'Richard Hudson', 'Diana DeGette', 'Tony Cárdenas', 'Lisa Blunt Rochester', 'Paul Tonko', 'Steve Scalise', 'Brett Guthrie', 'Kelly Armstrong', 'Bobby Rush', 'Anna Eshoo', 'John Joyce', 'Annie Kuster', 'K000378', 'Debbie Lesko', 'L000576', 'S001180', 'John Sarbanes', 'Raul Ruiz', 'G.K. Butterfield', 'L000566', 'W000800', 'M001163', 'C001103', 'Marc Veasey'\n",
    "                }\n",
    "            }, \n",
    "    'House Financial Services': {\n",
    "        '116': {\n",
    "            'Denny Heck', 'Bill Huizenga', 'Michael F.Q. San Nicolas', 'Jennifer Wexton', 'Sean Casten', 'Katie Porter', 'Madeleine Dean', 'P000616', 'A000378', 'G000583', 'Emanuel Cleaver', 'P000593', 'Ben McAdams', 'John Rose', 'G000588', 'H001072', 'Barry Loudermilk', 'Gregory W. Meeks', 'W000812', 'Alfred Lawson', 'Joyce Beatty', 'Warren Davidson', 'Alexandria Ocasio-Cortez', 'Juan Vargas', 'Alexander Mooney', 'Scott Tipton', 'Tom Emmer', 'Maxine Waters', 'Bill Foster', 'Ted Budd', 'Lee Zeldin', 'Rashida Tlaib', 'David Scott', 'Tulsi Gabbard', 'Andy Barr', 'K000392', 'Bryan Steil', 'Bill Posey', 'Lance Gooden', 'Nydia Velazquez', 'Sean Duffy', 'M001156', 'Steve Stivers', 'Carolyn B. Maloney', 'Ayanna Pressley', 'H001074', 'Brad Sherman', 'T000480', 'Frank Lucas', 'Peter King', 'Blaine Luetkemeyer', 'Jim Himes', 'Alma Adams', 'Denver Lee Riggleman III', 'Roger Williams', 'C001049', 'Al Green', 'Jesus Garcia', 'Sylvia Garcia', 'L000562', 'Vicente Gonzalez Jr.'\n",
    "            }, \n",
    "        '117': {\n",
    "            'Bill Huizenga', 'Michael F.Q. San Nicolas', 'Sean Casten', 'Madeleine Dean', 'A000378', 'G000583', 'Emanuel Cleaver', 'P000593', 'John Rose', 'G000588', 'H001072', 'Barry Loudermilk', 'Gregory W. Meeks', 'W000812', 'Alfred Lawson', 'Joyce Beatty', 'Warren Davidson', 'Alexandria Ocasio-Cortez', 'Juan Vargas', 'Jake Auchincloss', 'Alexander Mooney', 'Tom Emmer', 'Maxine Waters', 'Bill Foster', 'Ted Budd', 'Lee Zeldin', 'Nikema Williams', 'Rashida Tlaib', 'David Scott', 'Andy Barr', 'K000392', 'Ritchie Torres', 'Bryan Steil', 'Bill Posey', 'Lance Gooden', 'Nydia Velazquez', 'M001156', 'Steve Stivers', 'Carolyn B. Maloney', 'Ayanna Pressley', 'H001074', 'Brad Sherman', 'T000480', 'Frank Lucas', 'T000479', 'Blaine Luetkemeyer', 'Jim Himes', 'Alma Adams', 'Roger Williams', 'Al Green', 'Jesus Garcia', 'Sylvia Garcia', 'L000562', 'Vicente Gonzalez Jr.'\n",
    "                }\n",
    "            }, \n",
    "    'Senate Banking, Housing, and Urban Affairs': {\n",
    "        '116': {\n",
    "            'Catherine Cortez Masto', 'Elizabeth Warren', 'Mike Rounds', 'Jon Tester', 'Martha McSally', 'S384', 'Kevin Cramer', 'Richard Shelby', 'S389', 'Bob Menendez', 'S390', 'Brian E. Schatz', 'Ben Sasse', 'S327', 'Sherrod Brown', 'S351', 'S394', 'S347', 'Mike Crapo', 'Kyrsten Sinema', 'Tom Cotton', 'S259', 'Tim Scott'\n",
    "                    }, \n",
    "        '117': {\n",
    "            'Catherine Cortez Masto', 'Elizabeth Warren', 'Mike Rounds', 'Jon Ossoff', 'Jon Tester', 'S384', 'Kevin Cramer', 'Richard Shelby', 'S389', 'Bob Menendez', 'S390', 'Raphael Warnock', 'S327', 'Sherrod Brown', 'S351', 'S394', 'S347', 'S407', 'Kyrsten Sinema', 'Steve Daines', 'S410', 'Mike Crapo', 'S259', 'Tim Scott'}}, \n",
    "    'Senate Health, Education, Labor, and Pensions': {\n",
    "        '116': {\n",
    "            'Maggie Hassan', 'S252', 'Rand Paul', 'Mike Braun', 'Elizabeth Warren', 'Johnny Isakson', 'Lisa Murkowski', 'S373', 'S229', 'Mitt Romney', 'Bernie Sanders', 'S402', 'S394', 'Bob Casey Jr.', 'Richard Burr', 'Tammy Baldwin', 'S362', 'Christopher S. Murphy', 'Tim Scott'\n",
    "            }, \n",
    "        '117': {\n",
    "            'Maggie Hassan', 'S252', 'Rand Paul', 'Mike Braun', 'Lisa Murkowski', 'S373', 'S412', 'S229', 'Mitt Romney', 'Bernie Sanders', 'Ben Ray Luján', 'S402', 'S408', 'S394', 'S347', 'Bob Casey Jr.', 'Richard Burr', 'Tammy Baldwin', 'S362', 'Christopher S. Murphy', 'M001198', 'Tim Scott'\n",
    "                }\n",
    "            }, \n",
    "    'House Appropriations': {\n",
    "        '116': {\n",
    "            'Martha Roby', 'Lucille Roybal-Allard', 'Mark Pocan', 'Kay Granger', 'P000523', 'Mario Diaz-Balart', 'Steve Womack', 'A000055', 'Norma Torres', 'Pete Aguilar', 'Betty McCollum', 'John Moolenaar', 'Tim Ryan', 'Mark Amodei', 'Chellie Pingree', 'Barbara Lee', 'Bonnie Watson Coleman', 'M001188', 'Sanford Bishop Jr.', 'B001286', 'Marcy Kaptur', 'S001148', 'W000797', 'Jeffrey Fortenberry', 'Matt Cartwright', 'C001055', 'Nita Lowey', 'Steven Palazzo', 'Brenda Lawrence', 'Charlie Crist', 'F000459', 'J000295', 'Ken Calvert', 'José Serrano', 'John Carter', 'V000108', 'William Hurd', 'Mike Quigley', 'Chris Stewart', 'N000189', 'Henry Cuellar', 'C001101', 'Jaime Herrera Beutler', 'Derek Kilmer', 'C001053', 'Dutch Ruppersberger', 'Andrew Harris', 'F000462', 'R000609', 'Rosa L. DeLauro', 'Ann Kirkpatrick', 'R000395'\n",
    "            }, \n",
    "        '117': {\n",
    "            'Lucille Roybal-Allard', 'Mark Pocan', 'Jennifer Wexton', 'Kay Granger', 'P000523', 'Mario Diaz-Balart', 'Steve Womack', 'A000055', 'Norma Torres', 'Benjamin Lee Cline', 'Pete Aguilar', 'Betty McCollum', 'John Moolenaar', 'Tim Ryan', 'G000588', 'Mark Amodei', 'Chellie Pingree', 'Barbara Lee', 'Bonnie Watson Coleman', 'Ashley Hinson', 'Guy Reschenthaler', 'Marcy Kaptur', 'B001286', 'Sanford Bishop Jr.', 'M001188', 'S001148', 'T000483', 'W000797', 'Jeffrey Fortenberry', 'Matt Cartwright', 'C001055', 'Steven Palazzo', 'Lauren Underwood', 'Adriano Espaillat', 'Brenda Lawrence', 'Charlie Crist', 'F000459', 'J000295', 'Ken Calvert', 'G000061', 'John Carter', 'Mike Quigley', 'Chris Stewart', 'N000189', 'Henry Cuellar', 'C001101', 'Jaime Herrera Beutler', 'L000590', 'Derek Kilmer', 'Josh Harder', 'C001053', 'Dutch Ruppersberger', 'Andrew Harris', 'F000462', 'R000609', 'Rosa L. DeLauro', 'Ann Kirkpatrick', 'R000395', 'David G. Valadao'}\n",
    "            }, \n",
    "    'House Oversight and Reform': \n",
    "            {\n",
    "        '116': {\n",
    "            'F000450', 'Raja Krishnamoorthi', 'Stacey Plaskett', 'Ralph Norman', 'C001108', 'Katie Hill', 'Jimmy Gomez', 'Glenn Grothman', 'S001214', 'Alexandria Ocasio-Cortez', 'Mark Meadows', 'M001205', 'W000797', 'Jody Hice', 'Kweisi Mfume', 'Rashida Tlaib', 'K000389', 'Brenda Lawrence', 'Eleanor Holmes Norton', 'Robin Kelly', 'Jackie Speier', 'G000563', 'Jim Jordan', 'Carolyn B. Maloney', 'Ayanna Pressley', 'Paul Gosar', 'Kelly Armstrong', 'Chip Roy', 'R000616', 'M001184', 'R000606', 'Jim Cooper', 'Michael Cloud', 'John Sarbanes', 'Clay Higgins', 'C001049', 'C001078', 'Mark DeSaulnier', 'L000562', 'W000800', 'G000590'\n",
    "            }, \n",
    "        '117': {\n",
    "            'F000450', 'Raja Krishnamoorthi', 'Ralph Norman', 'Katie Porter', 'C001108', 'Yvette Herrell', 'Cori Bush', 'Jimmy Gomez', 'Glenn Grothman', 'Alexandria Ocasio-Cortez', 'W000797', 'Hank Johnson', 'Nancy Mace', 'Jody Hice', 'Kweisi Mfume', 'Rashida Tlaib', 'K000389', 'Fred Keller', 'Danny K. Davis', 'Eleanor Holmes Norton', 'Brenda Lawrence', 'Andrew Clyde', 'Robin Kelly', 'Jackie Speier', 'G000563', 'Jim Jordan', 'Carolyn B. Maloney', 'Ayanna Pressley', 'Jacob LaTurner', 'Scott Franklin', 'S000250', 'R000606', 'Jim Cooper', 'Michael Cloud', 'Byron Donalds', 'F000246', 'John Sarbanes', 'Clay Higgins', 'C001078', 'Mark DeSaulnier', 'L000562', 'W000800', 'Andy Biggs'}\n",
    "            }, \n",
    "    'House Ways and Means': \n",
    "            {\n",
    "        '116': {\n",
    "            'P000096', 'E000296', 'Jason Smith', 'Dan Kildee', 'Jodey Arrington', 'Mike Thompson', 'Adrian Smith', 'Brendan Boyle', 'Jimmy Gomez', 'Ronald James Kind', 'K000376', 'H001065', 'David Schweikert', 'R000597', 'Donald Sternoff Beyer Jr.', 'Danny K. Davis', 'Stephanie Murphy', 'Vern Buchanan', 'B000574', 'S001201', 'Devin Nunes', 'Richard Neal', 'E000298', 'C001080', 'Brad Wenstrup', 'Jimmy Panetta', 'L000557', 'Linda Sánchez', 'S001190', 'H001038', 'Tom Reed', 'Steven Horsford', 'Darin LaHood', 'Terri Sewell', 'M001158', 'D000399', 'Kevin Brady', 'Drew Ferguson', 'D000617', 'Jackie Walorski', 'Gwen Moore'}, \n",
    "        '117': {\n",
    "            'P000096', 'E000296', 'Jason Smith', 'Stacey Plaskett', 'Dan Kildee', 'Jodey Arrington', 'Mike Thompson', 'Adrian Smith', 'Brendan Boyle', 'Jimmy Gomez', 'Ronald James Kind', 'K000376', 'M001205', 'David Schweikert', 'R000597', 'Donald Sternoff Beyer Jr.', 'Danny K. Davis', 'H001082', 'Stephanie Murphy', 'Vern Buchanan', 'B000574', 'S001201', 'Brad Wenstrup', 'Richard Neal', 'E000298', 'C001080', 'S001199', 'Jimmy Panetta', 'L000557', 'Linda Sánchez', 'S001190', 'H001038', 'Tom Reed', 'Steven Horsford', 'Darin LaHood', 'Terri Sewell', 'D000399', 'Kevin Brady', 'Drew Ferguson', 'D000617', 'Jackie Walorski', 'Gwen Moore'\n",
    "            }\n",
    "            }, \n",
    "    'Senate Appropriations': {\n",
    "        '116': {\n",
    "            'S337', 'S252', 'S342', 'Patrick Leahy', 'Jeanne Shaheen', 'Dianne Feinstein', 'Lisa Murkowski', 'S343', 'Joe Manchin III', 'Jon Tester', 'Dick Durbin', 'Richard Shelby', 'S389', 'S229', 'S372', 'Jeff Merkley', 'Brian E. Schatz', 'S390', 'Marco Rubio', 'S347', 'S293', 'Tammy Baldwin', 'Christopher S. Murphy', 'Steve Daines', 'James Lankford', 'S174', 'Cindy Hyde-Smith', 'S344', 'S259'\n",
    "            }, \n",
    "            '117': {'S337', 'S252', 'Mike Braun', 'S342', 'Patrick Leahy', 'Jeanne Shaheen', 'Dianne Feinstein', 'Lisa Murkowski', 'S343', 'Joe Manchin III', 'Jon Tester', 'Dick Durbin', 'Richard Shelby', 'S389', 'S229', 'S372', 'Jeff Merkley', 'Brian E. Schatz', 'S390', 'Marco Rubio', 'S347', 'Martin Heinrich', 'S293', 'Tammy Baldwin', 'S407', 'Christopher S. Murphy', 'S174', 'Cindy Hyde-Smith', 'S344', 'S259'}\n",
    "            }, \n",
    "    'Senate Finance': {\n",
    "        '116': {'Maggie Hassan', 'Catherine Cortez Masto', 'Johnny Isakson', 'Debbie Stabenow', 'S316', 'S303', 'Rob Portman', 'S373', 'Bob Menendez', 'S330', 'S287', 'S327', 'Sherrod Brown', 'S351', 'Chuck Grassley', 'S277', 'S247', 'Bob Casey Jr.', 'Todd C. Young', 'S275', 'Richard Burr', 'S308', 'Steve Daines', 'James Lankford', 'Mike Crapo', 'Tim Scott'}, '117': {'Maggie Hassan', 'Catherine Cortez Masto', 'Elizabeth Warren', 'Debbie Stabenow', 'S316', 'S303', 'Rob Portman', 'S373', 'John Barrasso', 'Bob Menendez', 'S330', 'S287', 'Ben Sasse', 'S327', 'Sherrod Brown', 'S351', 'Chuck Grassley', 'S277', 'S247', 'Bob Casey Jr.', 'Todd C. Young', 'S275', 'Richard Burr', 'S308', 'Steve Daines', 'James Lankford', 'Mike Crapo', 'Tim Scott'}}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "91d4074f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique sectors in mapping file: ['Unspecified' 'Energy' 'Consumer Services' 'Consumer Discretionary'\n",
      " 'Finance' 'Industrials' 'Basic Materials' 'Miscellaneous' 'Technology'\n",
      " 'Telecommunications' 'Health Care' 'Consumer Staples' 'Utilities'\n",
      " 'Real Estate' 'Transportation']\n",
      "Sector mapping based on 'missing_fill_sectors.csv' applied.\n",
      "\n",
      "Number of tickers still having 'Unspecified' or similar sector: 0\n",
      "No tickers remain with 'Unspecified' sector after mapping (or no tickers at all).\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Sector Mapping Update\n",
    "\n",
    "# Define the path to your mapping file\n",
    "MAPPING_FILE_PATH = \"./missing_fill_sectors.csv\" # Use a constant\n",
    "\n",
    "if not os.path.exists(MAPPING_FILE_PATH):\n",
    "    print(f\"Warning: Mapping file not found at {MAPPING_FILE_PATH}. Sector mapping will not be performed.\")\n",
    "else:\n",
    "    try:\n",
    "        new_mappings_df = pd.read_csv(MAPPING_FILE_PATH)\n",
    "        if \"ticker\" not in new_mappings_df.columns or \"sector\" not in new_mappings_df.columns:\n",
    "            print(f\"Warning: Mapping file {MAPPING_FILE_PATH} must contain 'ticker' and 'sector' columns. Sector mapping skipped.\")\n",
    "        else:\n",
    "            print(\"Unique sectors in mapping file:\", new_mappings_df[\"sector\"].unique())\n",
    "            \n",
    "            # Create a mapping dictionary: Ticker (uppercase) -> Sector\n",
    "            # Dropna from ticker and sector to avoid issues if they have NaNs\n",
    "            new_mappings_df.dropna(subset=['ticker', 'sector'], inplace=True)\n",
    "            mapping_dict = dict(zip(new_mappings_df['ticker'].astype(str).str.upper(), new_mappings_df['sector'].astype(str)))\n",
    "\n",
    "            # Ensure 'ticker' column exists and is string in STOCK_TRANSACTIONS_DF\n",
    "            if 'ticker' in STOCK_TRANSACTIONS_DF.columns:\n",
    "                STOCK_TRANSACTIONS_DF['ticker'] = STOCK_TRANSACTIONS_DF['ticker'].astype(str).str.upper()\n",
    "                \n",
    "                # Apply the mapping:\n",
    "                # For each ticker, if it's in mapping_dict, use the new sector. Otherwise, keep the old one.\n",
    "                STOCK_TRANSACTIONS_DF['sector'] = STOCK_TRANSACTIONS_DF['ticker'].map(mapping_dict).fillna(STOCK_TRANSACTIONS_DF['sector'])\n",
    "                print(\"Sector mapping based on 'missing_fill_sectors.csv' applied.\")\n",
    "            else:\n",
    "                print(\"Warning: 'ticker' column not found in STOCK_TRANSACTIONS_DF. Sector mapping skipped.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during sector mapping process: {e}\")\n",
    "\n",
    "# --- Analysis of Unspecified Tickers (after potential mapping) ---\n",
    "if 'ticker' in STOCK_TRANSACTIONS_DF.columns and 'sector' in STOCK_TRANSACTIONS_DF.columns:\n",
    "    ticker_counts = STOCK_TRANSACTIONS_DF['ticker'].value_counts().reset_index()\n",
    "    ticker_counts.columns = ['ticker', 'count']\n",
    "    \n",
    "    # Get the first sector associated with each ticker (should be consistent after mapping)\n",
    "    # Using .groupby().first() is safer than assuming drop_duplicates will give the correct one if there were inconsistencies.\n",
    "    ticker_sectors = STOCK_TRANSACTIONS_DF.groupby('ticker')['sector'].first().reset_index()\n",
    "    \n",
    "    ticker_info = pd.merge(ticker_counts, ticker_sectors, on='ticker', how='left')\n",
    "    \n",
    "    # Ensure 'sector' column in ticker_info is string before comparison\n",
    "    ticker_info['sector'] = ticker_info['sector'].astype(str)\n",
    "    unspecified_tickers = ticker_info[\n",
    "        (ticker_info['sector'] == 'Unspecified_Sector') | (ticker_info['sector'] == 'unspecified') | (ticker_info['sector'].str.lower() == 'nan')\n",
    "    ].sort_values(by='count', ascending=False)\n",
    "\n",
    "    print(f\"\\nNumber of tickers still having 'Unspecified' or similar sector: {len(unspecified_tickers)}\")\n",
    "    if not unspecified_tickers.empty:\n",
    "        print(\"Top 50 Unspecified tickers with counts (after attempting mapping):\")\n",
    "        print(unspecified_tickers[['ticker', 'count', 'sector']].head(50).to_string())\n",
    "    else:\n",
    "        print(\"No tickers remain with 'Unspecified' sector after mapping (or no tickers at all).\")\n",
    "else:\n",
    "    print(\"Skipping unspecified ticker analysis as 'ticker' or 'sector' column is missing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "02faeb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Bipartite for Committee: House Energy and Commerce ---\n",
      "[Saved] committee_bipartite_member_sector/house_energy_and_commerce_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: House Financial Services ---\n",
      "[Saved] committee_bipartite_member_sector/house_financial_services_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: Senate Banking, Housing, and Urban Affairs ---\n",
      "[Saved] committee_bipartite_member_sector/senate_banking_housing_and_urban_affairs_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: Senate Health, Education, Labor, and Pensions ---\n",
      "[Saved] committee_bipartite_member_sector/senate_health_education_labor_and_pensions_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: House Appropriations ---\n",
      "[Saved] committee_bipartite_member_sector/house_appropriations_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: House Oversight and Reform ---\n",
      "[Saved] committee_bipartite_member_sector/house_oversight_and_reform_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: House Ways and Means ---\n",
      "[Saved] committee_bipartite_member_sector/house_ways_and_means_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: Senate Appropriations ---\n",
      "[Saved] committee_bipartite_member_sector/senate_appropriations_bipartite_member_sector.png\n",
      "\n",
      "--- Analyzing Bipartite for Committee: Senate Finance ---\n",
      "[Saved] committee_bipartite_member_sector/senate_finance_bipartite_member_sector.png\n",
      "Saved JSON report for all committees: committee_bipartite_member_sector/all_committees_bipartite_stats.json\n",
      "[Saved] committee_bipartite_member_sector/all_committees_sector_breakdown_filtered.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 5: Committee-Level Bipartite Network Analysis (Member-Sector Per Committee)\n",
    "# This cell will be refactored using a function.\n",
    "\n",
    "# --- Configuration for this cell's analysis ---\n",
    "BIPARTITE_INCLUDE_BUYS = False  # Set to True to include \"buy\" transactions\n",
    "BIPARTITE_INCLUDE_SELLS = True   # Set to True to include \"sell\" transactions\n",
    "\n",
    "BIPARTITE_MIN_SECTOR_NODE_SIZE = 200\n",
    "BIPARTITE_MAX_SECTOR_NODE_SIZE = 2000\n",
    "BIPARTITE_MEMBER_NODE_SIZE = 600\n",
    "BIPARTITE_MAX_EDGE_WIDTH = 8\n",
    "\n",
    "BIPARTITE_OUTPUT_DIR = \"committee_bipartite_member_sector\" # Renamed for clarity\n",
    "os.makedirs(BIPARTITE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Main function for single committee bipartite analysis ---\n",
    "def analyze_and_plot_committee_bipartite(\n",
    "    committee_name,\n",
    "    committee_members_set, # Pass the actual set of members\n",
    "    all_stock_transactions_df,\n",
    "    output_directory\n",
    "    ):\n",
    "    \n",
    "    print(f\"\\n--- Analyzing Bipartite for Committee: {committee_name} ---\")\n",
    "\n",
    "    if not committee_members_set:\n",
    "        print(f\"No members defined for '{committee_name}'. Skipping.\")\n",
    "        return {} # Return empty stats\n",
    "\n",
    "    df_ct = filter_stock_transactions_by_members(all_stock_transactions_df, committee_members_set)\n",
    "    if df_ct.empty:\n",
    "        print(f\"No transactions found for members of '{committee_name}'. Skipping.\")\n",
    "        return {\"num_members\": len(committee_members_set), \"num_transactions\": 0}\n",
    "\n",
    "    df_ct = df_ct.copy() # Avoid SettingWithCopyWarning\n",
    "    df_ct[\"category\"] = df_ct[\"type\"].str.lower().map(\n",
    "        lambda t: \"buy\" if \"purchase\" in t\n",
    "        else (\"sell\" if \"sale\" in t else (\"exchange\" if \"exchange\" in t else \"other\"))\n",
    "    )\n",
    "\n",
    "    mask = pd.Series(True, index=df_ct.index)\n",
    "    if not BIPARTITE_INCLUDE_BUYS:  mask &= df_ct[\"category\"] != \"buy\"\n",
    "    if not BIPARTITE_INCLUDE_SELLS: mask &= df_ct[\"category\"] != \"sell\"\n",
    "    df_ct = df_ct[mask]\n",
    "\n",
    "    if df_ct.empty:\n",
    "        print(f\"No transactions after type filter for '{committee_name}'. Skipping.\")\n",
    "        return {\"num_members\": len(committee_members_set), \"num_transactions_after_filter\": 0}\n",
    "\n",
    "    present_member_ids = set(df_ct[\"member_id\"].unique())\n",
    "    # Use global id2name or create a local one if 'member' column exists\n",
    "    if 'member' in df_ct.columns:\n",
    "        member_name_map = df_ct[[\"member_id\", \"member\"]].drop_duplicates().set_index(\"member_id\")[\"member\"].to_dict()\n",
    "    else: # Fallback if 'member' column is missing\n",
    "        member_name_map = {mid: mid for mid in present_member_ids} # Use ID as name\n",
    "    \n",
    "    # Ensure all present_member_ids have an entry in member_name_map\n",
    "    for mid in present_member_ids:\n",
    "        member_name_map.setdefault(mid, mid)\n",
    "\n",
    "\n",
    "    sorted_members = sorted(list(present_member_ids), key=lambda mid: member_name_map.get(mid, mid)) # Use .get for safety\n",
    "    \n",
    "    sector_counts = df_ct[\"sector\"].value_counts().to_dict()\n",
    "    if not sector_counts:\n",
    "        print(f\"No sector activity for '{committee_name}'. Skipping graph.\")\n",
    "        return {\"num_members\": len(sorted_members), \"total_tx_count\": int(df_ct.shape[0]), \"total_tx_volume\": float(df_ct[\"amount\"].sum())}\n",
    "\n",
    "\n",
    "    member_sector_agg = (\n",
    "        df_ct.groupby([\"member_id\", \"sector\", \"category\"])\n",
    "        .agg(tx_count=pd.NamedAgg(column=\"amount\", aggfunc=\"count\"),\n",
    "             tx_volume=pd.NamedAgg(column=\"amount\", aggfunc=\"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    max_edge_amount = member_sector_agg[\"tx_volume\"].max() if not member_sector_agg.empty else 1.0\n",
    "    if max_edge_amount == 0: max_edge_amount = 1.0 # Avoid division by zero\n",
    "\n",
    "    unique_sectors = sorted(sector_counts.keys())\n",
    "    G = nx.Graph() # Use Graph for bipartite, MultiGraph allows parallel edges if needed for buy/sell separately\n",
    "    G.add_nodes_from(sorted_members, bipartite=0)\n",
    "    G.add_nodes_from(unique_sectors, bipartite=1)\n",
    "\n",
    "    for _, row in member_sector_agg.iterrows():\n",
    "        m, s, cat, vol, cnt = (\n",
    "            row[\"member_id\"], row[\"sector\"], row[\"category\"],\n",
    "            row[\"tx_volume\"], int(row[\"tx_count\"])\n",
    "        )\n",
    "        if m in sorted_members: # Ensure member is in the graph's member set\n",
    "            # For a simple bipartite, aggregate buy/sell weights if not distinguished by edge type\n",
    "            # If you need separate edges for buy/sell, use MultiGraph and distinct edge keys or attributes\n",
    "            if G.has_edge(m,s):\n",
    "                G[m][s]['tx_volume'] = G[m][s].get('tx_volume',0) + vol # Sum volumes if multiple categories link same m-s\n",
    "                G[m][s]['tx_count'] = G[m][s].get('tx_count',0) + cnt\n",
    "                G[m][s]['categories'] = G[m][s].get('categories', set()).union({cat})\n",
    "            else:\n",
    "                G.add_edge(m, s, category=cat, tx_count=cnt, tx_volume=vol, categories={cat})\n",
    "\n",
    "\n",
    "    # --- Stats Calculation (Simplified) ---\n",
    "    committee_stats = {\n",
    "        \"num_members\": len(sorted_members), \"num_sectors\": len(unique_sectors),\n",
    "        \"num_edges\": G.number_of_edges(), \"total_tx_count\": int(df_ct.shape[0]),\n",
    "        \"total_tx_volume\": float(df_ct[\"amount\"].sum()), \"nodes\": {}\n",
    "    }\n",
    "    # Add per-node stats if needed, similar to your original code but for brevity here.\n",
    "\n",
    "    # --- Plotting ---\n",
    "    if not G.nodes() or not G.edges():\n",
    "        print(f\"Graph for {committee_name} is empty or has no edges. Skipping plot.\")\n",
    "        return committee_stats\n",
    "\n",
    "    node_colors_plot, node_sizes_plot, labels_plot = [], [], {}\n",
    "    max_sector_count_plot = max(sector_counts.values()) if sector_counts else 1.0\n",
    "    if max_sector_count_plot == 0: max_sector_count_plot = 1.0\n",
    "\n",
    "    for mid in sorted_members:\n",
    "        if mid in G: # Check if node exists in graph G\n",
    "            node_colors_plot.append(\"#1f78b4\")\n",
    "            node_sizes_plot.append(BIPARTITE_MEMBER_NODE_SIZE)\n",
    "            labels_plot[mid] = member_name_map.get(mid, mid)\n",
    "\n",
    "    current_graph_member_nodes = [n for n,d in G.nodes(data=True) if d['bipartite']==0]\n",
    "    current_graph_sector_nodes = [n for n,d in G.nodes(data=True) if d['bipartite']==1]\n",
    "\n",
    "\n",
    "    for sec_node_idx, sec in enumerate(current_graph_sector_nodes):\n",
    "        if sec in G: # Check if node exists\n",
    "            node_colors_plot.insert(len(current_graph_member_nodes) + sec_node_idx, \"#ff7f0e\") # Insert at correct position\n",
    "            cnt = sector_counts.get(sec, 0)\n",
    "            size = (cnt / max_sector_count_plot) * (BIPARTITE_MAX_SECTOR_NODE_SIZE - BIPARTITE_MIN_SECTOR_NODE_SIZE) + BIPARTITE_MIN_SECTOR_NODE_SIZE\n",
    "            node_sizes_plot.insert(len(current_graph_member_nodes) + sec_node_idx, size)\n",
    "            labels_plot[sec] = sec\n",
    "    \n",
    "    # Reorder nodes for plotting to match colors/sizes\n",
    "    plot_nodes_ordered = current_graph_member_nodes + current_graph_sector_nodes\n",
    "\n",
    "\n",
    "    edge_list_plot, edge_colors_plot, edge_widths_plot = [], [], []\n",
    "    for u, v, data in G.edges(data=True):\n",
    "        vol = data.get(\"tx_volume\",0) # Aggregate volume for simplicity\n",
    "        color = \"gray\" # Default, or color by net buy/sell if desired\n",
    "        # Example: if 'categories' stored, could color by mixed/buy/sell\n",
    "        width = (abs(vol) / max_edge_amount if max_edge_amount > 0 else 0) * BIPARTITE_MAX_EDGE_WIDTH + 0.5\n",
    "        edge_list_plot.append((u, v))\n",
    "        edge_colors_plot.append(color)\n",
    "        edge_widths_plot.append(min(width, BIPARTITE_MAX_EDGE_WIDTH)) # Cap width\n",
    "\n",
    "    if not current_graph_member_nodes: # Check if the set for layout is empty\n",
    "        print(f\"No member nodes in graph for {committee_name} to use for bipartite layout. Skipping plot.\")\n",
    "        return committee_stats\n",
    "\n",
    "    pos_plot = nx.bipartite_layout(G, current_graph_member_nodes) # Use nodes present in G\n",
    "    \n",
    "    plt.figure(figsize=(max(12, len(current_graph_member_nodes)*0.3), max(10, len(current_graph_sector_nodes)*0.3) )) # Dynamic figsize\n",
    "    nx.draw_networkx_nodes(G, pos_plot, nodelist=plot_nodes_ordered, node_color=node_colors_plot, node_size=node_sizes_plot, alpha=0.9)\n",
    "    nx.draw_networkx_edges(G, pos_plot, edgelist=edge_list_plot, edge_color=edge_colors_plot, width=edge_widths_plot, alpha=0.6)\n",
    "    nx.draw_networkx_labels(G, pos_plot, labels=labels_plot, font_size=8)\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    title = f\"{committee_name} — Members ↔ Sectors\\n(Buys={'Y' if BIPARTITE_INCLUDE_BUYS else 'N'} | Sells={'Y' if BIPARTITE_INCLUDE_SELLS else 'N'})\"\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    safe_name_plot = committee_name.replace(\" \", \"_\").replace(\",\", \"\").lower()\n",
    "    plt.savefig(f\"{output_directory}/{safe_name_plot}_bipartite_member_sector.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {output_directory}/{safe_name_plot}_bipartite_member_sector.png\")\n",
    "    \n",
    "    return committee_stats\n",
    "\n",
    "# --- Loop through committees and generate bipartite graphs ---\n",
    "all_committee_stats_json = {}\n",
    "committee_sector_counts_for_bar_chart = {} # For the combined bar chart later\n",
    "\n",
    "id2name = {}  # Global mapping for member_id to name, will be populated if not already done\n",
    "\n",
    "# Populate id2name globally if not already done (or ensure it's passed)\n",
    "if 'member' in STOCK_TRANSACTIONS_DF.columns:\n",
    "     id2name_global_temp = STOCK_TRANSACTIONS_DF[[\"member_id\", \"member\"]].drop_duplicates(\"member_id\").set_index(\"member_id\")[\"member\"].to_dict()\n",
    "     # Update global id2name\n",
    "     id2name.update(id2name_global_temp)\n",
    "    \n",
    "\n",
    "\n",
    "for committee_name_loop, period_map_loop in COMMITTEE_MEMBERSHIP_MAP.items():\n",
    "    members_116_loop = period_map_loop.get(\"116\", set())\n",
    "    members_117_loop = period_map_loop.get(\"117\", set())\n",
    "    current_committee_members_set = members_116_loop | members_117_loop\n",
    "    \n",
    "    stats = analyze_and_plot_committee_bipartite(\n",
    "        committee_name_loop,\n",
    "        current_committee_members_set,\n",
    "        STOCK_TRANSACTIONS_DF, # Pass the main, globally filtered stock DF\n",
    "        BIPARTITE_OUTPUT_DIR\n",
    "    )\n",
    "    all_committee_stats_json[committee_name_loop] = stats\n",
    "    \n",
    "    # For combined bar chart data (using raw counts for now)\n",
    "    # This part needs to be aligned with how df_ct was processed inside analyze_and_plot_committee_bipartite\n",
    "    # We'll re-filter here for simplicity for the bar chart data source\n",
    "    temp_df_ct = filter_stock_transactions_by_members(STOCK_TRANSACTIONS_DF, current_committee_members_set)\n",
    "    if not temp_df_ct.empty:\n",
    "        temp_df_ct = temp_df_ct.copy()\n",
    "        temp_df_ct[\"category\"] = temp_df_ct[\"type\"].str.lower().map(\n",
    "            lambda t: \"buy\" if \"purchase\" in t else (\"sell\" if \"sale\" in t else \"other\"))\n",
    "        mask_bar = pd.Series(True, index=temp_df_ct.index)\n",
    "        if not BIPARTITE_INCLUDE_BUYS:  mask_bar &= temp_df_ct[\"category\"] != \"buy\"\n",
    "        if not BIPARTITE_INCLUDE_SELLS: mask_bar &= temp_df_ct[\"category\"] != \"sell\"\n",
    "        temp_df_ct = temp_df_ct[mask_bar]\n",
    "        if not temp_df_ct.empty:\n",
    "            committee_sector_counts_for_bar_chart[committee_name_loop] = temp_df_ct[\"sector\"].value_counts().to_dict()\n",
    "\n",
    "\n",
    "# --- Save JSON report ---\n",
    "json_report_path = os.path.join(BIPARTITE_OUTPUT_DIR, \"all_committees_bipartite_stats.json\")\n",
    "with open(json_report_path, \"w\") as fp:\n",
    "    json.dump(all_committee_stats_json, fp, indent=2)\n",
    "print(f\"Saved JSON report for all committees: {json_report_path}\")\n",
    "\n",
    "# --- Combined Stacked Bar Chart (Simplified from your Cell 7) ---\n",
    "all_sectors_for_bar = sorted(list(set(sec for counts in committee_sector_counts_for_bar_chart.values() for sec in counts.keys())))\n",
    "if all_sectors_for_bar: # only plot if there's data\n",
    "    bar_data = []\n",
    "    sorted_committee_names_for_bar = sorted(committee_sector_counts_for_bar_chart.keys())\n",
    "    for c_name in sorted_committee_names_for_bar:\n",
    "        row = [committee_sector_counts_for_bar_chart[c_name].get(sec, 0) for sec in all_sectors_for_bar]\n",
    "        bar_data.append(row)\n",
    "    \n",
    "    df_combined_bar = pd.DataFrame(bar_data, index=sorted_committee_names_for_bar, columns=all_sectors_for_bar)\n",
    "\n",
    "    plt.figure(figsize=(12, 8)) # Keep original fig size\n",
    "    df_combined_bar.plot(kind=\"bar\", stacked=True, colormap=\"tab20\", width=0.8, figsize=(12,8)) # Pass figsize here too\n",
    "    plt.ylabel(\"Number of Transactions\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"Sector Breakdown of Transactions by Committee (Filtered by Buys/Sells)\")\n",
    "    plt.legend(title=\"Sector\", bbox_to_anchor=(1.02, 1), loc=\"upper left\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{BIPARTITE_OUTPUT_DIR}/all_committees_sector_breakdown_filtered.png\", dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {BIPARTITE_OUTPUT_DIR}/all_committees_sector_breakdown_filtered.png\")\n",
    "else:\n",
    "    print(\"No data available for combined sector breakdown bar chart.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "19cff554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Analyzing Bipartite for General Congress (Non-Committee Members) ---\n",
      "[Saved] general_congress_member_sector_bipartite/general_sector_counts.json\n",
      "[Saved] general_congress_member_sector_bipartite/general_congress_graph_stats.json\n",
      "[Saved] general_congress_member_sector_bipartite/general_congress_bipartite.png\n",
      "[Saved] general_congress_member_sector_bipartite/general_congress_sector_bar.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: General Congress Bipartite Network (Members not in specified committees)\n",
    "\n",
    "# --- Configuration for this cell's analysis ---\n",
    "GENERAL_INCLUDE_BUYS = False  # Consistent with your original, can be changed\n",
    "GENERAL_INCLUDE_SELLS = True\n",
    "\n",
    "GENERAL_MIN_SECTOR_NODE_SIZE = 200\n",
    "GENERAL_MAX_SECTOR_NODE_SIZE = 2000\n",
    "GENERAL_MEMBER_NODE_SIZE = 600\n",
    "GENERAL_MAX_EDGE_WIDTH = 8\n",
    "\n",
    "GENERAL_OUTPUT_DIR = \"general_congress_member_sector_bipartite\" # Specific output dir\n",
    "os.makedirs(GENERAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Function to Analyze General Congress Trading ---\n",
    "def analyze_general_congress_trading(\n",
    "    stock_transactions_df,\n",
    "    committee_membership_map,\n",
    "    output_directory\n",
    "    ):\n",
    "    \n",
    "    print(\"\\n--- Analyzing Bipartite for General Congress (Non-Committee Members) ---\")\n",
    "\n",
    "    # 1) Derive the full set of all committee-member IDs\n",
    "    all_committee_members = set()\n",
    "    for period_map in committee_membership_map.values():\n",
    "        for members_set in period_map.values(): # Iterate through sets of members for \"116\", \"117\"\n",
    "            all_committee_members.update({str(m) for m in members_set}) # Ensure string IDs\n",
    "\n",
    "    # 2) Filter STOCK_TRANSACTIONS_DF for \"general\" members\n",
    "    df_general = stock_transactions_df[\n",
    "        ~stock_transactions_df[\"member_id\"].astype(str).isin(all_committee_members) # Ensure comparison with string IDs\n",
    "    ].copy()\n",
    "\n",
    "    if df_general.empty:\n",
    "        print(\"No transactions found for 'General Congress' members. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "    # 3) Derive \"category\" and filter by buy/sell\n",
    "    df_general[\"category\"] = df_general[\"type\"].str.lower().map(\n",
    "        lambda t: \"buy\" if \"purchase\" in t\n",
    "        else (\"sell\" if \"sale\" in t else (\"exchange\" if \"exchange\" in t else \"other\"))\n",
    "    )\n",
    "    mask = pd.Series(True, index=df_general.index)\n",
    "    if not GENERAL_INCLUDE_BUYS:  mask &= df_general[\"category\"] != \"buy\"\n",
    "    if not GENERAL_INCLUDE_SELLS: mask &= df_general[\"category\"] != \"sell\"\n",
    "    df_general = df_general[mask].copy()\n",
    "\n",
    "    if df_general.empty:\n",
    "        print(\"No 'General Congress' transactions after type filter. Skipping.\")\n",
    "        return {}\n",
    "\n",
    "    # 4) Sector counts for general population\n",
    "    general_sector_counts = df_general[\"sector\"].value_counts().sort_index().to_dict()\n",
    "    general_sector_counts_path = os.path.join(output_directory, \"general_sector_counts.json\")\n",
    "    with open(general_sector_counts_path, \"w\") as fp:\n",
    "        json.dump(general_sector_counts, fp, indent=2)\n",
    "    print(f\"[Saved] {general_sector_counts_path}\")\n",
    "    \n",
    "    if not general_sector_counts:\n",
    "        print(\"No sector activity for 'General Congress'. Skipping graph and further stats.\")\n",
    "        return {\"num_general_members\": len(df_general[\"member_id\"].unique()), \"total_tx_count\": len(df_general)}\n",
    "\n",
    "    # 5) Build bipartite graph\n",
    "    present_member_ids_general = set(df_general[\"member_id\"].unique())\n",
    "    \n",
    "    if 'member' in df_general.columns:\n",
    "        member_name_map_general = df_general[[\"member_id\", \"member\"]].drop_duplicates().set_index(\"member_id\")[\"member\"].to_dict()\n",
    "    else:\n",
    "        member_name_map_general = {mid: mid for mid in present_member_ids_general}\n",
    "    for mid in present_member_ids_general: # Ensure all have an entry\n",
    "        member_name_map_general.setdefault(mid, mid)\n",
    "\n",
    "    sorted_members_general = sorted(list(present_member_ids_general), key=lambda mid: member_name_map_general.get(mid, mid))\n",
    "    unique_sectors_general = sorted(general_sector_counts.keys())\n",
    "\n",
    "    member_sector_agg_general = (\n",
    "        df_general.groupby([\"member_id\", \"sector\", \"category\"])\n",
    "        .agg(tx_count=pd.NamedAgg(column=\"amount\", aggfunc=\"count\"),\n",
    "             tx_volume=pd.NamedAgg(column=\"amount\", aggfunc=\"sum\"))\n",
    "        .reset_index()\n",
    "    )\n",
    "    max_edge_amount_general = member_sector_agg_general[\"tx_volume\"].max() if not member_sector_agg_general.empty else 1.0\n",
    "    if max_edge_amount_general == 0: max_edge_amount_general = 1.0\n",
    "\n",
    "    G_general = nx.Graph()\n",
    "    G_general.add_nodes_from(sorted_members_general, bipartite=0)\n",
    "    G_general.add_nodes_from(unique_sectors_general, bipartite=1)\n",
    "\n",
    "    for _, row in member_sector_agg_general.iterrows():\n",
    "        m, s, cat, vol, cnt = (\n",
    "            row[\"member_id\"], row[\"sector\"], row[\"category\"],\n",
    "            row[\"tx_volume\"], int(row[\"tx_count\"])\n",
    "        )\n",
    "        if m in sorted_members_general: # Ensure member is in graph\n",
    "             if G_general.has_edge(m,s):\n",
    "                G_general[m][s]['tx_volume'] = G_general[m][s].get('tx_volume',0) + vol\n",
    "                G_general[m][s]['tx_count'] = G_general[m][s].get('tx_count',0) + cnt\n",
    "                G_general[m][s]['categories'] = G_general[m][s].get('categories', set()).union({cat})\n",
    "             else:\n",
    "                G_general.add_edge(m, s, category=cat, tx_count=cnt, tx_volume=vol, categories={cat})\n",
    "\n",
    "    # 6) Graph-level and Node-level stats (simplified for brevity, can expand)\n",
    "    general_stats = {\n",
    "        \"num_general_members\": len(sorted_members_general), \"num_sectors\": len(unique_sectors_general),\n",
    "        \"num_edges\": G_general.number_of_edges(), \"total_tx_count\": int(df_general.shape[0]),\n",
    "        \"total_tx_volume\": float(df_general[\"amount\"].sum()), \"nodes\": {} # Can add per-node later\n",
    "    }\n",
    "    general_stats_path = os.path.join(output_directory, \"general_congress_graph_stats.json\")\n",
    "    with open(general_stats_path, \"w\") as fp:\n",
    "        json.dump(general_stats, fp, indent=2)\n",
    "    print(f\"[Saved] {general_stats_path}\")\n",
    "\n",
    "    # 7) Plotting (if graph has nodes and edges)\n",
    "    if not G_general.nodes() or not G_general.edges():\n",
    "        print(\"Graph for General Congress is empty or has no edges. Skipping plot.\")\n",
    "        return general_stats\n",
    "\n",
    "    node_colors_plot, node_sizes_plot, labels_plot = [], [], {}\n",
    "    max_sector_count_plot = max(general_sector_counts.values()) if general_sector_counts else 1.0\n",
    "    if max_sector_count_plot == 0 : max_sector_count_plot = 1.0\n",
    "\n",
    "    # Get nodes present in the actual graph G_general for layout and plotting\n",
    "    current_graph_member_nodes_gen = [n for n,d in G_general.nodes(data=True) if d['bipartite']==0]\n",
    "    current_graph_sector_nodes_gen = [n for n,d in G_general.nodes(data=True) if d['bipartite']==1]\n",
    "    plot_nodes_ordered_gen = current_graph_member_nodes_gen + current_graph_sector_nodes_gen\n",
    "\n",
    "\n",
    "    for mid in current_graph_member_nodes_gen: # Iterate over nodes actually in G_general\n",
    "        node_colors_plot.append(\"#636363\") # Darker grey for general members\n",
    "        node_sizes_plot.append(GENERAL_MEMBER_NODE_SIZE)\n",
    "        labels_plot[mid] = member_name_map_general.get(mid, mid)\n",
    "    \n",
    "    for sec_node_idx, sec in enumerate(current_graph_sector_nodes_gen):\n",
    "        node_colors_plot.insert(len(current_graph_member_nodes_gen) + sec_node_idx, \"#fdae61\") # Light orange for sectors\n",
    "        cnt = general_sector_counts.get(sec, 0)\n",
    "        size = (cnt / max_sector_count_plot) * (GENERAL_MAX_SECTOR_NODE_SIZE - GENERAL_MIN_SECTOR_NODE_SIZE) + GENERAL_MIN_SECTOR_NODE_SIZE\n",
    "        node_sizes_plot.insert(len(current_graph_member_nodes_gen) + sec_node_idx, size)\n",
    "        labels_plot[sec] = sec\n",
    "        \n",
    "    edge_list_plot, edge_colors_plot, edge_widths_plot = [], [], []\n",
    "    for u, v, data in G_general.edges(data=True):\n",
    "        vol = data.get(\"tx_volume\",0)\n",
    "        color = \"gray\" # Simplified edge color for general pop\n",
    "        width = (abs(vol) / max_edge_amount_general if max_edge_amount_general > 0 else 0) * GENERAL_MAX_EDGE_WIDTH + 0.5\n",
    "        edge_list_plot.append((u,v))\n",
    "        edge_colors_plot.append(color)\n",
    "        edge_widths_plot.append(min(width, GENERAL_MAX_EDGE_WIDTH))\n",
    "\n",
    "    if not current_graph_member_nodes_gen:\n",
    "        print(\"No member nodes in General Congress graph for layout. Skipping plot.\")\n",
    "        return general_stats\n",
    "\n",
    "    pos_plot = nx.bipartite_layout(G_general, current_graph_member_nodes_gen)\n",
    "    plt.figure(figsize=(max(12, len(current_graph_member_nodes_gen)*0.2), max(10, len(current_graph_sector_nodes_gen)*0.3) ))\n",
    "    nx.draw_networkx_nodes(G_general, pos_plot, nodelist=plot_nodes_ordered_gen, node_color=node_colors_plot, node_size=node_sizes_plot, alpha=0.9)\n",
    "    nx.draw_networkx_edges(G_general, pos_plot, edgelist=edge_list_plot, edge_color=edge_colors_plot, width=edge_widths_plot, alpha=0.5)\n",
    "    nx.draw_networkx_labels(G_general, pos_plot, labels=labels_plot, font_size=7) # Smaller font for potentially many members\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    title = f\"General Congress Members ↔ Sectors\\n(Buys={'Y' if GENERAL_INCLUDE_BUYS else 'N'} | Sells={'Y' if GENERAL_INCLUDE_SELLS else 'N'})\"\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    graph_path_plot = os.path.join(output_directory, \"general_congress_bipartite.png\")\n",
    "    plt.savefig(graph_path_plot, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {graph_path_plot}\")\n",
    "\n",
    "    # 8) Bar chart for general population sector counts\n",
    "    sectors_bar = list(general_sector_counts.keys())\n",
    "    counts_bar = [general_sector_counts[sec] for sec in sectors_bar]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(sectors_bar, counts_bar, color=\"#636363\")\n",
    "    plt.xlabel(\"Sector\")\n",
    "    plt.ylabel(\"Number of Transactions (General Population)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.title(\"General Congress Transactions by Sector\")\n",
    "    plt.tight_layout()\n",
    "    bar_path_plot = os.path.join(output_directory, \"general_congress_sector_bar.png\")\n",
    "    plt.savefig(bar_path_plot, dpi=150)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {bar_path_plot}\")\n",
    "    \n",
    "    return general_stats\n",
    "\n",
    "# --- Execute General Congress Analysis ---\n",
    "# This assumes STOCK_TRANSACTIONS_DF and COMMITTEE_MEMBERSHIP_MAP are globally available and processed\n",
    "# from previous cells (especially COMMITTEE_MEMBERSHIP_MAP having string IDs)\n",
    "general_congress_summary_stats = analyze_general_congress_trading(\n",
    "    STOCK_TRANSACTIONS_DF,\n",
    "    COMMITTEE_MEMBERSHIP_MAP,\n",
    "    GENERAL_OUTPUT_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "369ceda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Side-by-side plot to: combined_committee_sector_analysis/sector_proportions_side_by_side.png\n",
      "Saved underlying data CSVs to 'combined_committee_sector_analysis'.\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Combined Sector Breakdown Plots (Count and Volume Proportions)\n",
    "\n",
    "# --- Configuration for this cell ---\n",
    "COMBINED_PLOT_INCLUDE_BUYS = True\n",
    "COMBINED_PLOT_INCLUDE_SELLS = True\n",
    "COMBINED_PLOT_OUTPUT_DIR = \"combined_committee_sector_analysis\"\n",
    "os.makedirs(COMBINED_PLOT_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Data Aggregation Function ---\n",
    "def aggregate_sector_data_for_committees(\n",
    "    stock_df, committee_map, include_buys=True, include_sells=True\n",
    "):\n",
    "    stock_df[\"amount\"] = pd.to_numeric(stock_df[\"amount\"], errors=\"coerce\").fillna(0.0)\n",
    "    stock_df[\"type\"] = stock_df[\"type\"].astype(str).str.lower()\n",
    "\n",
    "    committee_sector_counts = {}\n",
    "    committee_sector_volumes = {}\n",
    "    all_sectors = set()\n",
    "\n",
    "    # Process defined committees\n",
    "    processed_committee_names_list = []\n",
    "    for committee_name_key, period_map_val in committee_map.items():\n",
    "        if committee_name_key == \"General Population\": continue # Handle GP separately\n",
    "        processed_committee_names_list.append(committee_name_key)\n",
    "\n",
    "        members_116 = period_map_val.get(\"116\", set())\n",
    "        members_117 = period_map_val.get(\"117\", set())\n",
    "        current_committee_members = {str(m) for m in (members_116 | members_117)}\n",
    "\n",
    "        if not current_committee_members:\n",
    "            committee_sector_counts[committee_name_key] = {}\n",
    "            committee_sector_volumes[committee_name_key] = {}\n",
    "            continue\n",
    "\n",
    "        df_c = stock_df[stock_df[\"member_id\"].astype(str).isin(current_committee_members)].copy()\n",
    "        if df_c.empty:\n",
    "            committee_sector_counts[committee_name_key] = {}\n",
    "            committee_sector_volumes[committee_name_key] = {}\n",
    "            continue\n",
    "        \n",
    "        df_c[\"category\"] = df_c[\"type\"].map(\n",
    "            lambda t: \"buy\" if (\"purchase\" in t or \"buy\" in t)\n",
    "            else (\"sell\" if (\"sale\" in t or \"sell\" in t)\n",
    "            else (\"exchange\" if \"exchange\" in t else \"other\"))\n",
    "        )\n",
    "        mask_c = pd.Series(True, index=df_c.index)\n",
    "        if not include_buys:  mask_c &= df_c[\"category\"] != \"buy\"\n",
    "        if not include_sells: mask_c &= df_c[\"category\"] != \"sell\"\n",
    "        df_c = df_c[mask_c]\n",
    "\n",
    "        if df_c.empty:\n",
    "            committee_sector_counts[committee_name_key] = {}\n",
    "            committee_sector_volumes[committee_name_key] = {}\n",
    "            continue\n",
    "            \n",
    "        committee_sector_counts[committee_name_key] = df_c[\"sector\"].value_counts().to_dict()\n",
    "        committee_sector_volumes[committee_name_key] = df_c.groupby(\"sector\")[\"amount\"].sum().to_dict()\n",
    "        all_sectors.update(committee_sector_counts[committee_name_key].keys())\n",
    "        all_sectors.update(committee_sector_volumes[committee_name_key].keys())\n",
    "\n",
    "    # Process General Population\n",
    "    all_committee_members_ever_set = set()\n",
    "    for committee_name_key, period_map_val in committee_map.items():\n",
    "        if committee_name_key == \"General Population\": continue\n",
    "        for member_s in period_map_val.values():\n",
    "            all_committee_members_ever_set.update({str(m) for m in member_s})\n",
    "\n",
    "    df_g = stock_df[~stock_df[\"member_id\"].astype(str).isin(all_committee_members_ever_set)].copy()\n",
    "    if not df_g.empty:\n",
    "        df_g[\"category\"] = df_g[\"type\"].map(\n",
    "            lambda t: \"buy\" if (\"purchase\" in t or \"buy\" in t)\n",
    "            else (\"sell\" if (\"sale\" in t or \"sell\" in t)\n",
    "            else (\"exchange\" if \"exchange\" in t else \"other\"))\n",
    "        )\n",
    "        mask_g = pd.Series(True, index=df_g.index)\n",
    "        if not include_buys:  mask_g &= df_g[\"category\"] != \"buy\"\n",
    "        if not include_sells: mask_g &= df_g[\"category\"] != \"sell\"\n",
    "        df_g = df_g[mask_g]\n",
    "        if not df_g.empty:\n",
    "            committee_sector_counts[\"General Population\"] = df_g[\"sector\"].value_counts().to_dict()\n",
    "            committee_sector_volumes[\"General Population\"] = df_g.groupby(\"sector\")[\"amount\"].sum().to_dict()\n",
    "            all_sectors.update(committee_sector_counts[\"General Population\"].keys())\n",
    "            all_sectors.update(committee_sector_volumes[\"General Population\"].keys())\n",
    "            if \"General Population\" not in processed_committee_names_list:\n",
    "                 processed_committee_names_list.insert(0, \"General Population\") # Add GP to the list for sorting\n",
    "        else:\n",
    "            committee_sector_counts[\"General Population\"] = {}\n",
    "            committee_sector_volumes[\"General Population\"] = {}\n",
    "    else:\n",
    "        committee_sector_counts[\"General Population\"] = {}\n",
    "        committee_sector_volumes[\"General Population\"] = {}\n",
    "\n",
    "    final_sorted_committees = sorted([\n",
    "        c_name for c_name in processed_committee_names_list\n",
    "        if committee_sector_counts.get(c_name) or committee_sector_volumes.get(c_name)\n",
    "    ], key=lambda x: (x != \"General Population\", x)) # Sorts \"General Population\" first, then alphabetically\n",
    "\n",
    "\n",
    "    final_all_sectors_list = sorted(list(s for s in all_sectors if pd.notna(s)))\n",
    "\n",
    "    if not final_sorted_committees or not final_all_sectors_list:\n",
    "        return pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), pd.DataFrame(), []\n",
    "\n",
    "    # DataFrames for counts and volumes (absolute)\n",
    "    data_c = [[committee_sector_counts.get(c, {}).get(s, 0) for s in final_all_sectors_list] for c in final_sorted_committees]\n",
    "    df_abs_count = pd.DataFrame(data_c, index=final_sorted_committees, columns=final_all_sectors_list)\n",
    "    data_v = [[committee_sector_volumes.get(c, {}).get(s, 0.0) for s in final_all_sectors_list] for c in final_sorted_committees]\n",
    "    df_abs_vol = pd.DataFrame(data_v, index=final_sorted_committees, columns=final_all_sectors_list)\n",
    "\n",
    "    # Normalized DataFrames (proportions)\n",
    "    sum_c = df_abs_count.sum(axis=1)\n",
    "    df_norm_count = df_abs_count.copy()\n",
    "    for col_n in df_norm_count.columns: df_norm_count[col_n] = np.where(sum_c > 0, df_norm_count[col_n].divide(sum_c, axis=0), 0)\n",
    "    df_norm_count = df_norm_count.fillna(0)\n",
    "\n",
    "    sum_v = df_abs_vol.sum(axis=1)\n",
    "    df_norm_vol = df_abs_vol.copy()\n",
    "    for col_n in df_norm_vol.columns: df_norm_vol[col_n] = np.where(sum_v > 0, df_norm_vol[col_n].divide(sum_v, axis=0), 0)\n",
    "    df_norm_vol = df_norm_vol.fillna(0)\n",
    "    \n",
    "    return df_norm_count, df_norm_vol, df_abs_count, df_abs_vol, final_all_sectors_list\n",
    "\n",
    "\n",
    "# --- Execute Data Aggregation ---\n",
    "(df_count_proportions, df_volume_proportions, \n",
    " df_absolute_counts, df_absolute_volumes,\n",
    " plotted_sectors) = aggregate_sector_data_for_committees(\n",
    "    STOCK_TRANSACTIONS_DF, COMMITTEE_MEMBERSHIP_MAP,\n",
    "    COMBINED_PLOT_INCLUDE_BUYS, COMBINED_PLOT_INCLUDE_SELLS\n",
    ")\n",
    "\n",
    "if df_count_proportions.empty and df_volume_proportions.empty:\n",
    "    print(\"No data available to plot for combined sector breakdowns. Exiting this cell's plotting.\")\n",
    "else:\n",
    "    # --- Plotting Side-by-Side ---\n",
    "    FIG_WIDTH_COMBINED = 10 # Adjusted for potentially more readable legend\n",
    "    FIG_HEIGHT_COMBINED = 5.5 # Base height, will adjust for legend\n",
    "\n",
    "    title_fontsize = 10\n",
    "    axis_label_fontsize = 8\n",
    "    tick_label_fontsize = 7\n",
    "    legend_fontsize = 6.5\n",
    "    legend_title_fontsize = 7.5\n",
    "    \n",
    "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(FIG_WIDTH_COMBINED, FIG_HEIGHT_COMBINED), sharey=True)\n",
    "    \n",
    "    # Define a consistent colormap\n",
    "    num_plot_sectors = len(plotted_sectors)\n",
    "    base_cmap_plot = plt.get_cmap(\"tab20\")\n",
    "    colors_plot = [base_cmap_plot(i % len(base_cmap_plot.colors)) for i in range(num_plot_sectors)]\n",
    "\n",
    "    # Plot 1: By Transaction Count Proportions\n",
    "    if not df_count_proportions.empty:\n",
    "        df_count_proportions.plot(kind=\"bar\", stacked=True, color=colors_plot, width=0.85, ax=axes[0], legend=False)\n",
    "        axes[0].set_ylabel(\"Proportion of Total Transactions\", fontsize=axis_label_fontsize)\n",
    "        axes[0].set_xticklabels(axes[0].get_xticklabels(), rotation=45, ha=\"right\", fontsize=tick_label_fontsize)\n",
    "        axes[0].set_title(\"By Transaction Count\", fontsize=title_fontsize, pad=8)\n",
    "        axes[0].tick_params(axis='y', labelsize=tick_label_fontsize)\n",
    "        axes[0].yaxis.set_major_formatter(mticker.FormatStrFormatter('%.1f'))\n",
    "    else:\n",
    "        axes[0].text(0.5, 0.5, \"No Count Data\", ha='center', va='center', transform=axes[0].transAxes)\n",
    "        axes[0].set_title(\"By Transaction Count\", fontsize=title_fontsize, pad=8)\n",
    "    axes[0].set_xlabel(\"\")\n",
    "    axes[0].spines['top'].set_visible(False); axes[0].spines['right'].set_visible(False)\n",
    "    axes[0].grid(axis='y', linestyle=':', linewidth=0.5, alpha=0.6, color='gray'); axes[0].set_axisbelow(True)\n",
    "\n",
    "    # Plot 2: By Transaction Volume Proportions\n",
    "    if not df_volume_proportions.empty:\n",
    "        df_volume_proportions.plot(kind=\"bar\", stacked=True, color=colors_plot, width=0.85, ax=axes[1], legend=False)\n",
    "        axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha=\"right\", fontsize=tick_label_fontsize)\n",
    "        axes[1].set_title(\"By Transaction Volume\", fontsize=title_fontsize, pad=8)\n",
    "        axes[1].tick_params(axis='y', labelsize=tick_label_fontsize)\n",
    "    else:\n",
    "        axes[1].text(0.5, 0.5, \"No Volume Data\", ha='center', va='center', transform=axes[1].transAxes)\n",
    "        axes[1].set_title(\"By Transaction Volume\", fontsize=title_fontsize, pad=8)\n",
    "    axes[1].set_xlabel(\"\")\n",
    "    axes[1].spines['top'].set_visible(False); axes[1].spines['right'].set_visible(False)\n",
    "    axes[1].grid(axis='y', linestyle=':', linewidth=0.5, alpha=0.6, color='gray'); axes[1].set_axisbelow(True)\n",
    "\n",
    "    # Shared Legend\n",
    "    handles, labels = [], []\n",
    "    source_ax_for_legend = axes[0] if not df_count_proportions.empty else (axes[1] if not df_volume_proportions.empty else None)\n",
    "    if source_ax_for_legend:\n",
    "        handles, labels = source_ax_for_legend.get_legend_handles_labels()\n",
    "\n",
    "    if handles and labels:\n",
    "        num_legend_cols_plot = min(5, (len(plotted_sectors) + 2) // 3 if len(plotted_sectors) > 0 else 1)\n",
    "        if len(plotted_sectors) <= 5: num_legend_cols_plot = len(plotted_sectors) if len(plotted_sectors) > 0 else 1\n",
    "        elif len(plotted_sectors) <= 10: num_legend_cols_plot = (len(plotted_sectors)+1)//2\n",
    "\n",
    "        fig.legend(handles, labels, title=\"Sector\", loc='lower center',\n",
    "                   bbox_to_anchor=(0.5, -0.02), # Adjusted y for possible more legend rows\n",
    "                   ncol=num_legend_cols_plot,\n",
    "                   fontsize=legend_fontsize, title_fontsize=legend_title_fontsize, frameon=False,\n",
    "                   labelspacing=0.3, columnspacing=0.8)\n",
    "        \n",
    "    fig.suptitle(\"Sector Investment Proportions: Committees vs. General Population\", fontsize=title_fontsize + 2, y=0.99)\n",
    "    plt.subplots_adjust(left=0.08, right=0.98, bottom=0.30, top=0.90, wspace=0.12) # Adjusted bottom and top\n",
    "\n",
    "    side_by_side_plot_path = os.path.join(COMBINED_PLOT_OUTPUT_DIR, \"sector_proportions_side_by_side.png\")\n",
    "    plt.savefig(side_by_side_plot_path, dpi=300)\n",
    "    plt.close(fig)\n",
    "    print(f\"[Saved] Side-by-side plot to: {side_by_side_plot_path}\")\n",
    "\n",
    "    # Save underlying data\n",
    "    df_count_proportions.to_csv(os.path.join(COMBINED_PLOT_OUTPUT_DIR, \"proportions_by_count.csv\"))\n",
    "    df_volume_proportions.to_csv(os.path.join(COMBINED_PLOT_OUTPUT_DIR, \"proportions_by_volume.csv\"))\n",
    "    df_absolute_counts.to_csv(os.path.join(COMBINED_PLOT_OUTPUT_DIR, \"absolute_counts_by_sector.csv\"))\n",
    "    df_absolute_volumes.to_csv(os.path.join(COMBINED_PLOT_OUTPUT_DIR, \"absolute_volumes_by_sector.csv\"))\n",
    "    print(f\"Saved underlying data CSVs to '{COMBINED_PLOT_OUTPUT_DIR}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "658dc4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Bipartite Graph (Count-Weighted) ---\n",
      "Bipartite graph B_count: 30 nodes, 196 edges.\n",
      "\n",
      "--- Projecting to Committee-Only Graph (Count Overlap) ---\n",
      "G_overlap_count has 10 nodes, 45 edges.\n",
      "\n",
      "--- Analyzing Committee Projection Network (Count-Based) ---\n",
      "Top 5 Committees by Eigenvector Centrality (Count-Based G_overlap):\n",
      "                 committee  degree_centrality  eigenvector_centrality  betweenness_centrality  core_number  louvain_community\n",
      "        General Population                1.0                0.440521                     0.0            9                  0\n",
      "  House Financial Services                1.0                0.402857                     0.0            9                  0\n",
      " House Energy and Commerce                1.0                0.383078                     0.0            9                  0\n",
      "      House Appropriations                1.0                0.371793                     0.0            9                  0\n",
      "House Oversight and Reform                1.0                0.319412                     0.0            9                  0\n",
      "[Saved] network_analysis_count_based/committee_projection_network_count.png\n",
      "\n",
      "--- Projecting to Sector-Only Graph (Count Overlap) ---\n",
      "G_sec_overlap_count has 20 nodes, 190 edges.\n",
      "Top 5 Sectors by Eigenvector Centrality (Count-Based G_sec_overlap):\n",
      "                sector  eigenvector_centrality  louvain_community\n",
      "            Technology                0.386743                  0\n",
      "               Finance                0.384282                  0\n",
      "           Health Care                0.356743                  0\n",
      "     Consumer Services                0.345924                  0\n",
      "Consumer Discretionary                0.261660                  0\n",
      "[Saved] network_analysis_count_based/sector_projection_network_count.png\n",
      "\n",
      "--- Modularity Test for Committee Projection (Count-Based) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random bipartite (count): 100%|██████████| 1000/1000 [00:00<00:00, 1426.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real G_overlap_count modularity: 0.0012\n",
      "Mean random modularity: 0.0000 ± 0.0005\n",
      "Empirical p-value for modularity >= real: 0.0080\n",
      "\n",
      "--- Dendrogram, Heatmap, KL-Div (Count Proportions) ---\n",
      "[Saved] network_analysis_count_based/committee_dendrogram_count.png\n",
      "[Saved] network_analysis_count_based/committee_sector_heatmap_clustered_count.png\n",
      "\n",
      "--- Chi-Square Test (Absolute Counts vs General Population) ---\n",
      "Top 5 Committees by smallest χ² p-value (absolute counts):\n",
      "                                    committee  chi2_statistic_count  p_value_count\n",
      "                   House Oversight and Reform           1761.655898   0.000000e+00\n",
      "                               Senate Finance            227.940717   8.765101e-38\n",
      "Senate Health, Education, Labor, and Pensions            222.933923   8.886901e-37\n",
      "                     House Financial Services            221.965066   1.390626e-36\n",
      "                         House Appropriations            218.414763   7.164736e-36\n",
      "\n",
      "COUNT-BASED Network Analysis complete. Outputs in 'network_analysis_count_based'\n"
     ]
    }
   ],
   "source": [
    "# Cells 8 & 9: Network Science Analysis (COUNT-BASED)\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "# from sklearn.metrics.pairwise import cosine_similarity # Not used here\n",
    "import community as community_louvain\n",
    "from scipy.stats import chi2_contingency, entropy\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram\n",
    "from scipy.spatial.distance import pdist # Used by linkage\n",
    "from tqdm import tqdm\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.patches as mpatches # For custom legends\n",
    "\n",
    "# --- Configuration for COUNT-BASED Analysis ---\n",
    "COUNT_ANALYSIS_OUTPUT_DIR = \"network_analysis_count_based\"\n",
    "os.makedirs(COUNT_ANALYSIS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Path to the JSON file containing committee-sector transaction COUNTS\n",
    "# This assumes the combined sector breakdown script (Cell 7 refactored) saves this.\n",
    "# If not, this data needs to be loaded or computed first.\n",
    "# For now, assuming 'df_absolute_counts' is available or can be loaded.\n",
    "# If 'df_absolute_counts' is from the previous cell:\n",
    "if 'df_absolute_counts' not in globals() or df_absolute_counts.empty:\n",
    "    print(\"CRITICAL: df_absolute_counts (from Cell 7/combined breakdown) not found or empty. Cannot proceed with count-based network analysis.\")\n",
    "    # As a fallback, try to load it if saved previously\n",
    "    counts_csv_path = os.path.join(\"combined_committee_sector_analysis\", \"absolute_counts_by_sector.csv\") # Path from previous cell output\n",
    "    if os.path.exists(counts_csv_path):\n",
    "        print(f\"Loading absolute counts from: {counts_csv_path}\")\n",
    "        df_counts = pd.read_csv(counts_csv_path, index_col=0)\n",
    "    else:\n",
    "        print(f\"CRITICAL: Count data CSV not found at {counts_csv_path} either.\")\n",
    "        exit() # Or handle error appropriately\n",
    "else:\n",
    "    df_counts = df_absolute_counts.copy() # Use if available from previous cell run\n",
    "\n",
    "# groups = list(df_counts.index) # Committees + General Population\n",
    "# all_sectors = list(df_counts.columns)\n",
    "\n",
    "# Ensure groups and all_sectors are correctly defined based on df_counts\n",
    "if df_counts.empty:\n",
    "    print(\"df_counts is empty, cannot proceed with network analysis.\")\n",
    "    exit()\n",
    "else:\n",
    "    groups = list(df_counts.index)\n",
    "    all_sectors = list(df_counts.columns)\n",
    "\n",
    "\n",
    "N_PERMUTATIONS_COUNT = 1000 # For null model testing\n",
    "\n",
    "# --- 1. Build Bipartite Graph (Committees <-> Sectors, weighted by COUNT) ---\n",
    "print(\"\\n--- Building Bipartite Graph (Count-Weighted) ---\")\n",
    "B_count = nx.Graph()\n",
    "B_count.add_nodes_from(groups, bipartite=0)\n",
    "B_count.add_nodes_from(all_sectors, bipartite=1)\n",
    "\n",
    "for committee in groups:\n",
    "    for sector in all_sectors:\n",
    "        count = df_counts.loc[committee, sector]\n",
    "        if count > 0:\n",
    "            B_count.add_edge(committee, sector, weight=int(count)) # Ensure weight is int for some algos\n",
    "\n",
    "print(f\"Bipartite graph B_count: {B_count.number_of_nodes()} nodes, {B_count.number_of_edges()} edges.\")\n",
    "\n",
    "# --- 2. Project Bipartite to Committee-Only Graph (G_overlap_count) ---\n",
    "print(\"\\n--- Projecting to Committee-Only Graph (Count Overlap) ---\")\n",
    "comm_nodes_count = [n for n, d in B_count.nodes(data=True) if d[\"bipartite\"] == 0]\n",
    "G_overlap_count = nx.Graph()\n",
    "G_overlap_count.add_nodes_from(comm_nodes_count)\n",
    "for c1, c2 in combinations(comm_nodes_count, 2):\n",
    "    common_sectors = set(B_count.neighbors(c1)) & set(B_count.neighbors(c2))\n",
    "    if not common_sectors: continue\n",
    "    overlap_weight = sum(min(B_count[c1][s][\"weight\"], B_count[c2][s][\"weight\"]) for s in common_sectors)\n",
    "    if overlap_weight > 0:\n",
    "        G_overlap_count.add_edge(c1, c2, weight=overlap_weight)\n",
    "print(f\"G_overlap_count has {G_overlap_count.number_of_nodes()} nodes, {G_overlap_count.number_of_edges()} edges.\")\n",
    "\n",
    "# --- 3. Metrics & Community Detection on G_overlap_count ---\n",
    "print(\"\\n--- Analyzing Committee Projection Network (Count-Based) ---\")\n",
    "if G_overlap_count.number_of_nodes() > 0 and G_overlap_count.number_of_edges() > 0: # Check graph validity\n",
    "    deg_cent_comm_c = nx.degree_centrality(G_overlap_count)\n",
    "    eig_cent_comm_c = nx.eigenvector_centrality_numpy(G_overlap_count, weight=\"weight\")\n",
    "    betw_cent_comm_c = nx.betweenness_centrality(G_overlap_count, weight=None, normalized=True)\n",
    "    core_numbers_comm_c = nx.core_number(G_overlap_count)\n",
    "    partition_comm_c = community_louvain.best_partition(G_overlap_count, weight=\"weight\", random_state=42) # Add random_state for reproducibility\n",
    "    for node in G_overlap_count.nodes(): partition_comm_c.setdefault(node, -1)\n",
    "\n",
    "    df_comm_metrics_c = pd.DataFrame({\n",
    "        \"committee\": list(G_overlap_count.nodes()),\n",
    "        \"degree_centrality\": [deg_cent_comm_c.get(c,0) for c in G_overlap_count.nodes()],\n",
    "        \"eigenvector_centrality\": [eig_cent_comm_c.get(c,0) for c in G_overlap_count.nodes()],\n",
    "        \"betweenness_centrality\": [betw_cent_comm_c.get(c,0) for c in G_overlap_count.nodes()],\n",
    "        \"core_number\": [core_numbers_comm_c.get(c,0) for c in G_overlap_count.nodes()],\n",
    "        \"louvain_community\": [partition_comm_c.get(c,-1) for c in G_overlap_count.nodes()]\n",
    "    }).sort_values(\"eigenvector_centrality\", ascending=False)\n",
    "    df_comm_metrics_c.to_csv(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"committee_projection_metrics_count.csv\"), index=False)\n",
    "    print(\"Top 5 Committees by Eigenvector Centrality (Count-Based G_overlap):\")\n",
    "    print(df_comm_metrics_c.head(5).to_string(index=False))\n",
    "\n",
    "    # Visualization (similar to your original, enhanced version)\n",
    "    plt.figure(figsize=(10, 8)) # Adjusted size\n",
    "    pos_comm_c = nx.spring_layout(G_overlap_count, seed=42, k=0.6, iterations=50)\n",
    "    node_colors_c = []\n",
    "    for n in G_overlap_count.nodes():\n",
    "        if n == \"General Population\": node_colors_c.append(\"#7f7f7f\")\n",
    "        elif n.startswith(\"House\"): node_colors_c.append(\"#1f77b4\")\n",
    "        else: node_colors_c.append(\"#2ca02c\")\n",
    "    \n",
    "    max_eig_c = max(eig_cent_comm_c.values()) if eig_cent_comm_c else 1.0\n",
    "    if max_eig_c == 0: max_eig_c = 1.0\n",
    "    comm_sizes_c = [(eig_cent_comm_c.get(n,0) / max_eig_c) * 2500 + 200 for n in G_overlap_count.nodes()]\n",
    "\n",
    "    edge_weights_c = [d[\"weight\"] for u, v, d in G_overlap_count.edges(data=True)]\n",
    "    max_w_c = max(edge_weights_c) if edge_weights_c else 1.0\n",
    "    if max_w_c == 0: max_w_c = 1.0\n",
    "    \n",
    "    # Dynamic edge styling based on weight distribution\n",
    "    percentiles_w_c = np.percentile(edge_weights_c, [0, 60, 80, 100]) if edge_weights_c else [0,0,0,0]\n",
    "    \n",
    "    for u, v, d in G_overlap_count.edges(data=True):\n",
    "        weight = d[\"weight\"]\n",
    "        if weight >= percentiles_w_c[2]: # Top 20%\n",
    "            lw, alpha = (weight / max_w_c) * 5 + 0.5, 0.7\n",
    "        elif weight >= percentiles_w_c[1]: # Next 20%\n",
    "            lw, alpha = (weight / max_w_c) * 3 + 0.3, 0.5\n",
    "        else: # Bottom 60%\n",
    "            lw, alpha = (weight / max_w_c) * 1.5 + 0.1, 0.3\n",
    "        lw = max(0.1, lw) # ensure min width\n",
    "        nx.draw_networkx_edges(G_overlap_count, pos_comm_c, edgelist=[(u, v)], width=lw, edge_color=\"#555555\", alpha=alpha)\n",
    "\n",
    "    nx.draw_networkx_nodes(G_overlap_count, pos_comm_c, node_size=comm_sizes_c, node_color=node_colors_c, edgecolors=\"black\", linewidths=0.7, alpha=0.9)\n",
    "    nx.draw_networkx_labels(G_overlap_count, pos_comm_c, font_size=9, font_color=\"black\",\n",
    "                            bbox=dict(facecolor=\"#f8f8f8\", edgecolor=\"none\", alpha=0.8, boxstyle=\"round,pad=0.2\"))\n",
    "    \n",
    "    patch_h = mpatches.Patch(color=\"#1f77b4\", label=\"House Committees\")\n",
    "    patch_s = mpatches.Patch(color=\"#2ca02c\", label=\"Senate Committees\")\n",
    "    patch_g = mpatches.Patch(color=\"#7f7f7f\", label=\"General Population\")\n",
    "    plt.legend(handles=[patch_h, patch_s, patch_g], loc=\"lower left\", frameon=False, fontsize=10)\n",
    "    plt.title(\"Committee Projection Network (Transaction Count Overlap)\\n(Node color = Chamber; size = Eigenvector Centrality)\", fontsize=12)\n",
    "    plt.axis(\"off\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"committee_projection_network_count.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {COUNT_ANALYSIS_OUTPUT_DIR}/committee_projection_network_count.png\")\n",
    "else:\n",
    "    print(\"Committee projection graph (count-based) is empty or has no edges. Skipping analysis and plot.\")\n",
    "\n",
    "\n",
    "# --- 4. Project Bipartite to Sector-Only Graph (G_sec_overlap_count) ---\n",
    "print(\"\\n--- Projecting to Sector-Only Graph (Count Overlap) ---\")\n",
    "sec_nodes_count = [n for n, d in B_count.nodes(data=True) if d[\"bipartite\"] == 1]\n",
    "G_sec_overlap_count = nx.Graph()\n",
    "G_sec_overlap_count.add_nodes_from(sec_nodes_count)\n",
    "for s1, s2 in combinations(sec_nodes_count, 2):\n",
    "    overlap_sum = 0\n",
    "    for committee in groups:\n",
    "        # Get counts for committee in s1 and s2 from df_counts\n",
    "        count_s1 = df_counts.loc[committee, s1] if s1 in df_counts.columns else 0\n",
    "        count_s2 = df_counts.loc[committee, s2] if s2 in df_counts.columns else 0\n",
    "        overlap_sum += min(count_s1, count_s2)\n",
    "    if overlap_sum > 0:\n",
    "        G_sec_overlap_count.add_edge(s1, s2, weight=overlap_sum)\n",
    "print(f\"G_sec_overlap_count has {G_sec_overlap_count.number_of_nodes()} nodes, {G_sec_overlap_count.number_of_edges()} edges.\")\n",
    "\n",
    "# --- 5. Metrics & Community Detection on G_sec_overlap_count ---\n",
    "# (Similar logic as for committee projection, apply to G_sec_overlap_count)\n",
    "# ... This part can be refactored into a function if you do it often ...\n",
    "if G_sec_overlap_count.number_of_nodes() > 0 and G_sec_overlap_count.number_of_edges() > 0:\n",
    "    eig_cent_sector_c = nx.eigenvector_centrality_numpy(G_sec_overlap_count, weight=\"weight\")\n",
    "    partition_sector_c = community_louvain.best_partition(G_sec_overlap_count, weight=\"weight\", random_state=42)\n",
    "    df_sector_metrics_c = pd.DataFrame({\n",
    "        \"sector\": list(G_sec_overlap_count.nodes()),\n",
    "        \"eigenvector_centrality\": [eig_cent_sector_c.get(s,0) for s in G_sec_overlap_count.nodes()],\n",
    "        \"louvain_community\": [partition_sector_c.get(s,-1) for s in G_sec_overlap_count.nodes()]\n",
    "    }).sort_values(\"eigenvector_centrality\", ascending=False)\n",
    "    df_sector_metrics_c.to_csv(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"sector_projection_metrics_count.csv\"), index=False)\n",
    "    print(\"Top 5 Sectors by Eigenvector Centrality (Count-Based G_sec_overlap):\")\n",
    "    print(df_sector_metrics_c.head(5).to_string(index=False))\n",
    "\n",
    "    # Visualization for Sector Projection\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    pos_sec_c = nx.spring_layout(G_sec_overlap_count, seed=24, k=0.8, iterations=50)\n",
    "    sec_colors_c = [partition_sector_c.get(s,-1) for s in G_sec_overlap_count.nodes()]\n",
    "    max_eig_sec_c = max(eig_cent_sector_c.values()) if eig_cent_sector_c else 1.0\n",
    "    if max_eig_sec_c == 0: max_eig_sec_c = 1.0\n",
    "    sec_sizes_c  = [(eig_cent_sector_c.get(s,0) / max_eig_sec_c) * 2500 + 200 for s in G_sec_overlap_count.nodes()]\n",
    "    \n",
    "    edge_weights_sec_c = [d[\"weight\"] for u,v,d in G_sec_overlap_count.edges(data=True)]\n",
    "    max_w_sec_c = max(edge_weights_sec_c) if edge_weights_sec_c else 1.0\n",
    "    if max_w_sec_c == 0: max_w_sec_c = 1.0\n",
    "    edge_widths_sec_c = [(d[\"weight\"] / max_w_sec_c) * 5 + 0.5 for u,v,d in G_sec_overlap_count.edges(data=True)]\n",
    "\n",
    "    nx.draw_networkx_nodes(G_sec_overlap_count, pos_sec_c, node_size=sec_sizes_c, node_color=sec_colors_c, cmap=plt.cm.viridis, alpha=0.9) # Changed cmap\n",
    "    nx.draw_networkx_edges(G_sec_overlap_count, pos_sec_c, width=edge_widths_sec_c, edge_color=\"#777777\", alpha=0.5)\n",
    "    nx.draw_networkx_labels(G_sec_overlap_count, pos_sec_c, font_size=9)\n",
    "    plt.title(\"Sector Projection Network (Transaction Count Overlap)\\n(Node color = Louvain Community, size = Eigenvector Centrality)\", fontsize=12)\n",
    "    plt.axis(\"off\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"sector_projection_network_count.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {COUNT_ANALYSIS_OUTPUT_DIR}/sector_projection_network_count.png\")\n",
    "else:\n",
    "    print(\"Sector projection graph (count-based) is empty or has no edges. Skipping analysis and plot.\")\n",
    "\n",
    "\n",
    "# --- 6. Statistical Validation (Modularity for G_overlap_count) ---\n",
    "print(\"\\n--- Modularity Test for Committee Projection (Count-Based) ---\")\n",
    "if G_overlap_count.number_of_nodes() > 0 and G_overlap_count.number_of_edges() > 0:\n",
    "    real_modularity_c = community_louvain.modularity(partition_comm_c, G_overlap_count, weight=\"weight\")\n",
    "    comm_degrees_c = [B_count.degree(c) for c in comm_nodes_count] # Degrees from bipartite\n",
    "    sec_degrees_c  = [B_count.degree(s) for s in sec_nodes_count]\n",
    "    rand_mods_c = []\n",
    "    for _ in tqdm(range(N_PERMUTATIONS_COUNT), desc=\"Random bipartite (count)\"):\n",
    "        B_r_c = bipartite.configuration_model(comm_degrees_c, sec_degrees_c, create_using=nx.Graph(), seed=np.random.randint(10000)) # Add seed\n",
    "        B_r_c = nx.Graph(((u, v) for u, v in B_r_c.edges() if u != v)); B_r_c.remove_edges_from(nx.selfloop_edges(B_r_c))\n",
    "        comm_nodes_rand_c = list(range(len(comm_nodes_count)))\n",
    "        G_r_c = bipartite.weighted_projected_graph(B_r_c, comm_nodes_rand_c) # This projection is unweighted if B_r_c is unweighted\n",
    "        if G_r_c.number_of_edges() > 0:\n",
    "            part_r_c = community_louvain.best_partition(G_r_c, weight=\"weight\", random_state=42) # Use weight even if 1\n",
    "            rand_mods_c.append(community_louvain.modularity(part_r_c, G_r_c, weight=\"weight\"))\n",
    "        else:\n",
    "            rand_mods_c.append(0) # Or handle as appropriate if no edges\n",
    "            \n",
    "    rand_mods_c = np.array(rand_mods_c)\n",
    "    emp_pval_c = np.mean(rand_mods_c >= real_modularity_c) if len(rand_mods_c) > 0 else 1.0\n",
    "    print(f\"Real G_overlap_count modularity: {real_modularity_c:.4f}\")\n",
    "    print(f\"Mean random modularity: {rand_mods_c.mean():.4f} ± {rand_mods_c.std():.4f}\")\n",
    "    print(f\"Empirical p-value for modularity >= real: {emp_pval_c:.4f}\")\n",
    "    pd.DataFrame({\"random_modularity_count\": rand_mods_c}).to_csv(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"null_modularity_distribution_count.csv\"), index=False)\n",
    "else:\n",
    "    print(\"Skipping modularity test for G_overlap_count as it's empty or has no edges.\")\n",
    "\n",
    "\n",
    "# --- 7. Hierarchical Clustering, Heatmap, KL Divergence on df_proportions (from count data) ---\n",
    "# Ensure df_counts is the one loaded/calculated based on 'COMBINED_JSON' at the start of this cell block\n",
    "# And df_prop is derived from it.\n",
    "df_prop_count = df_counts.div(df_counts.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "if not df_prop_count.empty:\n",
    "    print(\"\\n--- Dendrogram, Heatmap, KL-Div (Count Proportions) ---\")\n",
    "    distance_matrix_c = pdist(df_prop_count.values, metric=\"euclidean\")\n",
    "    Z_c = linkage(distance_matrix_c, method=\"ward\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dendro_data_c = dendrogram(Z_c, labels=df_prop_count.index.tolist(), leaf_rotation=90, leaf_font_size=9)\n",
    "    plt.title(\"Dendrogram: Committees & Gen Pop by Transaction Count Proportions\", fontsize=11)\n",
    "    plt.ylabel(\"Ward Distance\", fontsize=9); plt.yticks(fontsize=8); plt.xticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"committee_dendrogram_count.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {COUNT_ANALYSIS_OUTPUT_DIR}/committee_dendrogram_count.png\")\n",
    "\n",
    "    # KL Divergence\n",
    "    epsilon = 1e-9\n",
    "    if \"General Population\" in df_prop_count.index:\n",
    "        P_general_c = df_prop_count.loc[\"General Population\"].values + epsilon\n",
    "        kl_vals_c = {c: float(entropy(df_prop_count.loc[c].values + epsilon, P_general_c))\n",
    "                     for c in df_prop_count.index if c != \"General Population\"}\n",
    "        df_kl_c = pd.DataFrame.from_dict(kl_vals_c, orient=\"index\", columns=[\"kl_divergence_count\"]).sort_values(\"kl_divergence_count\", ascending=False)\n",
    "        df_kl_c.to_csv(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"committee_kl_divergence_count.csv\"))\n",
    "        # KL Plot omitted for brevity, can be added if needed\n",
    "    else:\n",
    "        print(\"Warning: 'General Population' not in df_prop_count for KL divergence.\")\n",
    "\n",
    "    # Clustered Heatmap\n",
    "    leaf_order_c = dendro_data_c[\"ivl\"]\n",
    "    df_clustered_c = df_prop_count.loc[leaf_order_c]\n",
    "    plt.figure(figsize=(11, 7)) # Adjusted size\n",
    "    plt.imshow(df_clustered_c.values, aspect=\"auto\", cmap=\"viridis\", vmin=0, vmax=max(0.1, df_clustered_c.values.max())) # Use viridis, ensure vmax > 0\n",
    "    plt.colorbar(label=\"Proportion of Transaction Counts\")\n",
    "    plt.yticks(ticks=np.arange(len(leaf_order_c)), labels=leaf_order_c, fontsize=7)\n",
    "    plt.xticks(ticks=np.arange(len(all_sectors)), labels=all_sectors, rotation=90, fontsize=6)\n",
    "    plt.title(\"Clustered Heatmap: Sector Transaction Count Proportions\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"committee_sector_heatmap_clustered_count.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {COUNT_ANALYSIS_OUTPUT_DIR}/committee_sector_heatmap_clustered_count.png\")\n",
    "else:\n",
    "    print(\"Count proportion DataFrame is empty. Skipping Dendrogram, Heatmap, KL-Div for counts.\")\n",
    "\n",
    "\n",
    "# --- 8. Chi-Square Tests (Absolute Counts) ---\n",
    "if \"General Population\" in df_counts.index and not df_counts.loc[\"General Population\"].empty:\n",
    "    print(\"\\n--- Chi-Square Test (Absolute Counts vs General Population) ---\")\n",
    "    general_vec_c = df_counts.loc[\"General Population\"].values.astype(int)\n",
    "    results_chi_c = []\n",
    "    for c_name_chi in groups:\n",
    "        if c_name_chi == \"General Population\": continue\n",
    "        comm_vec_c = df_counts.loc[c_name_chi].values.astype(int)\n",
    "        if comm_vec_c.sum() == 0 and general_vec_c.sum() == 0 : continue # Avoid all zero contingency\n",
    "        contingency_c = np.vstack([comm_vec_c, general_vec_c])\n",
    "        try:\n",
    "            chi2_c, p_c, dof_c, _ = chi2_contingency(contingency_c)\n",
    "            results_chi_c.append({\"committee\": c_name_chi, \"chi2_statistic_count\": float(chi2_c), \"p_value_count\": float(p_c)})\n",
    "        except ValueError as e_chi: # Handle cases where chi2 cannot be computed (e.g. low expected frequencies)\n",
    "            print(f\"Chi2 error for {c_name_chi}: {e_chi}. Assigning p_value=1.0\")\n",
    "            results_chi_c.append({\"committee\": c_name_chi, \"chi2_statistic_count\": np.nan, \"p_value_count\": 1.0})\n",
    "\n",
    "\n",
    "    if results_chi_c:\n",
    "        df_chi_c = pd.DataFrame(results_chi_c).sort_values(\"p_value_count\")\n",
    "        df_chi_c.to_csv(os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"chi2_vs_general_count.csv\"), index=False)\n",
    "        print(\"Top 5 Committees by smallest χ² p-value (absolute counts):\")\n",
    "        print(df_chi_c.head(5).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No Chi-square results to report for counts.\")\n",
    "else:\n",
    "    print(\"General Population data not found or empty in df_counts. Skipping Chi-square for counts.\")\n",
    "\n",
    "print(f\"\\nCOUNT-BASED Network Analysis complete. Outputs in '{COUNT_ANALYSIS_OUTPUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f35032b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Building Bipartite Graph (Volume-Weighted) ---\n",
      "Bipartite graph B_vol: 30 nodes, 196 edges.\n",
      "\n",
      "--- Projecting to Committee-Only Graph (Volume Overlap) ---\n",
      "G_overlap_vol has 10 nodes, 45 edges.\n",
      "[Saved] network_analysis_volume_based/committee_overlap_edges_volume.csv\n",
      "\n",
      "--- Analyzing Committee Projection Network (Volume-Based) ---\n",
      "Top 5 Committees by Eigenvector Centrality (Volume-Based G_overlap):\n",
      "                 committee  degree_centrality  eigenvector_centrality  betweenness_centrality  core_number  louvain_community\n",
      "        General Population                1.0                0.516638                     0.0            9                  0\n",
      "      House Ways and Means                1.0                0.514668                     0.0            9                  0\n",
      "  House Financial Services                1.0                0.510707                     0.0            9                  0\n",
      " House Energy and Commerce                1.0                0.286496                     0.0            9                  1\n",
      "House Oversight and Reform                1.0                0.193254                     0.0            9                  1\n",
      "[Saved] network_analysis_volume_based/committee_projection_network_volume.png\n",
      "\n",
      "--- Projecting to Sector-Only Graph (Volume Overlap) ---\n",
      "\n",
      "--- Modularity Test for Committee Projection (Volume-Based) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random bipartite (volume): 100%|██████████| 1000/1000 [00:00<00:00, 1699.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Real G_overlap_vol modularity: 0.0796\n",
      "Mean random modularity (volume): 0.0001 ± 0.0007\n",
      "Empirical p-value (volume) >= real: 0.0000\n",
      "\n",
      "--- Dendrogram, Heatmap, KL-Div (Volume Proportions) ---\n",
      "[Saved] network_analysis_volume_based/committee_dendrogram_volume.png\n",
      "[Saved] network_analysis_volume_based/committee_sector_heatmap_clustered_volume.png\n",
      "\n",
      "--- Chi-Square Test (Absolute Volumes vs General Population) ---\n",
      "Top 5 Committees by smallest χ² p-value (absolute volumes):\n",
      "                 committee  chi2_statistic_volume  p_value_volume\n",
      "      House Appropriations           1.409538e+07             0.0\n",
      " House Energy and Commerce           2.355701e+07             0.0\n",
      "  House Financial Services           1.420587e+08             0.0\n",
      "House Oversight and Reform           2.999258e+08             0.0\n",
      "      House Ways and Means           1.274248e+08             0.0\n",
      "\n",
      "VOLUME-BASED Network Analysis complete. Outputs in 'network_analysis_volume_based'\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Network Science Analysis (VOLUME-BASED)\n",
    "\n",
    "# --- Configuration for VOLUME-BASED Analysis ---\n",
    "VOLUME_ANALYSIS_OUTPUT_DIR = \"network_analysis_volume_based\"\n",
    "os.makedirs(VOLUME_ANALYSIS_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Path to the JSON file containing committee-sector transaction VOLUMES\n",
    "# This assumes the combined sector breakdown script (Cell 7 refactored) saves this.\n",
    "# If 'df_absolute_volumes' is available from previous cell:\n",
    "if 'df_absolute_volumes' not in globals() or df_absolute_volumes.empty:\n",
    "    print(\"CRITICAL: df_absolute_volumes (from Cell 7/combined breakdown) not found or empty. Cannot proceed with volume-based network analysis.\")\n",
    "    volumes_csv_path = os.path.join(\"combined_committee_sector_analysis\", \"absolute_volumes_by_sector.csv\")\n",
    "    if os.path.exists(volumes_csv_path):\n",
    "        print(f\"Loading absolute volumes from: {volumes_csv_path}\")\n",
    "        df_volumes = pd.read_csv(volumes_csv_path, index_col=0)\n",
    "    else:\n",
    "        print(f\"CRITICAL: Volume data CSV not found at {volumes_csv_path} either.\")\n",
    "        exit()\n",
    "else:\n",
    "    df_volumes = df_absolute_volumes.copy()\n",
    "\n",
    "# groups_vol = list(df_volumes.index)\n",
    "# all_sectors_vol = list(df_volumes.columns)\n",
    "\n",
    "if df_volumes.empty:\n",
    "    print(\"df_volumes is empty, cannot proceed with volume-based network analysis.\")\n",
    "    exit()\n",
    "else:\n",
    "    groups_vol = list(df_volumes.index)\n",
    "    all_sectors_vol = list(df_volumes.columns)\n",
    "\n",
    "\n",
    "N_PERMUTATIONS_VOL = 1000\n",
    "\n",
    "# --- 1. Build Bipartite Graph (Committees <-> Sectors, weighted by VOLUME) ---\n",
    "print(\"\\n--- Building Bipartite Graph (Volume-Weighted) ---\")\n",
    "B_vol = nx.Graph()\n",
    "B_vol.add_nodes_from(groups_vol, bipartite=0)\n",
    "B_vol.add_nodes_from(all_sectors_vol, bipartite=1)\n",
    "for committee in groups_vol:\n",
    "    for sector in all_sectors_vol:\n",
    "        volume = df_volumes.loc[committee, sector]\n",
    "        if volume > 0: # Only add edge if volume is positive\n",
    "            B_vol.add_edge(committee, sector, weight=float(volume))\n",
    "print(f\"Bipartite graph B_vol: {B_vol.number_of_nodes()} nodes, {B_vol.number_of_edges()} edges.\")\n",
    "\n",
    "# --- 2. Project Bipartite to Committee-Only Graph (G_overlap_vol) ---\n",
    "print(\"\\n--- Projecting to Committee-Only Graph (Volume Overlap) ---\")\n",
    "comm_nodes_vol = [n for n, d in B_vol.nodes(data=True) if d[\"bipartite\"] == 0]\n",
    "G_overlap_vol = nx.Graph()\n",
    "G_overlap_vol.add_nodes_from(comm_nodes_vol)\n",
    "for c1, c2 in combinations(comm_nodes_vol, 2):\n",
    "    common_sectors_vol = set(B_vol.neighbors(c1)) & set(B_vol.neighbors(c2))\n",
    "    if not common_sectors_vol: continue\n",
    "    overlap_weight_vol = sum(min(B_vol[c1][s][\"weight\"], B_vol[c2][s][\"weight\"]) for s in common_sectors_vol)\n",
    "    if overlap_weight_vol > 0:\n",
    "        G_overlap_vol.add_edge(c1, c2, weight=overlap_weight_vol)\n",
    "print(f\"G_overlap_vol has {G_overlap_vol.number_of_nodes()} nodes, {G_overlap_vol.number_of_edges()} edges.\")\n",
    "# Save edge list for G_overlap_vol\n",
    "edge_rows_vol = [(u,v,d[\"weight\"]) for u,v,d in G_overlap_vol.edges(data=True)]\n",
    "edge_csv_path_vol = os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_overlap_edges_volume.csv\")\n",
    "pd.DataFrame(edge_rows_vol, columns=[\"source\", \"target\", \"weight\"]).to_csv(edge_csv_path_vol, index=False)\n",
    "print(f\"[Saved] {edge_csv_path_vol}\")\n",
    "\n",
    "\n",
    "# --- 3. Metrics & Community Detection on G_overlap_vol ---\n",
    "print(\"\\n--- Analyzing Committee Projection Network (Volume-Based) ---\")\n",
    "if G_overlap_vol.number_of_nodes() > 0 and G_overlap_vol.number_of_edges() > 0:\n",
    "    deg_cent_comm_v = nx.degree_centrality(G_overlap_vol)\n",
    "    eig_cent_comm_v = nx.eigenvector_centrality_numpy(G_overlap_vol, weight=\"weight\")\n",
    "    betw_cent_comm_v = nx.betweenness_centrality(G_overlap_vol, weight=None, normalized=True)\n",
    "    core_numbers_comm_v = nx.core_number(G_overlap_vol)\n",
    "    partition_comm_v = community_louvain.best_partition(G_overlap_vol, weight=\"weight\", random_state=42)\n",
    "    for node in G_overlap_vol.nodes(): partition_comm_v.setdefault(node, -1)\n",
    "\n",
    "    df_comm_metrics_v = pd.DataFrame({\n",
    "        \"committee\": list(G_overlap_vol.nodes()),\n",
    "        \"degree_centrality\": [deg_cent_comm_v.get(c,0) for c in G_overlap_vol.nodes()],\n",
    "        \"eigenvector_centrality\": [eig_cent_comm_v.get(c,0) for c in G_overlap_vol.nodes()],\n",
    "        \"betweenness_centrality\": [betw_cent_comm_v.get(c,0) for c in G_overlap_vol.nodes()],\n",
    "        \"core_number\": [core_numbers_comm_v.get(c,0) for c in G_overlap_vol.nodes()],\n",
    "        \"louvain_community\": [partition_comm_v.get(c,-1) for c in G_overlap_vol.nodes()]\n",
    "    }).sort_values(\"eigenvector_centrality\", ascending=False)\n",
    "    df_comm_metrics_v.to_csv(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_projection_metrics_volume.csv\"), index=False)\n",
    "    print(\"Top 5 Committees by Eigenvector Centrality (Volume-Based G_overlap):\")\n",
    "    print(df_comm_metrics_v.head(5).to_string(index=False))\n",
    "\n",
    "    # Visualization (Volume-based committee projection)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    pos_comm_v = nx.spring_layout(G_overlap_vol, seed=42, k=0.6, iterations=50)\n",
    "    node_colors_v = []\n",
    "    for n in G_overlap_vol.nodes():\n",
    "        if n == \"General Population\": node_colors_v.append(\"#7f7f7f\")\n",
    "        elif n.startswith(\"House\"): node_colors_v.append(\"#1f77b4\")\n",
    "        else: node_colors_v.append(\"#2ca02c\")\n",
    "    max_eig_v = max(eig_cent_comm_v.values()) if eig_cent_comm_v else 1.0\n",
    "    if max_eig_v == 0 : max_eig_v = 1.0\n",
    "    comm_sizes_v = [(eig_cent_comm_v.get(n,0) / max_eig_v) * 2500 + 200 for n in G_overlap_vol.nodes()]\n",
    "    \n",
    "    edge_weights_v = [d[\"weight\"] for u,v,d in G_overlap_vol.edges(data=True)]\n",
    "    max_w_v = max(edge_weights_v) if edge_weights_v else 1.0\n",
    "    if max_w_v == 0: max_w_v = 1.0\n",
    "    percentiles_w_v = np.percentile(edge_weights_v, [0, 60, 80, 100]) if edge_weights_v else [0,0,0,0]\n",
    "\n",
    "    for u, v, d in G_overlap_vol.edges(data=True):\n",
    "        weight = d[\"weight\"]\n",
    "        if weight >= percentiles_w_v[2]: lw, alpha = (weight / max_w_v) * 5 + 0.5, 0.7\n",
    "        elif weight >= percentiles_w_v[1]: lw, alpha = (weight / max_w_v) * 3 + 0.3, 0.5\n",
    "        else: lw, alpha = (weight / max_w_v) * 1.5 + 0.1, 0.3\n",
    "        lw = max(0.1, lw)\n",
    "        nx.draw_networkx_edges(G_overlap_vol, pos_comm_v, edgelist=[(u,v)], width=lw, edge_color=\"#555555\", alpha=alpha)\n",
    "    \n",
    "    nx.draw_networkx_nodes(G_overlap_vol, pos_comm_v, node_size=comm_sizes_v, node_color=node_colors_v, edgecolors=\"black\", linewidths=0.7, alpha=0.9)\n",
    "    nx.draw_networkx_labels(G_overlap_vol, pos_comm_v, font_size=9, font_color=\"black\",\n",
    "                            bbox=dict(facecolor=\"#f8f8f8\", edgecolor=\"none\", alpha=0.8, boxstyle=\"round,pad=0.2\"))\n",
    "    patch_h = mpatches.Patch(color=\"#1f77b4\", label=\"House Committees\"); patch_s = mpatches.Patch(color=\"#2ca02c\", label=\"Senate Committees\"); patch_g = mpatches.Patch(color=\"#7f7f7f\", label=\"General Population\")\n",
    "    plt.legend(handles=[patch_h, patch_s, patch_g], loc=\"lower left\", frameon=False, fontsize=10)\n",
    "    plt.title(\"Committee Projection Network (Transaction Volume Overlap)\\n(Node color = Chamber; size = Eigenvector Centrality)\", fontsize=12)\n",
    "    plt.axis(\"off\"); plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_projection_network_volume.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {VOLUME_ANALYSIS_OUTPUT_DIR}/committee_projection_network_volume.png\")\n",
    "else:\n",
    "    print(\"Committee projection graph (volume-based) is empty or has no edges. Skipping.\")\n",
    "\n",
    "# --- 4. Project Bipartite to Sector-Only Graph (G_sec_overlap_vol) ---\n",
    "# ... (Similar projection and analysis as for counts, but using B_vol and df_volumes) ...\n",
    "# This section is analogous to the sector projection in the count-based analysis\n",
    "# For brevity, I'll assume this logic would be mirrored. If you need it explicitly, let me know.\n",
    "print(\"\\n--- Projecting to Sector-Only Graph (Volume Overlap) ---\")\n",
    "sec_nodes_vol = [n for n, d in B_vol.nodes(data=True) if d[\"bipartite\"] == 1]\n",
    "G_sec_overlap_vol = nx.Graph()\n",
    "G_sec_overlap_vol.add_nodes_from(sec_nodes_vol)\n",
    "# ... (rest of G_sec_overlap_vol creation, metrics, plotting - mirror count version) ...\n",
    "# This is a placeholder - you'd copy/adapt the G_sec_overlap_count logic here.\n",
    "if not G_sec_overlap_vol.nodes(): # Minimal check\n",
    "    print(\"Sector projection graph (volume-based) could not be built or is empty.\")\n",
    "\n",
    "\n",
    "# --- 5. Statistical Validation (Modularity for G_overlap_vol) ---\n",
    "print(\"\\n--- Modularity Test for Committee Projection (Volume-Based) ---\")\n",
    "if G_overlap_vol.number_of_nodes() > 0 and G_overlap_vol.number_of_edges() > 0:\n",
    "    real_modularity_v = community_louvain.modularity(partition_comm_v, G_overlap_vol, weight=\"weight\")\n",
    "    comm_degrees_v = [B_vol.degree(c) for c in comm_nodes_vol]\n",
    "    sec_degrees_v  = [B_vol.degree(s) for s in sec_nodes_vol]\n",
    "    rand_mods_v = []\n",
    "    for _ in tqdm(range(N_PERMUTATIONS_VOL), desc=\"Random bipartite (volume)\"):\n",
    "        B_r_v = bipartite.configuration_model(comm_degrees_v, sec_degrees_v, create_using=nx.Graph(), seed=np.random.randint(10000))\n",
    "        B_r_v = nx.Graph(((u,v) for u,v in B_r_v.edges() if u!=v)); B_r_v.remove_edges_from(nx.selfloop_edges(B_r_v))\n",
    "        comm_nodes_rand_v = list(range(len(comm_nodes_vol)))\n",
    "        G_r_v = bipartite.weighted_projected_graph(B_r_v, comm_nodes_rand_v)\n",
    "        if G_r_v.number_of_edges() > 0:\n",
    "            part_r_v = community_louvain.best_partition(G_r_v, weight=\"weight\", random_state=42)\n",
    "            rand_mods_v.append(community_louvain.modularity(part_r_v, G_r_v, weight=\"weight\"))\n",
    "        else:\n",
    "            rand_mods_v.append(0)\n",
    "    rand_mods_v = np.array(rand_mods_v)\n",
    "    emp_pval_v = np.mean(rand_mods_v >= real_modularity_v) if len(rand_mods_v) > 0 else 1.0\n",
    "    print(f\"Real G_overlap_vol modularity: {real_modularity_v:.4f}\")\n",
    "    print(f\"Mean random modularity (volume): {rand_mods_v.mean():.4f} ± {rand_mods_v.std():.4f}\")\n",
    "    print(f\"Empirical p-value (volume) >= real: {emp_pval_v:.4f}\")\n",
    "    pd.DataFrame({\"random_modularity_volume\": rand_mods_v}).to_csv(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"null_modularity_distribution_volume.csv\"), index=False)\n",
    "else:\n",
    "    print(\"Skipping modularity test for G_overlap_vol as it's empty or has no edges.\")\n",
    "\n",
    "\n",
    "# --- 6. Hierarchical Clustering, Heatmap, KL Divergence on df_vol_prop ---\n",
    "df_vol_prop = df_volumes.div(df_volumes.sum(axis=1), axis=0).fillna(0) # Recalculate based on current df_volumes\n",
    "if not df_vol_prop.empty:\n",
    "    print(\"\\n--- Dendrogram, Heatmap, KL-Div (Volume Proportions) ---\")\n",
    "    distance_matrix_v = pdist(df_vol_prop.values, metric=\"euclidean\")\n",
    "    Z_v = linkage(distance_matrix_v, method=\"ward\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    dendro_data_v = dendrogram(Z_v, labels=df_vol_prop.index.tolist(), leaf_rotation=90, leaf_font_size=9)\n",
    "    plt.title(\"Dendrogram: Committees & Gen Pop by Transaction Volume Proportions\", fontsize=11)\n",
    "    plt.ylabel(\"Ward Distance\", fontsize=9); plt.yticks(fontsize=8); plt.xticks(fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_dendrogram_volume.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {VOLUME_ANALYSIS_OUTPUT_DIR}/committee_dendrogram_volume.png\")\n",
    "\n",
    "    if \"General Population\" in df_vol_prop.index:\n",
    "        P_general_v = df_vol_prop.loc[\"General Population\"].values + 1e-9\n",
    "        kl_vals_v = {c: float(entropy(df_vol_prop.loc[c].values + 1e-9, P_general_v))\n",
    "                     for c in df_vol_prop.index if c != \"General Population\"}\n",
    "        df_kl_v = pd.DataFrame.from_dict(kl_vals_v, orient=\"index\", columns=[\"kl_divergence_volume\"]).sort_values(\"kl_divergence_volume\", ascending=False)\n",
    "        df_kl_v.to_csv(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_kl_divergence_volume.csv\"))\n",
    "        # KL Plot for volume omitted for brevity\n",
    "    else:\n",
    "        print(\"Warning: 'General Population' not in df_vol_prop for KL divergence (volume).\")\n",
    "\n",
    "    leaf_order_v = dendro_data_v[\"ivl\"]\n",
    "    df_clustered_v = df_vol_prop.loc[leaf_order_v]\n",
    "    plt.figure(figsize=(11, 7))\n",
    "    plt.imshow(df_clustered_v.values, aspect=\"auto\", cmap=\"viridis\", vmin=0, vmax=max(0.1, df_clustered_v.values.max()))\n",
    "    plt.colorbar(label=\"Proportion of Transaction Volume\")\n",
    "    plt.yticks(ticks=np.arange(len(leaf_order_v)), labels=leaf_order_v, fontsize=7)\n",
    "    plt.xticks(ticks=np.arange(len(all_sectors_vol)), labels=all_sectors_vol, rotation=90, fontsize=6)\n",
    "    plt.title(\"Clustered Heatmap: Sector Transaction Volume Proportions\", fontsize=11)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_sector_heatmap_clustered_volume.png\"), dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"[Saved] {VOLUME_ANALYSIS_OUTPUT_DIR}/committee_sector_heatmap_clustered_volume.png\")\n",
    "else:\n",
    "    print(\"Volume proportion DataFrame is empty. Skipping Dendrogram, Heatmap, KL-Div for volumes.\")\n",
    "\n",
    "# --- 7. Chi-Square Tests (Absolute Volumes) ---\n",
    "if \"General Population\" in df_volumes.index and not df_volumes.loc[\"General Population\"].empty:\n",
    "    print(\"\\n--- Chi-Square Test (Absolute Volumes vs General Population) ---\")\n",
    "    general_vec_v = df_volumes.loc[\"General Population\"].round().astype(int).values\n",
    "    results_chi_v = []\n",
    "    for c_name_chi_v in groups_vol: # Use groups_vol\n",
    "        if c_name_chi_v == \"General Population\": continue\n",
    "        comm_vec_v = df_volumes.loc[c_name_chi_v].round().astype(int).values\n",
    "        if comm_vec_v.sum() == 0 and general_vec_v.sum() == 0 : continue\n",
    "        contingency_v = np.vstack([comm_vec_v, general_vec_v])\n",
    "        try:\n",
    "            chi2_v, p_v, dof_v, _ = chi2_contingency(contingency_v)\n",
    "            results_chi_v.append({\"committee\": c_name_chi_v, \"chi2_statistic_volume\": float(chi2_v), \"p_value_volume\": float(p_v)})\n",
    "        except ValueError as e_chi_v:\n",
    "            print(f\"Chi2 error for {c_name_chi_v} (volume): {e_chi_v}. Assigning p_value=1.0\")\n",
    "            results_chi_v.append({\"committee\": c_name_chi_v, \"chi2_statistic_volume\": np.nan, \"p_value_volume\": 1.0})\n",
    "    if results_chi_v:\n",
    "        df_chi_v = pd.DataFrame(results_chi_v).sort_values(\"p_value_volume\")\n",
    "        df_chi_v.to_csv(os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"chi2_vs_general_volume.csv\"), index=False)\n",
    "        print(\"Top 5 Committees by smallest χ² p-value (absolute volumes):\")\n",
    "        print(df_chi_v.head(5).to_string(index=False))\n",
    "    else:\n",
    "        print(\"No Chi-square results to report for volumes.\")\n",
    "else:\n",
    "    print(\"General Population data not found or empty in df_volumes. Skipping Chi-square for volumes.\")\n",
    "\n",
    "\n",
    "print(f\"\\nVOLUME-BASED Network Analysis complete. Outputs in '{VOLUME_ANALYSIS_OUTPUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b40230c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209 weekly windows between 2019-01-03 and 2023-01-01\n",
      "Binary activity matrix X_tda.shape = (186, 209)\n",
      "H₀ bars found: 182\n",
      "[Saved] tda_temporal_weekly_activity/member_trade_barcode_weekly.png\n",
      "[Saved] H0 intervals to tda_temporal_weekly_activity/betti0_intervals_weekly.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/luisbravo/Desktop/CongressTradeNet/venv/lib/python3.13/site-packages/ripser/ripser.py:253: UserWarning: The input point cloud has more columns than rows; did you mean to transpose?\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Temporal Persistence (Betti-0 Barcode)\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "TDA_WINDOW_DAYS = 7\n",
    "TDA_TOP_N_MEMBERS = None # Set to an int (e.g., 200) to limit analysis, None for all\n",
    "TDA_OUTPUT_DIR = \"tda_temporal_weekly_activity\"\n",
    "os.makedirs(TDA_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Ensure STOCK_TRANSACTIONS_DF is available and prepped ---\n",
    "if 'STOCK_TRANSACTIONS_DF' not in globals() or STOCK_TRANSACTIONS_DF.empty:\n",
    "    print(\"CRITICAL: STOCK_TRANSACTIONS_DF not available for TDA. Skipping.\")\n",
    "    # exit()\n",
    "else:\n",
    "    df_tda = STOCK_TRANSACTIONS_DF.copy()\n",
    "    # Ensure date column is datetime and named 'date_dt' for consistency\n",
    "    if 'transaction_date_dt' not in df_tda.columns: # If using the original column name directly\n",
    "        if 'transaction_date' in df_tda.columns:\n",
    "            df_tda['date_dt'] = pd.to_datetime(df_tda['transaction_date'], errors='coerce')\n",
    "        else: # If neither 'transaction_date_dt' nor 'transaction_date' exists\n",
    "            print(\"CRITICAL: Suitable date column ('transaction_date_dt' or 'transaction_date') not found in STOCK_TRANSACTIONS_DF for TDA.\")\n",
    "            exit() # or raise error\n",
    "    else: # If 'transaction_date_dt' exists, ensure it's datetime\n",
    "        df_tda['date_dt'] = pd.to_datetime(df_tda['transaction_date_dt'], errors='coerce')\n",
    "\n",
    "    df_tda.dropna(subset=['date_dt'], inplace=True) # Remove rows where date parsing failed\n",
    "    df_tda[\"member_id\"] = df_tda[\"member_id\"].astype(str)\n",
    "\n",
    "    # Filter for 'sale' or 'purchase' types\n",
    "    type_mask = df_tda[\"type\"].astype(str).str.contains(\"sale|purchase\", case=False, na=False)\n",
    "    df_tda = df_tda[type_mask]\n",
    "\n",
    "    if df_tda.empty:\n",
    "        print(\"No 'sale' or 'purchase' transactions found for TDA. Skipping.\")\n",
    "        # exit()\n",
    "\n",
    "    # --- Build weekly windows ---\n",
    "    if not df_tda.empty:\n",
    "        t0_tda = df_tda[\"date_dt\"].min().normalize()\n",
    "        tN_tda = df_tda[\"date_dt\"].max().normalize() + timedelta(days=1) # Ensure last day is included\n",
    "        \n",
    "        # Create window indices\n",
    "        df_tda[\"w_idx\"] = ((df_tda[\"date_dt\"].dt.normalize() - t0_tda).dt.days // TDA_WINDOW_DAYS)\n",
    "        num_windows_tda = df_tda[\"w_idx\"].max() + 1 if not df_tda[\"w_idx\"].empty else 0\n",
    "        \n",
    "        print(f\"{num_windows_tda} weekly windows between {t0_tda.date()} and {tN_tda.date()}\")\n",
    "\n",
    "        # --- Build binary activity matrix ---\n",
    "        if TDA_TOP_N_MEMBERS and 'amount' in df_tda.columns:\n",
    "            top_ids_tda = (\n",
    "                df_tda.groupby(\"member_id\")[\"amount\"]\n",
    "                .sum().sort_values(ascending=False)\n",
    "                .head(TDA_TOP_N_MEMBERS).index\n",
    "            )\n",
    "            df_tda = df_tda[df_tda[\"member_id\"].isin(top_ids_tda)]\n",
    "        \n",
    "        member_list_tda = sorted(df_tda[\"member_id\"].unique())\n",
    "        if not member_list_tda or num_windows_tda == 0:\n",
    "            print(\"Not enough members or windows to create activity matrix for TDA. Skipping.\")\n",
    "            # exit()\n",
    "        else:\n",
    "            member_index_tda = {m: i for i, m in enumerate(member_list_tda)}\n",
    "            M_tda = len(member_list_tda)\n",
    "            X_tda = np.zeros((M_tda, num_windows_tda), dtype=np.uint8)\n",
    "\n",
    "            for mid_tda, w_idx_tda in df_tda[[\"member_id\", \"w_idx\"]].drop_duplicates().itertuples(index=False):\n",
    "                if mid_tda in member_index_tda and 0 <= w_idx_tda < num_windows_tda:\n",
    "                     X_tda[member_index_tda[mid_tda], w_idx_tda] = 1\n",
    "            print(f\"Binary activity matrix X_tda.shape = {X_tda.shape}\")\n",
    "\n",
    "            # --- Compute persistent homology ---\n",
    "            if X_tda.shape[0] >= 2 and X_tda.shape[1] > 0: # Need at least 2 members and some windows\n",
    "                try:\n",
    "                    diagrams_tda = ripser(X_tda, maxdim=0, metric=\"euclidean\")[\"dgms\"]\n",
    "                    h0_tda = diagrams_tda[0]\n",
    "                    print(f\"H₀ bars found: {len(h0_tda)}\")\n",
    "\n",
    "                    # --- Plot barcode ---\n",
    "                    if len(h0_tda) > 0:\n",
    "                        plt.figure(figsize=(10, 4.5)) # Adjusted for better title fit\n",
    "                        plot_diagrams(diagrams_tda, show=False, lifetime=True, \n",
    "                                      labels=[\"H₀ Intervals\"], legend=True) # Added labels and legend\n",
    "                        plt.title(\"Betti-0 Barcode — Weekly Trading Activity\\n(Long bars = members trading in similar weeks)\", fontsize=11)\n",
    "                        plt.xlabel(\"Filtration ε (Euclidean on binary weekly vectors)\", fontsize=9)\n",
    "                        plt.ylabel(\"Interval Index\", fontsize=9) # More generic y-label\n",
    "                        plt.xticks(fontsize=8); plt.yticks(fontsize=8)\n",
    "                        plt.tight_layout()\n",
    "                        barcode_path_tda = os.path.join(TDA_OUTPUT_DIR, \"member_trade_barcode_weekly.png\")\n",
    "                        plt.savefig(barcode_path_tda, dpi=200) # Increased dpi\n",
    "                        plt.close()\n",
    "                        print(f\"[Saved] {barcode_path_tda}\")\n",
    "\n",
    "                        # --- Export H0 intervals ---\n",
    "                        df_h0_intervals_tda = pd.DataFrame(h0_tda, columns=[\"birth\", \"death\"])\n",
    "                        # Add member_id to the H0 intervals\n",
    "                        if len(member_list_tda) >= len(df_h0_intervals_tda):\n",
    "                             df_h0_intervals_tda['member_id'] = member_list_tda[:len(df_h0_intervals_tda)]\n",
    "                             if id2name: # Check if id2name is populated\n",
    "                                df_h0_intervals_tda['member_name'] = df_h0_intervals_tda['member_id'].map(lambda x: id2name.get(x,x))\n",
    "                        df_h0_intervals_tda.to_csv(os.path.join(TDA_OUTPUT_DIR, \"betti0_intervals_weekly.csv\"), index=False)\n",
    "                        print(f\"[Saved] H0 intervals to {os.path.join(TDA_OUTPUT_DIR, 'betti0_intervals_weekly.csv')}\")\n",
    "                    else:\n",
    "                        print(\"No H0 intervals found to plot.\")\n",
    "                except Exception as e_ripser:\n",
    "                    print(f\"Error during Ripser computation or plotting: {e_ripser}\")\n",
    "            else:\n",
    "                print(\"Not enough data points or windows for Ripser computation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0fe3b137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Member-Level Infomap Analysis...\n",
      "Built member_to_committee_infomap with 355 entries.\n",
      "Using global id2name map.\n",
      "Using id2name_im_local with 190 entries.\n",
      "Pivoting transaction data...\n",
      "Pivot table shape: (186, 20)\n",
      "Number of members with committee assignments and trades: 112\n",
      "Building full member-member graph...\n",
      "Full member graph (Infomap): 112 nodes, 5022 edges.\n",
      "Sparsifying graph, keeping edges with weights >= 99th percentile...\n",
      "Sparse member graph (Infomap): 112 nodes, 51 edges.\n",
      "Preparing data for Infomap...\n",
      "Added 112 nodes to Infomap instance.\n",
      "Added 51 links to Infomap instance.\n",
      "Running Infomap algorithm...\n",
      "Infomap run completed: Codelength L=3.494 bits; Found 97 top-level modules.\n",
      "Extracting module assignments from Infomap result...\n",
      "Successfully extracted 112 module assignments for 97 unique modules.\n",
      "=======================================================\n",
      "  Infomap v2.8.0 starts at 2025-06-05 03:02:04\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "  -> Configuration: two-level\n",
      "=======================================================\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'undirected'... \n",
      "  -> Using undirected links.\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 112 nodes and 51 links...\n",
      "  -> One-level codelength: 3.5408604\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2025-06-05 03:02:04\n",
      "================================================\n",
      "Two-level compression: 0.095% 1.2% \n",
      "Partitioned to codelength 0.0797496267 + 3.41464679 = 3.494396414 in 97 (2 non-trivial) modules.\n",
      "\n",
      "=> Trial 1/1 finished in 0.000128875s with codelength 3.49439641\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 2 levels:\n",
      "Per level number of modules:         [         97,           0] (sum: 97)\n",
      "Per level number of leaf nodes:      [          0,         112] (sum: 112)\n",
      "Per level average child degree:      [         97,     1.15464] (average: 45.6379)\n",
      "Per level codelength for modules:    [0.079749627, 0.000000000] (sum: 0.079749627)\n",
      "Per level codelength for leaf nodes: [0.000000000, 3.414646787] (sum: 3.414646787)\n",
      "Per level codelength total:          [0.079749627, 3.414646787] (sum: 3.494396414)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2025-06-05 03:02:04\n",
      "  (Elapsed time: 0.000214s)\n",
      "===================================================\n",
      "[Saved] Infomap module assignments to network_analysis_count_based/member_infomap_volume_weighted/member_infomap_modules.csv\n",
      "\n",
      "Preparing visualization (Full Graph)...\n",
      "Calculating layout (Full Graph)...\n",
      "Drawing graph (Full Graph)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/7k6544kn35n00wd0kcwf51300000gn/T/ipykernel_14504/472869236.py:175: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  try: palette_im = cm.get_cmap(\"tab20\", num_colors_needed if num_colors_needed > 0 else 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving full plot to network_analysis_count_based/member_infomap_volume_weighted/member_infomap_vol_committee_color_FULL.png...\n",
      "[Saved] Visualization (Full Graph): network_analysis_count_based/member_infomap_volume_weighted/member_infomap_vol_committee_color_FULL.png\n",
      "\n",
      "Preparing visualization (Zoomed-in Graph)...\n",
      "Zoomed graph has 11 nodes and 37 edges.\n",
      "Calculating layout (Zoomed Graph)...\n",
      "Drawing graph (Zoomed Graph)...\n",
      "Saving zoomed plot to network_analysis_count_based/member_infomap_volume_weighted/member_infomap_vol_ZOOMED_cluster.png...\n",
      "[Saved] Visualization (Zoomed Graph): network_analysis_count_based/member_infomap_volume_weighted/member_infomap_vol_ZOOMED_cluster.png\n",
      "Member-Level Infomap Analysis finished.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Member-Level Infomap Network (Volume-Weighted Shared Sector Activity)\n",
    "\n",
    "try:\n",
    "    from infomap import Infomap\n",
    "except ImportError:\n",
    "    print(\"CRITICAL ERROR: 'infomap' library not found. Please install it (e.g., pip install infomap). This cell will be skipped.\")\n",
    "    # For now, we'll let it proceed and error out if Infomap is actually used without being imported.\n",
    "\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.patches as mpatches\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "\n",
    "# --- Assume these are defined in previous cells and are valid ---\n",
    "# COUNT_ANALYSIS_OUTPUT_DIR = \"your/output/directory/path\"\n",
    "# STOCK_TRANSACTIONS_DF = pd.DataFrame(...) # Make sure this is loaded\n",
    "# COMMITTEE_MEMBERSHIP_MAP = {...} # Make sure this is loaded\n",
    "# id2name = {...} # Optional global name map, make sure this is loaded if used\n",
    "\n",
    "# --- Configuration ---\n",
    "INFOMAP_OUTPUT_DIR = os.path.join(COUNT_ANALYSIS_OUTPUT_DIR, \"member_infomap_volume_weighted\")\n",
    "os.makedirs(INFOMAP_OUTPUT_DIR, exist_ok=True)\n",
    "SPARSIFY_PERCENTILE = 99\n",
    "\n",
    "# --- Ensure prerequisites are available ---\n",
    "if 'STOCK_TRANSACTIONS_DF' not in globals() or STOCK_TRANSACTIONS_DF.empty:\n",
    "    print(\"CRITICAL: STOCK_TRANSACTIONS_DF not available for Infomap analysis. Skipping.\")\n",
    "elif 'COMMITTEE_MEMBERSHIP_MAP' not in globals() or not COMMITTEE_MEMBERSHIP_MAP:\n",
    "    print(\"CRITICAL: COMMITTEE_MEMBERSHIP_MAP not available. Skipping.\")\n",
    "elif 'Infomap' not in globals(): # Check if Infomap class was successfully imported\n",
    "    print(\"CRITICAL: Infomap library could not be imported. Skipping Infomap analysis.\")\n",
    "else:\n",
    "    print(\"Starting Member-Level Infomap Analysis...\")\n",
    "    # A) member_id -> primary committee\n",
    "    member_to_committee_infomap = {}\n",
    "    for comm_im, period_map_im in COMMITTEE_MEMBERSHIP_MAP.items():\n",
    "        for members_im_set in period_map_im.values():\n",
    "            for m_im in members_im_set:\n",
    "                member_to_committee_infomap.setdefault(str(m_im), comm_im) # Ensure member ID is string\n",
    "    print(f\"Built member_to_committee_infomap with {len(member_to_committee_infomap)} entries.\")\n",
    "\n",
    "    # B) Aggregate member-sector VOLUME + lookup pretty names\n",
    "    mask_im = STOCK_TRANSACTIONS_DF[\"type\"].str.contains(\"sale|purchase\", case=False, na=False)\n",
    "    df_tx_im = STOCK_TRANSACTIONS_DF[mask_im].copy()\n",
    "    df_tx_im[\"member_id\"] = df_tx_im[\"member_id\"].astype(str) # Ensure member_id is string for consistency\n",
    "\n",
    "    id2name_im_local = {}\n",
    "    if 'id2name' in globals() and isinstance(id2name, dict) and id2name:\n",
    "        print(\"Using global id2name map.\")\n",
    "        id2name_im_local = id2name\n",
    "    elif 'member' in df_tx_im.columns:\n",
    "        print(\"Building local id2name from transactions 'member' column...\")\n",
    "        id2name_im_local = df_tx_im[[\"member_id\", \"member\"]].drop_duplicates(\"member_id\").set_index(\"member_id\")[\"member\"].to_dict()\n",
    "    else:\n",
    "        print(\"Warning: No global id2name and no 'member' column in transactions. Using member_id as name.\")\n",
    "        unique_member_ids = df_tx_im[\"member_id\"].unique()\n",
    "        id2name_im_local = {mid: mid for mid in unique_member_ids}\n",
    "    print(f\"Using id2name_im_local with {len(id2name_im_local)} entries.\")\n",
    "\n",
    "    print(\"Pivoting transaction data...\")\n",
    "    df_tx_im['amount'] = pd.to_numeric(df_tx_im['amount'], errors='coerce').fillna(0.0)\n",
    "    pivot_im = (df_tx_im.groupby([\"member_id\", \"sector\"])[\"amount\"]\n",
    "                  .sum().unstack(fill_value=0.0))\n",
    "    pivot_im.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    pivot_im.fillna(0.0, inplace=True)\n",
    "    print(f\"Pivot table shape: {pivot_im.shape}\")\n",
    "\n",
    "    members_in_pivot_with_committee = [idx for idx in pivot_im.index if idx in member_to_committee_infomap]\n",
    "    pivot_im = pivot_im.loc[members_in_pivot_with_committee]\n",
    "    members_list_im = pivot_im.index.tolist()\n",
    "    print(f\"Number of members with committee assignments and trades: {len(members_list_im)}\")\n",
    "\n",
    "    if not members_list_im:\n",
    "        print(\"No members with committee assignments and trades for Infomap analysis. Skipping.\")\n",
    "    else:\n",
    "        print(\"Building full member-member graph...\")\n",
    "        G_mem_im = nx.Graph()\n",
    "        G_mem_im.add_nodes_from(members_list_im)\n",
    "\n",
    "        for i, j in combinations(members_list_im, 2):\n",
    "            series_i_vals = pivot_im.loc[i].values\n",
    "            series_j_vals = pivot_im.loc[j].values\n",
    "            w_im = np.minimum(series_i_vals, series_j_vals).sum()\n",
    "            if np.isnan(w_im) or np.isinf(w_im):\n",
    "                # This should ideally not happen due to prior cleaning\n",
    "                # print(f\"Warning: Calculated weight for pair ({i}, {j}) is NaN or Inf. Skipping edge.\")\n",
    "                continue\n",
    "            if w_im > 0:\n",
    "                G_mem_im.add_edge(i, j, weight=w_im)\n",
    "        print(f\"Full member graph (Infomap): {G_mem_im.number_of_nodes()} nodes, {G_mem_im.number_of_edges()} edges.\")\n",
    "\n",
    "        def keep_edges_above_percentile_im(G, percentile_to_keep_above):\n",
    "            if not G.number_of_edges(): return G.copy()\n",
    "            weights = [d[\"weight\"] for _, _, d in G.edges(data=True)]\n",
    "            if not weights: return G.copy()\n",
    "            if percentile_to_keep_above == 0: threshold = -np.inf # Keep all\n",
    "            elif percentile_to_keep_above == 100: threshold = np.max(weights) if weights else np.inf # Keep only max weight edges\n",
    "            else: threshold = np.percentile(weights, percentile_to_keep_above)\n",
    "            H = nx.Graph()\n",
    "            H.add_nodes_from(G.nodes(data=True)) # Keep all nodes\n",
    "            for u, v, d_edge in G.edges(data=True):\n",
    "                if d_edge[\"weight\"] >= threshold: H.add_edge(u, v, **d_edge)\n",
    "            return H\n",
    "        \n",
    "        print(f\"Sparsifying graph, keeping edges with weights >= {SPARSIFY_PERCENTILE}th percentile...\")\n",
    "        G_mem_im_sparse = keep_edges_above_percentile_im(G_mem_im, SPARSIFY_PERCENTILE)\n",
    "        print(f\"Sparse member graph (Infomap): {G_mem_im_sparse.number_of_nodes()} nodes, {G_mem_im_sparse.number_of_edges()} edges.\")\n",
    "\n",
    "        if G_mem_im_sparse.number_of_edges() > 0:\n",
    "            print(\"Preparing data for Infomap...\")\n",
    "            # Add \"--silent\" to im_instance if you want to suppress Infomap's C++ console output\n",
    "            im_instance = Infomap(\"--two-level\") \n",
    "            \n",
    "            id_map_im_str_to_int = {mem_id_str: i for i, mem_id_str in enumerate(G_mem_im_sparse.nodes())}\n",
    "            id_map_im_int_to_str = {i: mem_id_str for mem_id_str, i in id_map_im_str_to_int.items()}\n",
    "\n",
    "            for mem_id_str, int_id in id_map_im_str_to_int.items():\n",
    "                im_instance.add_node(int_id, id2name_im_local.get(mem_id_str, mem_id_str))\n",
    "            print(f\"Added {im_instance.num_nodes} nodes to Infomap instance.\")\n",
    "            \n",
    "            link_count = 0\n",
    "            for u_str, v_str, edge_data in G_mem_im_sparse.edges(data=True):\n",
    "                u_int = id_map_im_str_to_int.get(u_str)\n",
    "                v_int = id_map_im_str_to_int.get(v_str)\n",
    "                if u_int is not None and v_int is not None:\n",
    "                    im_instance.add_link(u_int, v_int, edge_data[\"weight\"])\n",
    "                    link_count += 1\n",
    "                else:\n",
    "                    print(f\"Warning: Edge ({u_str}, {v_str}) refers to node(s) not in id_map_im_str_to_int. Skipping.\")\n",
    "            print(f\"Added {link_count} links to Infomap instance.\")\n",
    "\n",
    "            print(\"Running Infomap algorithm...\")\n",
    "            im_instance.run()\n",
    "            print(f\"Infomap run completed: Codelength L={im_instance.codelength:.3f} bits; Found {im_instance.num_top_modules} top-level modules.\")\n",
    "\n",
    "            modules_im = {}\n",
    "            print(\"Extracting module assignments from Infomap result...\")\n",
    "            for node_iterator_item in im_instance.nodes: \n",
    "                current_node_int_id = node_iterator_item.id()      \n",
    "                current_node_module_id = node_iterator_item.module_id  \n",
    "\n",
    "                if current_node_int_id in id_map_im_int_to_str:\n",
    "                    original_member_id_str = id_map_im_int_to_str[current_node_int_id]\n",
    "                    modules_im[original_member_id_str] = current_node_module_id\n",
    "                else:\n",
    "                    print(f\"Warning: Infomap node with integer id {current_node_int_id} not found in id_map_im_int_to_str. This is unexpected.\")\n",
    "            \n",
    "            if not modules_im and G_mem_im_sparse.number_of_nodes() > 0:\n",
    "                print(\"Warning: No modules were extracted, but graph had nodes. Check Infomap results and mapping logic.\")\n",
    "            elif modules_im:\n",
    "                print(f\"Successfully extracted {len(modules_im)} module assignments for {len(set(modules_im.values()))} unique modules.\")\n",
    "\n",
    "            if modules_im:\n",
    "                assignment_df_im = pd.DataFrame({\n",
    "                    \"member_id\": list(modules_im.keys()),\n",
    "                    \"member_name\": [id2name_im_local.get(mid_str, mid_str) for mid_str in modules_im.keys()],\n",
    "                    \"module_id\": list(modules_im.values()),\n",
    "                    \"committee\": [member_to_committee_infomap.get(mid_str, \"Unassigned\") for mid_str in modules_im.keys()]\n",
    "                })\n",
    "                assignment_filepath = os.path.join(INFOMAP_OUTPUT_DIR, \"member_infomap_modules.csv\")\n",
    "                assignment_df_im.to_csv(assignment_filepath, index=False)\n",
    "                print(f\"[Saved] Infomap module assignments to {assignment_filepath}\")\n",
    "\n",
    "                # --- Common data for both plots ---\n",
    "                nodes_to_draw_full = list(G_mem_im_sparse.nodes())\n",
    "                unique_comms_im = sorted(list(set(c for c in member_to_committee_infomap.values() if c is not None and c)))\n",
    "                if not unique_comms_im: unique_comms_im = [\"Unassigned\"]\n",
    "                \n",
    "                num_colors_needed = len(unique_comms_im)\n",
    "                try: palette_im = cm.get_cmap(\"tab20\", num_colors_needed if num_colors_needed > 0 else 1)\n",
    "                except ValueError:\n",
    "                    print(f\"Warning: Number of unique committees ({num_colors_needed}) exceeds tab20 palette size. Using 'viridis' colormap.\")\n",
    "                    palette_im = cm.get_cmap(\"viridis\", num_colors_needed if num_colors_needed > 0 else 1)\n",
    "                comm2color_im = {c: palette_im(i) for i, c in enumerate(unique_comms_im)}\n",
    "                comm2color_im.setdefault(\"Unassigned\", \"silver\")\n",
    "                \n",
    "                node_total_volumes = {mid_str: pivot_im.loc[mid_str].sum() for mid_str in G_mem_im_sparse.nodes() if mid_str in pivot_im.index}\n",
    "                default_volume = np.median(list(node_total_volumes.values())) if node_total_volumes else 1.0\n",
    "                if default_volume <= 0: default_volume = 1.0\n",
    "\n",
    "                # --- G) Visualise (Original Full Graph) ---\n",
    "                print(\"\\nPreparing visualization (Full Graph)...\")\n",
    "                \n",
    "                node_colors_full = [comm2color_im.get(member_to_committee_infomap.get(mid_str, \"Unassigned\"), \"silver\") for mid_str in nodes_to_draw_full]\n",
    "                \n",
    "                raw_sizes_full = [math.sqrt(max(1, node_total_volumes.get(mid_str, default_volume))) for mid_str in nodes_to_draw_full]\n",
    "                min_raw_size_f, max_raw_size_f = (min(raw_sizes_full), max(raw_sizes_full)) if raw_sizes_full else (1,1)\n",
    "                \n",
    "                node_sizes_final_full = []\n",
    "                if max_raw_size_f > min_raw_size_f : \n",
    "                    node_sizes_final_full = [50 + 950 * (rs - min_raw_size_f) / (max_raw_size_f - min_raw_size_f) for rs in raw_sizes_full]\n",
    "                elif raw_sizes_full: \n",
    "                    node_sizes_final_full = [200] * len(raw_sizes_full)\n",
    "                else: \n",
    "                    node_sizes_final_full = []\n",
    "\n",
    "                print(\"Calculating layout (Full Graph)...\")\n",
    "                k_val_full = 0.5 / math.sqrt(max(1, G_mem_im_sparse.number_of_nodes())) if G_mem_im_sparse.number_of_nodes() > 0 else 0.5\n",
    "                pos_full = nx.spring_layout(G_mem_im_sparse, seed=44, weight=\"weight\", k=k_val_full, iterations=30)\n",
    "                \n",
    "                edge_weights_data_full = [d['weight'] for _,_,d in G_mem_im_sparse.edges(data=True)]\n",
    "                max_w_full = max(edge_weights_data_full) if edge_weights_data_full else 1.0\n",
    "                if max_w_full == 0: max_w_full = 1.0\n",
    "                edge_widths_full = [(d[\"weight\"] / max_w_full) * 3.0 + 0.2 for _, _, d in G_mem_im_sparse.edges(data=True)]\n",
    "\n",
    "                print(\"Drawing graph (Full Graph)...\")\n",
    "                plt.figure(figsize=(18, 15))\n",
    "                nx.draw_networkx_edges(G_mem_im_sparse, pos_full, width=edge_widths_full, alpha=0.15, edge_color=\"#888888\")\n",
    "                \n",
    "                if len(node_sizes_final_full) != len(nodes_to_draw_full):\n",
    "                    print(f\"Warning (Full Graph): Mismatch in node_sizes_final_full ({len(node_sizes_final_full)}) and nodes_to_draw_full ({len(nodes_to_draw_full)}). Using default size.\")\n",
    "                    node_sizes_final_full = 100\n",
    "                nx.draw_networkx_nodes(G_mem_im_sparse, pos_full, nodelist=nodes_to_draw_full, node_color=node_colors_full, \n",
    "                                       node_size=node_sizes_final_full, alpha=0.85, edgecolors='black', linewidths=0.3)\n",
    "\n",
    "                labels_to_draw_full = {}\n",
    "                if node_sizes_final_full and nodes_to_draw_full:\n",
    "                    node_id_to_size_map_f = {node_id: size for node_id, size in zip(nodes_to_draw_full, node_sizes_final_full)}\n",
    "                    sorted_nodes_by_size_f = sorted(node_id_to_size_map_f.keys(), key=lambda nid: node_id_to_size_map_f[nid], reverse=True)\n",
    "                    num_labels_f = min(20, int(len(sorted_nodes_by_size_f) * 0.05) + 1) \n",
    "                    nodes_to_label_ids_f = sorted_nodes_by_size_f[:num_labels_f]\n",
    "                    labels_to_draw_full = {nid: id2name_im_local.get(nid, nid) for nid in nodes_to_label_ids_f}\n",
    "                if labels_to_draw_full:\n",
    "                    nx.draw_networkx_labels(G_mem_im_sparse, pos_full, labels=labels_to_draw_full, font_size=8, font_weight='normal',\n",
    "                                            bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.1'))\n",
    "\n",
    "                plt.title(\"Member-Level Trading Network (Shared Sector Volume, Sparsified)\\nColor: Committee | Node Size: Total Trading Volume | Communities by Infomap\", pad=20, fontsize=16)\n",
    "                handles_im_full = [mpatches.Patch(color=comm2color_im.get(c, \"silver\"), label=str(c)) for c in unique_comms_im]\n",
    "                plt.legend(handles=handles_im_full, title=\"Committees\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", frameon=False, fontsize=10)\n",
    "                plt.axis(\"off\"); plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "                \n",
    "                out_png_full = os.path.join(INFOMAP_OUTPUT_DIR, \"member_infomap_vol_committee_color_FULL.png\") # Added _FULL\n",
    "                print(f\"Saving full plot to {out_png_full}...\")\n",
    "                plt.savefig(out_png_full, dpi=250, bbox_inches='tight'); plt.close()\n",
    "                print(f\"[Saved] Visualization (Full Graph): {out_png_full}\")\n",
    "\n",
    "                # --- G.2) Visualise (Zoomed-in on Central Cluster) ---\n",
    "                print(\"\\nPreparing visualization (Zoomed-in Graph)...\")\n",
    "                \n",
    "                target_node_name = \"Josh Gottheimer\" \n",
    "                target_node_id = None\n",
    "                for mid, name in id2name_im_local.items():\n",
    "                    if name == target_node_name:\n",
    "                        target_node_id = mid\n",
    "                        break\n",
    "                \n",
    "                if target_node_id and target_node_id in G_mem_im_sparse:\n",
    "                    nodes_for_zoom = {target_node_id}\n",
    "        \n",
    "                    for neighbor in G_mem_im_sparse.neighbors(target_node_id):\n",
    "                        nodes_for_zoom.add(neighbor)\n",
    "\n",
    "                    G_zoom = G_mem_im_sparse.subgraph(list(nodes_for_zoom)).copy()\n",
    "\n",
    "                    if G_zoom.number_of_nodes() > 1 and G_zoom.number_of_edges() >= 0: # Allow 0 edges if it's a star around central\n",
    "                        print(f\"Zoomed graph has {G_zoom.number_of_nodes()} nodes and {G_zoom.number_of_edges()} edges.\")\n",
    "\n",
    "                        node_colors_zoom = [comm2color_im.get(member_to_committee_infomap.get(mid_str, \"Unassigned\"), \"silver\") \n",
    "                                            for mid_str in G_zoom.nodes()]\n",
    "                        \n",
    "                        raw_sizes_zoom = [math.sqrt(max(1, node_total_volumes.get(mid_str, default_volume))) \n",
    "                                          for mid_str in G_zoom.nodes()]\n",
    "                        min_raw_size_z, max_raw_size_z = (min(raw_sizes_zoom), max(raw_sizes_zoom)) if raw_sizes_zoom else (1,1)\n",
    "                        \n",
    "                        node_sizes_final_zoom = []\n",
    "                        # Make nodes larger in zoomed plot\n",
    "                        min_zoom_size, max_zoom_size = 200, 2500 \n",
    "                        if max_raw_size_z > min_raw_size_z :\n",
    "                            node_sizes_final_zoom = [min_zoom_size + (max_zoom_size - min_zoom_size) * (rs - min_raw_size_z) / (max_raw_size_z - min_raw_size_z) for rs in raw_sizes_zoom]\n",
    "                        elif raw_sizes_zoom:\n",
    "                            node_sizes_final_zoom = [ (min_zoom_size + max_zoom_size) / 2 ] * len(raw_sizes_zoom) # Mid-range size\n",
    "                        else:\n",
    "                            node_sizes_final_zoom = []\n",
    "\n",
    "                        print(\"Calculating layout (Zoomed Graph)...\")\n",
    "                        k_val_zoom = 0.9 / math.sqrt(max(1, G_zoom.number_of_nodes())) if G_zoom.number_of_nodes() > 0 else 0.9\n",
    "                        pos_zoom = nx.spring_layout(G_zoom, seed=43, weight=\"weight\", k=k_val_zoom, iterations=100) # Different seed, more iterations\n",
    "\n",
    "                        edge_weights_data_zoom = [d['weight'] for _,_,d in G_zoom.edges(data=True)]\n",
    "                        max_w_zoom = max(edge_weights_data_zoom) if edge_weights_data_zoom else 1.0\n",
    "                        if max_w_zoom == 0: max_w_zoom = 1.0\n",
    "                        edge_widths_zoom = [(d[\"weight\"] / max_w_zoom) * 4.0 + 0.5 for _, _, d in G_zoom.edges(data=True)]\n",
    "\n",
    "                        print(\"Drawing graph (Zoomed Graph)...\")\n",
    "                        plt.figure(figsize=(14, 12)) \n",
    "                        nx.draw_networkx_edges(G_zoom, pos_zoom, width=edge_widths_zoom, alpha=0.25, edge_color=\"#666666\")\n",
    "                        \n",
    "                        nx.draw_networkx_nodes(G_zoom, pos_zoom, nodelist=list(G_zoom.nodes()), node_color=node_colors_zoom, \n",
    "                                               node_size=node_sizes_final_zoom, alpha=0.9, edgecolors='black', linewidths=0.4)\n",
    "                        \n",
    "                        labels_zoom = {nid: id2name_im_local.get(nid, nid) for nid in G_zoom.nodes()}\n",
    "                        nx.draw_networkx_labels(G_zoom, pos_zoom, labels=labels_zoom, font_size=10, font_weight='bold', # Larger font\n",
    "                                                bbox=dict(facecolor='white', alpha=0.6, edgecolor='none', boxstyle='round,pad=0.2'))\n",
    "\n",
    "                        plt.title(f\"Zoomed: Cluster around {target_node_name}\\n(Member-Level Trading Network)\", pad=15, fontsize=16)\n",
    "                        plt.axis(\"off\")\n",
    "                        plt.tight_layout()\n",
    "                        \n",
    "                        out_png_zoom = os.path.join(INFOMAP_OUTPUT_DIR, \"member_infomap_vol_ZOOMED_cluster.png\")\n",
    "                        print(f\"Saving zoomed plot to {out_png_zoom}...\")\n",
    "                        plt.savefig(out_png_zoom, dpi=250, bbox_inches='tight'); plt.close()\n",
    "                        print(f\"[Saved] Visualization (Zoomed Graph): {out_png_zoom}\")\n",
    "                    else:\n",
    "                        print(f\"Zoomed subgraph for '{target_node_name}' is too small or has no edges. Skipping zoomed plot.\")\n",
    "                else:\n",
    "                    print(f\"Target node '{target_node_name}' not found or not in graph. Skipping zoomed plot.\")\n",
    "            else:\n",
    "                print(\"Infomap modules could not be determined or no modules found. Skipping assignments saving and plot.\")\n",
    "        else:\n",
    "            print(\"Sparsified graph for Infomap has no edges. Skipping Infomap run and visualization.\")\n",
    "    print(\"Member-Level Infomap Analysis finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2f1c07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Committee vs. Infomap Module Crosstab ---\n",
      "module_id                                      1  2  3  4  5  6  7  8  9  10  \\\n",
      "committee                                                                      \n",
      "Senate Finance                                 2  0  0  0  0  0  0  0  0   0   \n",
      "Senate Health, Education, Labor, and Pensions  2  0  0  0  0  0  0  0  0   0   \n",
      "House Ways and Means                           3  0  0  0  0  0  0  0  0   0   \n",
      "House Oversight and Reform                     1  2  0  0  0  0  0  0  0   0   \n",
      "House Financial Services                       1  2  0  0  0  0  0  0  1   0   \n",
      "Senate Appropriations                          0  0  0  0  0  0  0  0  0   0   \n",
      "Senate Banking, Housing, and Urban Affairs     1  0  0  0  0  0  0  0  0   0   \n",
      "House Energy and Commerce                      2  0  0  1  1  1  1  0  0   0   \n",
      "House Appropriations                           1  0  1  0  0  0  0  1  0   1   \n",
      "\n",
      "module_id                                      ...  89  90  91  92  93  94  \\\n",
      "committee                                      ...                           \n",
      "Senate Finance                                 ...   0   0   0   0   0   0   \n",
      "Senate Health, Education, Labor, and Pensions  ...   1   0   0   0   0   0   \n",
      "House Ways and Means                           ...   0   0   0   0   0   0   \n",
      "House Oversight and Reform                     ...   0   0   0   0   0   0   \n",
      "House Financial Services                       ...   0   0   1   0   0   0   \n",
      "Senate Appropriations                          ...   0   0   0   0   0   0   \n",
      "Senate Banking, Housing, and Urban Affairs     ...   0   1   0   0   0   0   \n",
      "House Energy and Commerce                      ...   0   0   0   1   0   1   \n",
      "House Appropriations                           ...   0   0   0   0   1   0   \n",
      "\n",
      "module_id                                      95  96  97  \\\n",
      "committee                                                   \n",
      "Senate Finance                                  0   0   0   \n",
      "Senate Health, Education, Labor, and Pensions   0   0   0   \n",
      "House Ways and Means                            0   0   0   \n",
      "House Oversight and Reform                      0   0   0   \n",
      "House Financial Services                        0   0   0   \n",
      "Senate Appropriations                           0   0   0   \n",
      "Senate Banking, Housing, and Urban Affairs      0   0   0   \n",
      "House Energy and Commerce                       0   0   1   \n",
      "House Appropriations                            1   1   0   \n",
      "\n",
      "module_id                                      dominant_module_proportion  \n",
      "committee                                                                  \n",
      "Senate Finance                                                   0.400000  \n",
      "Senate Health, Education, Labor, and Pensions                    0.285714  \n",
      "House Ways and Means                                             0.176471  \n",
      "House Oversight and Reform                                       0.166667  \n",
      "House Financial Services                                         0.153846  \n",
      "Senate Appropriations                                            0.142857  \n",
      "Senate Banking, Housing, and Urban Affairs                       0.142857  \n",
      "House Energy and Commerce                                        0.076923  \n",
      "House Appropriations                                             0.055556  \n",
      "\n",
      "[9 rows x 98 columns]\n",
      "[Saved] Crosstab to network_analysis_count_based/member_infomap_volume_weighted/committee_vs_infomap_module_crosstab.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Crosstab of Committee vs. Infomap Module\n",
    "\n",
    "if 'assignment_df_im' in globals() and not assignment_df_im.empty:\n",
    "    print(\"\\n--- Committee vs. Infomap Module Crosstab ---\")\n",
    "    # Ensure 'committee' and 'module_id' columns exist\n",
    "    if 'committee' in assignment_df_im.columns and 'module_id' in assignment_df_im.columns:\n",
    "        ct_im = pd.crosstab(assignment_df_im['committee'], assignment_df_im['module_id'])\n",
    "        \n",
    "        # Calculate row-wise proportion of the largest module for each committee\n",
    "        # Avoid division by zero if a committee row sum is 0 (though unlikely if members were assigned)\n",
    "        row_sums_im = ct_im.sum(axis=1)\n",
    "        # ct_im['row_prop'] = ct_im.max(axis=1).divide(row_sums_im, axis=0).fillna(0) # Old way\n",
    "        \n",
    "        # A more robust way to get the proportion of the dominant module\n",
    "        dominant_module_prop = []\n",
    "        for index, row in ct_im.iterrows():\n",
    "            if row.sum() > 0:\n",
    "                dominant_module_prop.append(row.max() / row.sum())\n",
    "            else:\n",
    "                dominant_module_prop.append(0)\n",
    "        ct_im['dominant_module_proportion'] = dominant_module_prop\n",
    "\n",
    "        ct_im_sorted = ct_im.sort_values('dominant_module_proportion', ascending=False)\n",
    "        print(ct_im_sorted)\n",
    "        \n",
    "        # Save to CSV\n",
    "        ct_im_sorted.to_csv(os.path.join(INFOMAP_OUTPUT_DIR, \"committee_vs_infomap_module_crosstab.csv\"))\n",
    "        print(f\"[Saved] Crosstab to {INFOMAP_OUTPUT_DIR}/committee_vs_infomap_module_crosstab.csv\")\n",
    "    else:\n",
    "        print(\"Warning: 'committee' or 'module_id' not found in assignment_df_im. Skipping crosstab.\")\n",
    "else:\n",
    "    print(\"Skipping Committee vs. Infomap Module Crosstab: 'assignment_df_im' not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "69d4516e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Committee Assortativity ---\n",
      "Committee assortativity r = -0.109\n",
      "[Saved] Assortativity to network_analysis_count_based/member_infomap_volume_weighted/committee_assortativity.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 14: Attribute Assortativity\n",
    "\n",
    "if 'G_mem_im_sparse' in globals() and G_mem_im_sparse.number_of_nodes() > 0:\n",
    "    print(\"\\n--- Committee Assortativity ---\")\n",
    "    # Ensure 'member_to_committee_infomap' is used, as it's scoped for this Infomap block\n",
    "    comm_attr_im = {m: member_to_committee_infomap.get(m, 'Unassigned_Comm') for m in G_mem_im_sparse.nodes()}\n",
    "    nx.set_node_attributes(G_mem_im_sparse, comm_attr_im, 'committee_attr') # Use a unique attribute name\n",
    "    \n",
    "    if G_mem_im_sparse.number_of_edges() > 0:\n",
    "        try:\n",
    "            r_assortativity = nx.attribute_assortativity_coefficient(G_mem_im_sparse, 'committee_attr')\n",
    "            print(f\"Committee assortativity r = {r_assortativity:.3f}\")\n",
    "            \n",
    "            # Save to a file\n",
    "            with open(os.path.join(INFOMAP_OUTPUT_DIR, \"committee_assortativity.txt\"), \"w\") as f:\n",
    "                f.write(f\"Committee assortativity r = {r_assortativity:.3f}\\n\")\n",
    "            print(f\"[Saved] Assortativity to {INFOMAP_OUTPUT_DIR}/committee_assortativity.txt\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate assortativity: {e}\")\n",
    "    else:\n",
    "        print(\"Graph has no edges, cannot calculate assortativity.\")\n",
    "else:\n",
    "    print(\"Skipping Assortativity: Infomap graph 'G_mem_im_sparse' not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "87b8ae92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Top Bridges (Edge Betweenness Centrality) ---\n",
      "Top 15 Bridges (by betweenness centrality):\n",
      "Trey Hollingsworth (H001074) ↔ Kevin Hern (H001082): 0.0032 (Committees: House Financial Services | House Ways and Means)\n",
      "Kevin Hern (H001082) ↔ Kenny Marchant (M001158): 0.0026 (Committees: House Ways and Means | House Ways and Means)\n",
      "Kevin Hern (H001082) ↔ John W. Hickenlooper (S408): 0.0023 (Committees: House Ways and Means | Senate Health, Education, Labor, and Pensions)\n",
      "Greg Gianforte (G000584) ↔ Van Taylor (T000479): 0.0021 (Committees: House Energy and Commerce | House Financial Services)\n",
      "Kevin Hern (H001082) ↔ Susie Lee (L000590): 0.0019 (Committees: House Ways and Means | House Appropriations)\n",
      "Mark E. Green (G000590) ↔ Trey Hollingsworth (H001074): 0.0018 (Committees: House Oversight and Reform | House Financial Services)\n",
      "Kevin Hern (H001082) ↔ Thomas R. Carper (S277): 0.0018 (Committees: House Ways and Means | Senate Finance)\n",
      "Josh Gottheimer (G000583) ↔ John W. Hickenlooper (S408): 0.0016 (Committees: House Financial Services | Senate Health, Education, Labor, and Pensions)\n",
      "Kevin Hern (H001082) ↔ Ron Wyden (S247): 0.0016 (Committees: House Ways and Means | Senate Finance)\n",
      "Virginia Foxx (F000450) ↔ Tommy Tuberville (S412): 0.0014 (Committees: House Oversight and Reform | Senate Health, Education, Labor, and Pensions)\n",
      "Kevin Hern (H001082) ↔ Kim Schrier (S001216): 0.0014 (Committees: House Ways and Means | House Energy and Commerce)\n",
      "Suzan K. DelBene (D000617) ↔ Greg Gianforte (G000584): 0.0013 (Committees: House Ways and Means | House Energy and Commerce)\n",
      "Greg Gianforte (G000584) ↔ John W. Hickenlooper (S408): 0.0013 (Committees: House Energy and Commerce | Senate Health, Education, Labor, and Pensions)\n",
      "Kevin Hern (H001082) ↔ Bill Hagerty (S407): 0.0013 (Committees: House Ways and Means | Senate Banking, Housing, and Urban Affairs)\n",
      "Susie Lee (L000590) ↔ Tommy Tuberville (S412): 0.0013 (Committees: House Appropriations | Senate Health, Education, Labor, and Pensions)\n",
      "[Saved] Top bridges to network_analysis_count_based/member_infomap_volume_weighted/top_bridges.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Top Bridges\n",
    "\n",
    "if 'G_mem_im_sparse' in globals() and G_mem_im_sparse.number_of_edges() > 0 : # Needs edges\n",
    "    print(\"\\n--- Top Bridges (Edge Betweenness Centrality) ---\")\n",
    "    try:\n",
    "        # Note: Edge betweenness can be slow on larger graphs.\n",
    "        # If G_mem_im_sparse is very large, consider sampling or a faster approximation.\n",
    "        bridges_im = nx.edge_betweenness_centrality(G_mem_im_sparse, weight='weight', normalized=True)\n",
    "        \n",
    "        if bridges_im: # Check if bridges dictionary is not empty\n",
    "            top_bridges_im = sorted(bridges_im.items(), key=lambda x: -x[1])[:15] # Top 15\n",
    "            print(f\"Top {len(top_bridges_im)} Bridges (by betweenness centrality):\")\n",
    "            \n",
    "            bridge_data_to_save = []\n",
    "            for (u_br, v_br), bc_br in top_bridges_im:\n",
    "                comm_u = member_to_committee_infomap.get(u_br, \"N/A\")\n",
    "                comm_v = member_to_committee_infomap.get(v_br, \"N/A\")\n",
    "                print(f\"{id2name_im_local.get(u_br, u_br)} ({u_br}) ↔ {id2name_im_local.get(v_br, v_br)} ({v_br}): {bc_br:.4f} (Committees: {comm_u} | {comm_v})\")\n",
    "                bridge_data_to_save.append({\n",
    "                    \"member1_id\": u_br, \"member1_name\": id2name_im_local.get(u_br, u_br), \"member1_committee\": comm_u,\n",
    "                    \"member2_id\": v_br, \"member2_name\": id2name_im_local.get(v_br, v_br), \"member2_committee\": comm_v,\n",
    "                    \"betweenness\": bc_br\n",
    "                })\n",
    "            \n",
    "            pd.DataFrame(bridge_data_to_save).to_csv(os.path.join(INFOMAP_OUTPUT_DIR, \"top_bridges.csv\"), index=False)\n",
    "            print(f\"[Saved] Top bridges to {INFOMAP_OUTPUT_DIR}/top_bridges.csv\")\n",
    "        else:\n",
    "            print(\"No bridge edges found (edge_betweenness_centrality returned empty).\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Could not calculate edge betweenness centrality: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Top Bridges: Infomap graph 'G_mem_im_sparse' not available or has no edges.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7da8a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Infomap on Committee-Committee Graph (Volume Overlap) ---\n",
      "Committee-Level Infomap: L=3.121 bits; modules=1\n",
      "\n",
      "Committee-Level Infomap Modules (Volume Overlap):\n",
      "  Module 1 (10 committees): General Population, House Appropriations, House Energy and Commerce, House Financial Services, House Oversight and Reform, House Ways and Means, Senate Appropriations, Senate Banking, Housing, and Urban Affairs, Senate Finance, Senate Health, Education, Labor, and Pensions\n",
      "=======================================================\n",
      "  Infomap v2.8.0 starts at 2025-06-05 03:02:05\n",
      "  -> Input network: \n",
      "  -> No file output!\n",
      "  -> Configuration: two-level\n",
      "=======================================================\n",
      "  -> Ordinary network input, using the Map Equation for first order network flows\n",
      "Calculating global network flow using flow model 'undirected'... \n",
      "  -> Using undirected links.\n",
      "  => Sum node flow: 1, sum link flow: 1\n",
      "Build internal network with 10 nodes and 45 links...\n",
      "  -> One-level codelength: 3.12089394\n",
      "\n",
      "================================================\n",
      "Trial 1/1 starting at 2025-06-05 03:02:05\n",
      "================================================\n",
      "Two-level compression: -1.8e-13% \n",
      "Partitioned to codelength 0 + 3.12089394 = 3.120893938 in 1 (0 non-trivial) modules.\n",
      "\n",
      "=> Trial 1/1 finished in 2.5333e-05s with codelength 3.12089394\n",
      "\n",
      "\n",
      "================================================\n",
      "Summary after 1 trial\n",
      "================================================\n",
      "Best end modular solution in 2 levels (warning: worse than one-level solution):\n",
      "Per level number of modules:         [          1,           0] (sum: 1)\n",
      "Per level number of leaf nodes:      [          0,          10] (sum: 10)\n",
      "Per level average child degree:      [          1,          10] (average: 9.18182)\n",
      "Per level codelength for modules:    [0.000000000, 0.000000000] (sum: 0.000000000)\n",
      "Per level codelength for leaf nodes: [0.000000000, 3.120893938] (sum: 3.120893938)\n",
      "Per level codelength total:          [0.000000000, 3.120893938] (sum: 3.120893938)\n",
      "\n",
      "===================================================\n",
      "  Infomap ends at 2025-06-05 03:02:05\n",
      "  (Elapsed time: 7.2666e-05s)\n",
      "===================================================\n",
      "[Saved] Committee Infomap modules to network_analysis_volume_based/committee_level_infomap/committee_infomap_modules_volume.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] network_analysis_volume_based/committee_level_infomap/committee_projection_infomap_volume.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/7k6544kn35n00wd0kcwf51300000gn/T/ipykernel_14504/837260771.py:56: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  palette_cl = cm.get_cmap(\"tab20\", len(module_ids_cl_list) if len(module_ids_cl_list) > 0 else 1)\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Committee-Level Infomap (using Committee Overlap Edges)\n",
    "\n",
    "# This uses G_overlap_vol (committee-committee graph based on shared sector VOLUME)\n",
    "# Ensure G_overlap_vol is defined and populated from Cell 10 (Volume-based analysis)\n",
    "\n",
    "INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR = os.path.join(VOLUME_ANALYSIS_OUTPUT_DIR, \"committee_level_infomap\")\n",
    "os.makedirs(INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "if 'G_overlap_vol' in globals() and G_overlap_vol.number_of_nodes() > 0 and G_overlap_vol.number_of_edges() > 0:\n",
    "    print(\"\\n--- Infomap on Committee-Committee Graph (Volume Overlap) ---\")\n",
    "    try:\n",
    "        im_comm_level = Infomap(\"--two-level\")\n",
    "        id_map_comm_level = {} # committee_name -> integer_id\n",
    "        next_id_comm_level = 1\n",
    "\n",
    "        for node_cl in G_overlap_vol.nodes():\n",
    "            id_map_comm_level[node_cl] = next_id_comm_level\n",
    "            im_comm_level.add_node(next_id_comm_level, node_cl) # Use committee name as Infomap node name\n",
    "            next_id_comm_level += 1\n",
    "        \n",
    "        for u_cl, v_cl, d_cl in G_overlap_vol.edges(data=True):\n",
    "            weight_cl = d_cl.get(\"weight\", 1.0) # Default to 1.0 if weight is missing\n",
    "            if weight_cl > 0 : # Infomap usually expects positive weights\n",
    "                 im_comm_level.add_link(id_map_comm_level[u_cl], id_map_comm_level[v_cl], weight_cl)\n",
    "\n",
    "        im_comm_level.run()\n",
    "        print(f\"Committee-Level Infomap: L={im_comm_level.codelength:.3f} bits; modules={im_comm_level.num_top_modules}\")\n",
    "\n",
    "        inv_id_map_cl = {v: k for k, v in id_map_comm_level.items()}\n",
    "        modules_cl = {}\n",
    "        for node_iterator_item_cl in im_comm_level.nodes:\n",
    "            node_int_id_cl = node_iterator_item_cl.id()      # Call id() as a method\n",
    "            node_module_id_cl = node_iterator_item_cl.module_id # Access module_id as a property\n",
    "            if node_int_id_cl in inv_id_map_cl:\n",
    "                committee_name = inv_id_map_cl[node_int_id_cl]\n",
    "                modules_cl[committee_name] = node_module_id_cl\n",
    "            else:\n",
    "                print(f\"Warning (Committee-Level): Infomap node with int_id {node_int_id_cl} not in inv_id_map_cl.\")\n",
    "\n",
    "        # Save and Print Modules\n",
    "        comm_module_data = []\n",
    "        print(\"\\nCommittee-Level Infomap Modules (Volume Overlap):\")\n",
    "        for mod_id_cl in sorted(set(modules_cl.values())):\n",
    "            members_in_mod_cl = [comm_name for comm_name, m_id in modules_cl.items() if m_id == mod_id_cl]\n",
    "            print(f\"  Module {mod_id_cl} ({len(members_in_mod_cl)} committees): {', '.join(sorted(members_in_mod_cl))}\")\n",
    "            for comm_name in sorted(members_in_mod_cl):\n",
    "                comm_module_data.append({\"committee\": comm_name, \"infomap_module_id\": mod_id_cl})\n",
    "        \n",
    "        pd.DataFrame(comm_module_data).to_csv(os.path.join(INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR, \"committee_infomap_modules_volume.csv\"), index=False)\n",
    "        print(f\"[Saved] Committee Infomap modules to {INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR}/committee_infomap_modules_volume.csv\")\n",
    "\n",
    "\n",
    "        # Visualization (Color nodes by their Infomap module)\n",
    "        module_ids_cl_list = sorted(list(set(modules_cl.values())))\n",
    "        palette_cl = cm.get_cmap(\"tab20\", len(module_ids_cl_list) if len(module_ids_cl_list) > 0 else 1)\n",
    "        node_colors_cl = [palette_cl(module_ids_cl_list.index(modules_cl.get(n,-1))) for n in G_overlap_vol.nodes()]\n",
    "        \n",
    "        # Node sizes can be based on original eigenvector centrality from G_overlap_vol if available\n",
    "        node_sizes_cl = [eig_cent_comm_v.get(n, 0.01) * 8000 + 200 if 'eig_cent_comm_v' in globals() else 1000 for n in G_overlap_vol.nodes()]\n",
    "\n",
    "\n",
    "        pos_cl = nx.spring_layout(G_overlap_vol, seed=42, weight=\"weight\", k=0.7, iterations=50)\n",
    "        edge_weights_cl_data = [d.get('weight',1.0) for _,_,d in G_overlap_vol.edges(data=True)]\n",
    "        max_w_cl = max(edge_weights_cl_data) if edge_weights_cl_data else 1.0\n",
    "        if max_w_cl == 0: max_w_cl = 1.0\n",
    "        edge_widths_cl = [(d.get(\"weight\",1.0) / max_w_cl) * 5 + 0.5 for _, _, d in G_overlap_vol.edges(data=True)]\n",
    "\n",
    "        plt.figure(figsize=(13, 11))\n",
    "        nx.draw_networkx_edges(G_overlap_vol, pos_cl, width=edge_widths_cl, alpha=0.3, edge_color=\"#888888\")\n",
    "        nx.draw_networkx_nodes(G_overlap_vol, pos_cl, node_color=node_colors_cl, node_size=node_sizes_cl, alpha=0.9, edgecolors='black', linewidths=0.5)\n",
    "        nx.draw_networkx_labels(G_overlap_vol, pos_cl, font_size=8, font_weight='normal')\n",
    "        plt.title(\"Committee Network (Shared Sector Volume) - Infomap Communities\", fontsize=14, pad=15)\n",
    "        \n",
    "        # Create legend for modules\n",
    "        handles_cl = [mpatches.Patch(color=palette_cl(module_ids_cl_list.index(mod_id)), label=f\"Module {mod_id}\") for mod_id in module_ids_cl_list]\n",
    "        if handles_cl:\n",
    "            plt.legend(handles=handles_cl, title=\"Infomap Modules\", bbox_to_anchor=(1.02, 1), loc=\"upper left\", fontsize=9, title_fontsize=10)\n",
    "        \n",
    "        plt.axis(\"off\"); plt.tight_layout(rect=[0,0,0.85,1])\n",
    "        plt.savefig(os.path.join(INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR, \"committee_projection_infomap_volume.png\"), dpi=200)\n",
    "        plt.close()\n",
    "        print(f\"[Saved] {INFOMAP_COMMITTEE_LEVEL_OUTPUT_DIR}/committee_projection_infomap_volume.png\")\n",
    "\n",
    "    except NameError as e: # Catch if Infomap is not imported\n",
    "        print(f\"Skipping Committee-Level Infomap due to missing library or error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Committee-Level Infomap: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Committee-Level Infomap: 'G_overlap_vol' (committee-committee graph from volume) not available or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "54c14f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOCK_TRANSACTIONS_DF shape: (17043, 16), COMMITTEE_MEETINGS_DF shape: (2763, 9), len(COMMITTEE_MEMBERSHIP_MAP): 9, len(CONGRESS_PERIOD_MAP): 2\n",
      "\n",
      "--- Starting Meeting Day Bipartite Network Analysis (volume_net-weighted) ---\n",
      "Window: 12 days before, 3 days after meeting.\n",
      "Processing 2431 unique committee-meeting-day events...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating trades around meetings: 100%|██████████| 2431/2431 [00:03<00:00, 767.69it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 8401 detailed committee-meeting-day <-> sector links.\n",
      "[Saved] Detailed links with contributors to: meeting_day_sector_bipartite_analysis_v3_detailed/meeting_day_sector_links_detailed_volume_net.csv\n",
      "Meeting Day Bipartite Graph (for viz): 1065 nodes, 5379 edges.\n",
      "Plotting filtered graph: 26 nodes, 77 edges.\n",
      "Saved final bipartite meeting day plot: meeting_day_sector_bipartite_analysis_v3_detailed/meeting_days_sectors_bipartite_final_volume_net.png\n",
      "\n",
      "--- Detailed Analysis of Top Plotted Committee-Meeting-Days ---\n",
      "\n",
      "  - Committee-Day: House Ways and Means on 2022-02-15 (Degree in plot: 9)\n",
      "    Associated Meeting Titles:\n",
      "      1. Examining the Economic Impact of Federal Infrastructure Investment\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Technology                | Volume_net: 15,051,000 | Top Contributors: Suzan K. DelBene (15,000,000), Earl Blumenauer (35,000), Ron Estes (8,000)\n",
      "      * Sector: Public Utilities          | Volume_net:     32,000 | Top Contributors: Earl Blumenauer (16,000), Ron Estes (8,000), Kevin Hern (8,000)\n",
      "      * Sector: Health Care               | Volume_net:     24,000 | Top Contributors: Earl Blumenauer (24,000)\n",
      "      * Sector: Energy                    | Volume_net:    -19,000 | Top Contributors: Earl Blumenauer (-35,000), Ron Estes (8,000), Kevin Hern (8,000)\n",
      "      * Sector: Basic Industries          | Volume_net:      8,000 | Top Contributors: Lloyd Doggett (8,000)\n",
      "      * Sector: Consumer Services         | Volume_net:      8,000 | Top Contributors: Earl Blumenauer (8,000)\n",
      "      * Sector: Finance                   | Volume_net:      8,000 | Top Contributors: Ron Estes (8,000), Earl Blumenauer (0)\n",
      "      * Sector: Miscellaneous             | Volume_net:      8,000 | Top Contributors: Kevin Hern (8,000)\n",
      "      * Sector: Telecommunications        | Volume_net:      8,000 | Top Contributors: Earl Blumenauer (8,000)\n",
      "\n",
      "  - Committee-Day: House Ways and Means on 2021-09-14 (Degree in plot: 10)\n",
      "    Associated Meeting Titles:\n",
      "      1. Legislative proposals to comply with the reconciliation directive included in section 2002 of the Concurrent Resolution \n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Technology                | Volume_net: -14,984,000 | Top Contributors: Suzan K. DelBene (-15,000,000), Lloyd Doggett (8,000), Kevin Hern (8,000)\n",
      "      * Sector: Capital Goods             | Volume_net:    -54,000 | Top Contributors: Earl Blumenauer (-70,000), Kevin Hern (16,000)\n",
      "      * Sector: Unspecified               | Volume_net:    -35,000 | Top Contributors: Thomas Suozzi (-35,000)\n",
      "      * Sector: Finance                   | Volume_net:     24,000 | Top Contributors: Kevin Hern (24,000)\n",
      "      * Sector: Consumer Services         | Volume_net:     16,000 | Top Contributors: Lloyd Doggett (8,000), Kevin Hern (8,000)\n",
      "      * Sector: Energy                    | Volume_net:     16,000 | Top Contributors: Kevin Hern (16,000)\n",
      "      * Sector: Basic Industries          | Volume_net:      8,000 | Top Contributors: Lloyd Doggett (8,000)\n",
      "      * Sector: Health Care               | Volume_net:      8,000 | Top Contributors: Kevin Hern (8,000)\n",
      "      * Sector: Public Utilities          | Volume_net:      8,000 | Top Contributors: Kevin Hern (8,000)\n",
      "      * Sector: Consumer Discretionary    | Volume_net:     -3,000 | Top Contributors: Thomas Suozzi (-35,000), Earl Blumenauer (32,000)\n",
      "\n",
      "  - Committee-Day: House Financial Services on 2022-12-01 (Degree in plot: 9)\n",
      "    Associated Meeting Titles:\n",
      "      1. Boom and Bust: The Need for Bold Investments in Fair and Affordable Housing to Combat Inflation\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Energy                    | Volume_net: -3,758,000 | Top Contributors: Van Taylor (-3,758,000)\n",
      "      * Sector: Health Care               | Volume_net: -3,358,000 | Top Contributors: Van Taylor (-3,358,000)\n",
      "      * Sector: Finance                   | Volume_net:   -375,000 | Top Contributors: Van Taylor (-375,000)\n",
      "      * Sector: Unspecified               | Volume_net:    -35,000 | Top Contributors: Van Taylor (-35,000)\n",
      "      * Sector: Capital Goods             | Volume_net:     16,000 | Top Contributors: Cynthia Axne (16,000)\n",
      "      * Sector: Basic Industries          | Volume_net:      8,000 | Top Contributors: Cynthia Axne (8,000)\n",
      "      * Sector: Consumer Services         | Volume_net:      8,000 | Top Contributors: Cynthia Axne (8,000)\n",
      "      * Sector: Miscellaneous             | Volume_net:      8,000 | Top Contributors: Josh Gottheimer (8,000)\n",
      "      * Sector: Technology                | Volume_net:      8,000 | Top Contributors: Josh Gottheimer (8,000)\n",
      "\n",
      "  - Committee-Day: House Financial Services on 2022-12-06 (Degree in plot: 5)\n",
      "    Associated Meeting Titles:\n",
      "      1. Unfinished Business: A Review of Progress Made and a Plan to Achieve Full Economic Inclusion for Every American\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Energy                    | Volume_net: -3,758,000 | Top Contributors: Van Taylor (-3,758,000)\n",
      "      * Sector: Health Care               | Volume_net: -3,350,000 | Top Contributors: Van Taylor (-3,358,000), Josh Gottheimer (8,000)\n",
      "      * Sector: Finance                   | Volume_net:   -383,000 | Top Contributors: Van Taylor (-375,000), Josh Gottheimer (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net:    -35,000 | Top Contributors: Van Taylor (-35,000)\n",
      "      * Sector: Technology                | Volume_net:      8,000 | Top Contributors: Josh Gottheimer (8,000)\n",
      "\n",
      "  - Committee-Day: House Financial Services on 2022-12-07 (Degree in plot: 5)\n",
      "    Associated Meeting Titles:\n",
      "      1. An Enduring Legacy: The Role of Financial Institutions in the Horrors of Slavery and the Need for Atonement, Part Two\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Energy                    | Volume_net: -3,758,000 | Top Contributors: Van Taylor (-3,758,000)\n",
      "      * Sector: Health Care               | Volume_net: -3,350,000 | Top Contributors: Van Taylor (-3,358,000), Josh Gottheimer (8,000)\n",
      "      * Sector: Finance                   | Volume_net:   -383,000 | Top Contributors: Van Taylor (-375,000), Josh Gottheimer (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net:    -35,000 | Top Contributors: Van Taylor (-35,000)\n",
      "      * Sector: Technology                | Volume_net:      8,000 | Top Contributors: Josh Gottheimer (8,000)\n",
      "\n",
      "  - Committee-Day: House Financial Services on 2022-12-08 (Degree in plot: 5)\n",
      "    Associated Meeting Titles:\n",
      "      1. E, S, G and W: Examining Private Sector Disclosure of Workforce Management, Investment, and Diversity Data\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Energy                    | Volume_net: -3,758,000 | Top Contributors: Van Taylor (-3,758,000)\n",
      "      * Sector: Health Care               | Volume_net: -3,350,000 | Top Contributors: Van Taylor (-3,358,000), Josh Gottheimer (8,000)\n",
      "      * Sector: Finance                   | Volume_net:   -383,000 | Top Contributors: Van Taylor (-375,000), Josh Gottheimer (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net:    -35,000 | Top Contributors: Van Taylor (-35,000)\n",
      "      * Sector: Technology                | Volume_net:      8,000 | Top Contributors: Josh Gottheimer (8,000)\n",
      "\n",
      "  - Committee-Day: Senate Banking, Housing, and Urban Affairs on 2019-02-26 (Degree in plot: 8)\n",
      "    Associated Meeting Titles:\n",
      "      1. S.Hrg. 116-2 — THE SEMIANNUAL MONETARY POLICY REPORT TO THE CONGRESS\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Consumer Services         | Volume_net: -3,008,000 | Top Contributors: Mark R. Warner (-3,000,000), Jack Reed (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net: -1,125,000 | Top Contributors: Mark R. Warner (-1,125,000)\n",
      "      * Sector: Technology                | Volume_net:   -153,000 | Top Contributors: Jack Reed (-153,000)\n",
      "      * Sector: Health Care               | Volume_net:    -91,000 | Top Contributors: Jack Reed (-91,000)\n",
      "      * Sector: Capital Goods             | Volume_net:    -24,000 | Top Contributors: Jack Reed (-24,000)\n",
      "      * Sector: Transportation            | Volume_net:    -16,000 | Top Contributors: Jack Reed (-16,000)\n",
      "      * Sector: Consumer Non-Durables     | Volume_net:     -8,000 | Top Contributors: Jack Reed (-8,000)\n",
      "      * Sector: Energy                    | Volume_net:     -8,000 | Top Contributors: Jack Reed (-8,000)\n",
      "\n",
      "  - Committee-Day: Senate Banking, Housing, and Urban Affairs on 2019-02-28 (Degree in plot: 8)\n",
      "    Associated Meeting Titles:\n",
      "      1. S.Hrg. 116-90 — LEGISLATIVE PROPOSALS ON CAPITAL FORMATION AND CORPORATE GOVERNANCE\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Consumer Services         | Volume_net: -3,008,000 | Top Contributors: Mark R. Warner (-3,000,000), Jack Reed (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net: -1,125,000 | Top Contributors: Mark R. Warner (-1,125,000)\n",
      "      * Sector: Technology                | Volume_net:   -153,000 | Top Contributors: Jack Reed (-153,000)\n",
      "      * Sector: Health Care               | Volume_net:    -91,000 | Top Contributors: Jack Reed (-91,000)\n",
      "      * Sector: Capital Goods             | Volume_net:    -24,000 | Top Contributors: Jack Reed (-24,000)\n",
      "      * Sector: Transportation            | Volume_net:    -16,000 | Top Contributors: Jack Reed (-16,000)\n",
      "      * Sector: Consumer Non-Durables     | Volume_net:     -8,000 | Top Contributors: Jack Reed (-8,000)\n",
      "      * Sector: Energy                    | Volume_net:     -8,000 | Top Contributors: Jack Reed (-8,000)\n",
      "\n",
      "  - Committee-Day: Senate Finance on 2019-02-26 (Degree in plot: 7)\n",
      "    Associated Meeting Titles:\n",
      "      1. S.Hrg. 116-39 — DRUG PRICING IN AMERICA: A PRESCRIPTION FOR CHANGE, PART II\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Consumer Services         | Volume_net: -3,000,000 | Top Contributors: Mark R. Warner (-3,000,000), Thomas R. Carper (8,000), Sheldon Whitehouse (-8,000)\n",
      "      * Sector: Unspecified               | Volume_net: -1,125,000 | Top Contributors: Mark R. Warner (-1,125,000)\n",
      "      * Sector: Consumer Staples          | Volume_net:    -35,000 | Top Contributors: Sheldon Whitehouse (-35,000)\n",
      "      * Sector: Technology                | Volume_net:    -35,000 | Top Contributors: Sheldon Whitehouse (-35,000)\n",
      "      * Sector: Basic Industries          | Volume_net:      8,000 | Top Contributors: Sheldon Whitehouse (8,000)\n",
      "      * Sector: Consumer Non-Durables     | Volume_net:      8,000 | Top Contributors: Sheldon Whitehouse (8,000)\n",
      "      * Sector: Public Utilities          | Volume_net:      8,000 | Top Contributors: Sheldon Whitehouse (8,000)\n",
      "\n",
      "  - Committee-Day: House Ways and Means on 2021-06-29 (Degree in plot: 11)\n",
      "    Associated Meeting Titles:\n",
      "      1. Expanding Access to Higher Education and the Promise it Holds\n",
      "    Trades in Sectors (around this meeting):\n",
      "      * Sector: Health Care               | Volume_net:  1,145,000 | Top Contributors: Kevin Hern (1,145,000)\n",
      "      * Sector: Finance                   | Volume_net:    860,000 | Top Contributors: Kevin Hern (860,000)\n",
      "      * Sector: Technology                | Volume_net:    708,000 | Top Contributors: Kevin Hern (700,000), Suzan K. DelBene (8,000)\n",
      "      * Sector: Basic Materials           | Volume_net:    210,000 | Top Contributors: Kevin Hern (210,000)\n",
      "      * Sector: Consumer Services         | Volume_net:    183,000 | Top Contributors: Kevin Hern (175,000), Lloyd Doggett (8,000)\n",
      "      * Sector: Miscellaneous             | Volume_net:    175,000 | Top Contributors: Kevin Hern (175,000)\n",
      "      * Sector: Unspecified               | Volume_net:    175,000 | Top Contributors: Kevin Hern (175,000)\n",
      "      * Sector: Capital Goods             | Volume_net:    150,000 | Top Contributors: Kevin Hern (150,000)\n",
      "      * Sector: Energy                    | Volume_net:    118,000 | Top Contributors: Kevin Hern (118,000)\n",
      "      * Sector: Consumer Non-Durables     | Volume_net:      8,000 | Top Contributors: Lloyd Doggett (8,000)\n",
      "      * Sector: Public Utilities          | Volume_net:      8,000 | Top Contributors: Kevin Hern (8,000)\n",
      "\n",
      "--- Meeting Day Bipartite Detailed Analysis Script Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "# from networkx.algorithms import bipartite # Not strictly needed if calling nx.bipartite directly\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- User Adjustable Parameters ---\n",
    "MEETING_DAYS_BEFORE = 12\n",
    "MEETING_DAYS_AFTER = 3\n",
    "MEETING_TOP_K_TO_PLOT = 10\n",
    "MEETING_EDGE_WEIGHTING = 'volume_net' # 'count', 'volume', or 'volume_net'\n",
    "MEETING_MIN_TX_FOR_EDGE = 1 # Min transactions if weighting by count (for count)\n",
    "                            # For volume/volume_net, a small positive float might be better e.g. 0.001\n",
    "TOP_N_CONTRIBUTING_MEMBERS_TO_SHOW = 3 # How many top members to show per sector link\n",
    "\n",
    "MEETING_BIPARTITE_OUTPUT_DIR = \"meeting_day_sector_bipartite_analysis_v3_detailed\" # New output dir\n",
    "os.makedirs(MEETING_BIPARTITE_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- ASSUME THESE ARE ALREADY DEFINED AND POPULATED ---\n",
    "# STOCK_TRANSACTIONS_DF (with 'member_id', 'member' (full name), 'transaction_date_dt', 'sector', 'amount', 'type')\n",
    "# COMMITTEE_MEETINGS_DF (with 'Committee', 'Meeting Title', 'meeting_date_dt')\n",
    "# COMMITTEE_MEMBERSHIP_MAP\n",
    "# CONGRESS_PERIOD_MAP\n",
    "# id2name (dictionary mapping member_id to full name)\n",
    "\n",
    "if 'id2name' not in globals(): # Ensure id2name exists for prettier output\n",
    "    print(\"Warning: id2name map not found. Member names might be missing in detailed output.\")\n",
    "    id2name = {}\n",
    "\n",
    "print(f\"STOCK_TRANSACTIONS_DF shape: {STOCK_TRANSACTIONS_DF.shape}, COMMITTEE_MEETINGS_DF shape: {COMMITTEE_MEETINGS_DF.shape}, len(COMMITTEE_MEMBERSHIP_MAP): {len(COMMITTEE_MEMBERSHIP_MAP)}, len(CONGRESS_PERIOD_MAP): {len(CONGRESS_PERIOD_MAP)}\")\n",
    "\n",
    "if 'STOCK_TRANSACTIONS_DF' not in globals() or 'COMMITTEE_MEETINGS_DF' not in globals() or \\\n",
    "   'COMMITTEE_MEMBERSHIP_MAP' not in globals() or 'CONGRESS_PERIOD_MAP' not in globals():\n",
    "    print(\"CRITICAL: Required DataFrames/Mappings not found. Skipping Meeting Day Analysis.\")\n",
    "    exit() # Or handle gracefully\n",
    "else:\n",
    "    print(f\"\\n--- Starting Meeting Day Bipartite Network Analysis ({MEETING_EDGE_WEIGHTING}-weighted) ---\")\n",
    "    print(f\"Window: {MEETING_DAYS_BEFORE} days before, {MEETING_DAYS_AFTER} days after meeting.\")\n",
    "\n",
    "    # --- Preprocessing ---\n",
    "    # (Assuming preprocessing from your script is done: datetime conversions, string types, fillna)\n",
    "    # For robustness, let's ensure key columns again\n",
    "    for df_check, name, date_col_orig, date_col_dt in [\n",
    "        (STOCK_TRANSACTIONS_DF, \"Stocks\", \"transaction_date\", \"transaction_date_dt\"),\n",
    "        (COMMITTEE_MEETINGS_DF, \"Meetings\", \"Meeting Date\", \"meeting_date_dt\")]:\n",
    "        if date_col_dt not in df_check.columns or not pd.api.types.is_datetime64_any_dtype(df_check[date_col_dt]):\n",
    "            df_check[date_col_dt] = pd.to_datetime(df_check[date_col_orig], errors='coerce')\n",
    "        if df_check[date_col_dt].dt.tz is not None:\n",
    "            df_check[date_col_dt] = df_check[date_col_dt].dt.tz_localize(None)\n",
    "        df_check.dropna(subset=[date_col_dt], inplace=True)\n",
    "\n",
    "    STOCK_TRANSACTIONS_DF[\"member_id\"] = STOCK_TRANSACTIONS_DF[\"member_id\"].astype(str)\n",
    "    STOCK_TRANSACTIONS_DF[\"sector\"] = STOCK_TRANSACTIONS_DF[\"sector\"].fillna(\"Unspecified_Sector\").astype(str)\n",
    "    STOCK_TRANSACTIONS_DF[\"amount\"] = pd.to_numeric(STOCK_TRANSACTIONS_DF[\"amount\"], errors='coerce').fillna(0.0)\n",
    "    STOCK_TRANSACTIONS_DF[\"type\"] = STOCK_TRANSACTIONS_DF[\"type\"].astype(str).str.lower()\n",
    "\n",
    "    COMMITTEE_MEETINGS_DF.dropna(subset=['Committee', 'Meeting Title'], inplace=True)\n",
    "    COMMITTEE_MEETINGS_DF['Meeting Title'] = COMMITTEE_MEETINGS_DF['Meeting Title'].astype(str)\n",
    "    COMMITTEE_MEETINGS_DF['Committee'] = COMMITTEE_MEETINGS_DF['Committee'].astype(str)\n",
    "    if 'meeting_date_str_original' not in COMMITTEE_MEETINGS_DF.columns:\n",
    "        COMMITTEE_MEETINGS_DF['meeting_date_str_original'] = COMMITTEE_MEETINGS_DF['meeting_date_dt'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "    for comm_md_key, periods_md_val in COMMITTEE_MEMBERSHIP_MAP.items():\n",
    "        for period_md_key, members_md_val_set in periods_md_val.items():\n",
    "            COMMITTEE_MEMBERSHIP_MAP[comm_md_key][period_md_key] = {str(m) for m in members_md_val_set}\n",
    "\n",
    "    # --- Aggregate Trades around Meeting Days ---\n",
    "    year_to_congress_period_md_map = {yr: p for p, yrs in CONGRESS_PERIOD_MAP.items() for yr in yrs}\n",
    "    meeting_day_sector_links_detailed = [] # To store more detailed info including members\n",
    "\n",
    "    unique_meeting_events_df = COMMITTEE_MEETINGS_DF.groupby(\n",
    "        ['Committee', 'meeting_date_str_original', 'meeting_date_dt']\n",
    "    )['Meeting Title'].apply(lambda x: sorted(list(set(x)))).reset_index() # Ensure unique, sorted titles\n",
    "    \n",
    "    print(f\"Processing {len(unique_meeting_events_df)} unique committee-meeting-day events...\")\n",
    "\n",
    "    for _, event_row_data in tqdm(unique_meeting_events_df.iterrows(), total=len(unique_meeting_events_df), desc=\"Aggregating trades around meetings\"):\n",
    "        committee_name_md_val = event_row_data['Committee']\n",
    "        meeting_date_obj_md_val = event_row_data['meeting_date_dt']\n",
    "        meeting_date_str_md_val = event_row_data['meeting_date_str_original']\n",
    "        meeting_titles_md_list = event_row_data['Meeting Title']\n",
    "\n",
    "        meeting_year_md_val = meeting_date_obj_md_val.year\n",
    "        congress_p_md_val = year_to_congress_period_md_map.get(meeting_year_md_val)\n",
    "        if not congress_p_md_val: continue\n",
    "        \n",
    "        comm_members_md_set = COMMITTEE_MEMBERSHIP_MAP.get(committee_name_md_val, {}).get(congress_p_md_val)\n",
    "        if not comm_members_md_set: continue\n",
    "\n",
    "        start_window_dt = meeting_date_obj_md_val - timedelta(days=MEETING_DAYS_BEFORE)\n",
    "        end_window_dt = meeting_date_obj_md_val + timedelta(days=MEETING_DAYS_AFTER)\n",
    "\n",
    "        period_tx_df_slice = STOCK_TRANSACTIONS_DF[\n",
    "            (STOCK_TRANSACTIONS_DF['member_id'].isin(comm_members_md_set)) &\n",
    "            (STOCK_TRANSACTIONS_DF['transaction_date_dt'] >= start_window_dt) &\n",
    "            (STOCK_TRANSACTIONS_DF['transaction_date_dt'] <= end_window_dt)\n",
    "        ]\n",
    "        \n",
    "        is_buy_md_series = period_tx_df_slice['type'].str.contains('purchase|buy', case=False, na=False)\n",
    "        is_sell_md_series = period_tx_df_slice['type'].str.contains('sale|sell', case=False, na=False)\n",
    "        relevant_tx_md_df = period_tx_df_slice[is_buy_md_series | is_sell_md_series].copy()\n",
    "\n",
    "        if not relevant_tx_md_df.empty:\n",
    "            # Calculate signed amount for net volume calculation\n",
    "            relevant_tx_md_df.loc[:, 'signed_amount'] = np.where(\n",
    "                is_buy_md_series[relevant_tx_md_df.index], # Align index for where condition\n",
    "                relevant_tx_md_df['amount'], \n",
    "                -relevant_tx_md_df['amount']\n",
    "            )\n",
    "            \n",
    "            # Group by sector to get overall link weight AND contributing members\n",
    "            for sector_name, sector_group_df in relevant_tx_md_df.groupby('sector'):\n",
    "                link_weight = 0\n",
    "                if MEETING_EDGE_WEIGHTING == 'count':\n",
    "                    link_weight = len(sector_group_df)\n",
    "                elif MEETING_EDGE_WEIGHTING == 'volume':\n",
    "                    link_weight = sector_group_df['amount'].sum()\n",
    "                elif MEETING_EDGE_WEIGHTING == 'volume_net':\n",
    "                    link_weight = sector_group_df['signed_amount'].sum()\n",
    "                \n",
    "                min_thresh_val = MEETING_MIN_TX_FOR_EDGE if MEETING_EDGE_WEIGHTING == 'count' else 0.001\n",
    "                if abs(link_weight) >= min_thresh_val:\n",
    "                    # Identify top contributing members to this sector's net volume/count\n",
    "                    member_contributions = sector_group_df.groupby('member_id')['signed_amount' if MEETING_EDGE_WEIGHTING == 'volume_net' else 'amount'].sum()\n",
    "                    if MEETING_EDGE_WEIGHTING == 'count':\n",
    "                        member_contributions = sector_group_df['member_id'].value_counts()\n",
    "                    \n",
    "                    # Sort members by absolute contribution for 'volume_net', or by count/volume otherwise\n",
    "                    if MEETING_EDGE_WEIGHTING == 'volume_net':\n",
    "                        sorted_members_contrib = member_contributions.abs().sort_values(ascending=False)\n",
    "                    else:\n",
    "                        sorted_members_contrib = member_contributions.sort_values(ascending=False)\n",
    "                    \n",
    "                    top_contributors_list = []\n",
    "                    for mem_id, contrib_val in sorted_members_contrib.head(TOP_N_CONTRIBUTING_MEMBERS_TO_SHOW).items():\n",
    "                        mem_name = id2name.get(mem_id, mem_id)\n",
    "                        # For volume_net, show actual signed contribution\n",
    "                        actual_contrib = member_contributions.get(mem_id, contrib_val) if MEETING_EDGE_WEIGHTING == 'volume_net' else contrib_val\n",
    "                        top_contributors_list.append(f\"{mem_name} ({actual_contrib:,.0f})\")\n",
    "\n",
    "\n",
    "                    meeting_day_id_str_md = f\"{committee_name_md_val}|{meeting_date_str_md_val}\"\n",
    "                    meeting_day_sector_links_detailed.append({\n",
    "                        'meeting_day_id': meeting_day_id_str_md,\n",
    "                        'committee': committee_name_md_val,\n",
    "                        'date': meeting_date_str_md_val,\n",
    "                        'sector': sector_name,\n",
    "                        'weight': link_weight,\n",
    "                        'plot_weight': abs(link_weight),\n",
    "                        'titles': meeting_titles_md_list,\n",
    "                        'top_contributors': \", \".join(top_contributors_list) if top_contributors_list else \"N/A\"\n",
    "                    })\n",
    "    \n",
    "    if not meeting_day_sector_links_detailed:\n",
    "        print(\"No relevant trades found linking meeting days to sectors after detailed aggregation.\")\n",
    "    else:\n",
    "        df_meeting_links_detailed = pd.DataFrame(meeting_day_sector_links_detailed)\n",
    "        print(f\"Generated {len(df_meeting_links_detailed)} detailed committee-meeting-day <-> sector links.\")\n",
    "        \n",
    "        # Save detailed links to CSV\n",
    "        detailed_links_csv_path = os.path.join(MEETING_BIPARTITE_OUTPUT_DIR, f\"meeting_day_sector_links_detailed_{MEETING_EDGE_WEIGHTING}.csv\")\n",
    "        df_meeting_links_detailed.to_csv(detailed_links_csv_path, index=False)\n",
    "        print(f\"[Saved] Detailed links with contributors to: {detailed_links_csv_path}\")\n",
    "\n",
    "\n",
    "        # --- Build Bipartite Graph (same as before) ---\n",
    "        B_meetings_viz = nx.Graph()\n",
    "        all_meeting_day_ids_viz = df_meeting_links_detailed['meeting_day_id'].unique()\n",
    "        all_linked_sectors_viz = df_meeting_links_detailed['sector'].unique()\n",
    "        \n",
    "        B_meetings_viz.add_nodes_from(all_meeting_day_ids_viz, bipartite=0, type='meeting_day_event')\n",
    "        B_meetings_viz.add_nodes_from(all_linked_sectors_viz, bipartite=1, type='sector')\n",
    "\n",
    "        # Store titles and top contributors as node attributes for meeting_day_id nodes\n",
    "        meeting_day_attributes = df_meeting_links_detailed.groupby('meeting_day_id').agg(\n",
    "            titles=('titles', 'first'), # Assuming titles are same for a given meeting_day_id\n",
    "            # Note: Top contributors are per sector link, not per meeting day.\n",
    "            # We will access this from df_meeting_links_detailed during printout.\n",
    "        ).to_dict('index')\n",
    "        nx.set_node_attributes(B_meetings_viz, meeting_day_attributes)\n",
    "\n",
    "        for _, link_row_data in df_meeting_links_detailed.iterrows():\n",
    "            B_meetings_viz.add_edge(link_row_data['meeting_day_id'], link_row_data['sector'], \n",
    "                                weight=link_row_data['weight'], plot_weight=link_row_data['plot_weight'])\n",
    "        print(f\"Meeting Day Bipartite Graph (for viz): {B_meetings_viz.number_of_nodes()} nodes, {B_meetings_viz.number_of_edges()} edges.\")\n",
    "        \n",
    "        # --- Plotting (Focus on Top K Meeting Days) ---\n",
    "        graph_to_plot_final_md = B_meetings_viz\n",
    "        meeting_day_nodes_for_plot_final = list(all_meeting_day_ids_viz)\n",
    "\n",
    "        if MEETING_TOP_K_TO_PLOT > 0 and len(all_meeting_day_ids_viz) > MEETING_TOP_K_TO_PLOT:\n",
    "            md_abs_weights_plot = defaultdict(float)\n",
    "            for u, v, data_edge in B_meetings_viz.edges(data=True): # Iterate over graph edges\n",
    "                node_u_type = B_meetings_viz.nodes[u].get('type')\n",
    "                node_v_type = B_meetings_viz.nodes[v].get('type')\n",
    "                plot_w = data_edge.get('plot_weight', 0)\n",
    "\n",
    "                if node_u_type == 'meeting_day_event': md_abs_weights_plot[u] += plot_w\n",
    "                elif node_v_type == 'meeting_day_event': md_abs_weights_plot[v] += plot_w # Should not happen if u is always meeting_day in construction\n",
    "            \n",
    "            if md_abs_weights_plot:\n",
    "                top_md_nodes_plot = sorted(md_abs_weights_plot, key=md_abs_weights_plot.get, reverse=True)[:MEETING_TOP_K_TO_PLOT]\n",
    "                nodes_to_include_plot = set(top_md_nodes_plot)\n",
    "                for md_node_p in top_md_nodes_plot:\n",
    "                    nodes_to_include_plot.update(B_meetings_viz.neighbors(md_node_p))\n",
    "                graph_to_plot_final_md = B_meetings_viz.subgraph(nodes_to_include_plot).copy()\n",
    "                meeting_day_nodes_for_plot_final = [n for n in top_md_nodes_plot if n in graph_to_plot_final_md]\n",
    "                print(f\"Plotting filtered graph: {graph_to_plot_final_md.number_of_nodes()} nodes, {graph_to_plot_final_md.number_of_edges()} edges.\")\n",
    "            # ... (else part for no weights or full graph plotting as before) ...\n",
    "\n",
    "        if graph_to_plot_final_md.number_of_edges() > 0:\n",
    "            # ... (Plotting logic - largely same, ensure to use graph_to_plot_final_md) ...\n",
    "            # (Make sure node attributes like 'type' are correctly carried to the subgraph if filtering)\n",
    "            # For example, when getting bipartite set for layout:\n",
    "            layout_meeting_day_nodes_final = [n for n in graph_to_plot_final_md.nodes() if graph_to_plot_final_md.nodes[n].get('type') == 'meeting_day_event']\n",
    "            if not layout_meeting_day_nodes_final : # Fallback if subgraph doesn't retain type or no meeting days\n",
    "                layout_meeting_day_nodes_final = [n for n in graph_to_plot_final_md.nodes() if B_meetings_viz.nodes[n].get('type') == 'meeting_day_event']\n",
    "\n",
    "            if not layout_meeting_day_nodes_final: print(\"No 'meeting_day_event' nodes for layout in plot graph. Skipping plot.\")\n",
    "            else:\n",
    "                # (Rest of your plotting code from previous version, using graph_to_plot_final_md and layout_meeting_day_nodes_final)\n",
    "                # ... PLOTTING CODE ...\n",
    "                plt.figure(figsize=(max(18, len(layout_meeting_day_nodes_final)*0.6), 14)) # Adjusted figsize\n",
    "                pos_md_plot = nx.bipartite_layout(graph_to_plot_final_md, layout_meeting_day_nodes_final, align='vertical', scale=3, aspect_ratio=0.35)\n",
    "                node_colors_plot_list = ['#a6cee3' if graph_to_plot_final_md.nodes[n]['type'] == 'meeting_day_event' else '#fdbf6f' for n in graph_to_plot_final_md.nodes()]\n",
    "                node_degrees_plot_dict = dict(graph_to_plot_final_md.degree())\n",
    "                node_sizes_plot_list = [node_degrees_plot_dict.get(n,1) * 100 + 200 for n in graph_to_plot_final_md.nodes()] # Smaller base size\n",
    "                edge_plot_weights_md = [d.get('plot_weight',1) for _,_,d in graph_to_plot_final_md.edges(data=True)]\n",
    "                max_pw_md_val = max(edge_plot_weights_md) if edge_plot_weights_md else 1.0; max_pw_md_val = 1.0 if max_pw_md_val == 0 else max_pw_md_val\n",
    "                edge_widths_md_plot = [0.4 + 4.5 * (d.get('plot_weight',1) / max_pw_md_val) for _,_,d in graph_to_plot_final_md.edges(data=True)]\n",
    "                edge_colors_md_plot = ['#33a02c' if d.get('weight',0) > 0 else ('#e31a1c' if d.get('weight',0) < 0 else 'lightgray') for _,_,d in graph_to_plot_final_md.edges(data=True)] if MEETING_EDGE_WEIGHTING == 'volume_net' else ['dimgray'] * graph_to_plot_final_md.number_of_edges()\n",
    "                nx.draw_networkx_nodes(graph_to_plot_final_md, pos_md_plot, node_color=node_colors_plot_list, node_size=node_sizes_plot_list, alpha=0.9, edgecolors='grey', linewidths=0.5)\n",
    "                nx.draw_networkx_edges(graph_to_plot_final_md, pos_md_plot, width=edge_widths_md_plot, alpha=0.5, edge_color=edge_colors_md_plot)\n",
    "                labels_md_plot = {n: (f\"{n.split('|')[0][:25]}..\\\\n{n.split('|')[1]}\" if graph_to_plot_final_md.nodes[n]['type'] == 'meeting_day_event' else n) for n in graph_to_plot_final_md.nodes()}\n",
    "                nx.draw_networkx_labels(graph_to_plot_final_md, pos_md_plot, labels=labels_md_plot, font_size=7, font_weight='normal')\n",
    "                plot_title_str_md = f\"Bipartite Network: Committee Meeting Days & Sector Trades ({MEETING_EDGE_WEIGHTING}-weighted)\\\\nWindow: {MEETING_DAYS_BEFORE} days before, {MEETING_DAYS_AFTER} days after meeting day\"\n",
    "                if MEETING_EDGE_WEIGHTING == 'volume_net': plot_title_str_md += \" (Green=Net Buys, Red=Net Sells)\"\n",
    "                plt.title(plot_title_str_md, fontsize=12); plt.axis('off'); plt.tight_layout(pad=0.5)\n",
    "                plot_filename_final = os.path.join(MEETING_BIPARTITE_OUTPUT_DIR, f\"meeting_days_sectors_bipartite_final_{MEETING_EDGE_WEIGHTING}.png\")\n",
    "                plt.savefig(plot_filename_final, dpi=200); plt.close()\n",
    "                print(f\"Saved final bipartite meeting day plot: {plot_filename_final}\")\n",
    "\n",
    "                # --- Enhanced Printout for Top K Meeting Days ---\n",
    "                print(\"\\n--- Detailed Analysis of Top Plotted Committee-Meeting-Days ---\")\n",
    "                for md_id_print in meeting_day_nodes_for_plot_final: # Use the actual list of plotted meeting day nodes\n",
    "                    committee_name_print, date_print = md_id_print.split('|')\n",
    "                    print(f\"\\n  - Committee-Day: {committee_name_print} on {date_print} (Degree in plot: {graph_to_plot_final_md.degree(md_id_print)})\")\n",
    "                    \n",
    "                    # Get meeting titles from node attribute if stored, otherwise from map\n",
    "                    titles_print = graph_to_plot_final_md.nodes[md_id_print].get('meeting_titles', meeting_day_attributes.get(md_id_print, {}).get('titles', [\"N/A\"]))\n",
    "                    print(f\"    Associated Meeting Titles:\")\n",
    "                    for i_title, title_p in enumerate(titles_print): print(f\"      {i_title+1}. {title_p[:120]}\") # Truncate long titles\n",
    "\n",
    "                    print(f\"    Trades in Sectors (around this meeting):\")\n",
    "                    # Iterate through the links in df_meeting_links_detailed for this specific meeting_day_id\n",
    "                    links_for_this_day = df_meeting_links_detailed[df_meeting_links_detailed['meeting_day_id'] == md_id_print].sort_values(by='plot_weight', ascending=False)\n",
    "                    for _, link_detail_row in links_for_this_day.iterrows():\n",
    "                        sector_p = link_detail_row['sector']\n",
    "                        weight_p = link_detail_row['weight']\n",
    "                        plot_weight_p = link_detail_row['plot_weight']\n",
    "                        contributors_p = link_detail_row['top_contributors']\n",
    "                        \n",
    "                        weight_str_p = f\"{weight_p:,.0f}\" if MEETING_EDGE_WEIGHTING != 'count' else str(weight_p)\n",
    "                        print(f\"      * Sector: {sector_p.ljust(25)} | {MEETING_EDGE_WEIGHTING.capitalize()}: {weight_str_p.rjust(10)} | Top Contributors: {contributors_p}\")\n",
    "        else:\n",
    "            print(\"Graph for meeting day analysis (potentially filtered) has no edges to plot.\")\n",
    "print(\"\\n--- Meeting Day Bipartite Detailed Analysis Script Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305cea6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Leader-Follower Pairs (lag ≤ 3 days):\n",
      "           leader_name         follower_name  count\n",
      "143    Josh Gottheimer        Greg Gianforte     30\n",
      "236    Josh Gottheimer      Gilbert Cisneros     26\n",
      "133      Dean Phillips     Michael T. McCaul     22\n",
      "26         Pat Roberts       Josh Gottheimer     21\n",
      "234  David B. McKinley       Josh Gottheimer     17\n",
      "474    Josh Gottheimer        Kenny Marchant     16\n",
      "573   Tommy Tuberville          Cynthia Axne     15\n",
      "453        John Curtis       Josh Gottheimer     14\n",
      "185       David Perdue           Steve Cohen     14\n",
      "452       Bill Cassidy       Josh Gottheimer     13\n",
      "287      Kurt Schrader       Josh Gottheimer     13\n",
      "502  Robert J. Wittman       Josh Gottheimer     12\n",
      "41     Susan W. Brooks        Kelly Loeffler     12\n",
      "157       David Perdue             Ron Wyden     11\n",
      "490    Josh Gottheimer  Shelley Moore Capito     11\n",
      "30         Pat Roberts     David B. McKinley     10\n",
      "46        David Perdue    K. Michael Conaway     10\n",
      "491    Josh Gottheimer         Pete Sessions     10\n",
      "136          Ro Khanna         Kathy Manning     10\n",
      "285       David Perdue        Kelly Loeffler      9\n",
      "[Saved] All leader-follower pairs to leader_follower_analysis/leader_follower_all_pairs.csv\n",
      "\n",
      "Leader-Follower Graph: 50 members, 79 edges.\n",
      "\n",
      "Top 10 Leaders by Weighted Out-Degree:\n",
      "               member_name  weighted_out_degree\n",
      "G000583    Josh Gottheimer                  161\n",
      "S379          David Perdue                   76\n",
      "S260           Pat Roberts                   31\n",
      "C001123   Gilbert Cisneros                   27\n",
      "P000616      Dean Phillips                   27\n",
      "S405        Kelly Loeffler                   22\n",
      "S412      Tommy Tuberville                   20\n",
      "K000389          Ro Khanna                   20\n",
      "S247             Ron Wyden                   19\n",
      "M001180  David B. McKinley                   17\n",
      "\n",
      "Top 10 Followers by Weighted In-Degree:\n",
      "               member_name  weighted_in_degree\n",
      "G000583    Josh Gottheimer                 164\n",
      "C001123   Gilbert Cisneros                  42\n",
      "G000584     Greg Gianforte                  36\n",
      "S405        Kelly Loeffler                  35\n",
      "K000389          Ro Khanna                  32\n",
      "S260           Pat Roberts                  30\n",
      "M001180  David B. McKinley                  26\n",
      "A000378       Cynthia Axne                  26\n",
      "M001157  Michael T. McCaul                  22\n",
      "S412      Tommy Tuberville                  21\n",
      "[Saved] Leader-follower network plot (PageRank-colored) to leader_follower_analysis/leader_follower_network_min5_lag3_pagerank.png\n"
     ]
    }
   ],
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as mcolors\n",
    "import os\n",
    "\n",
    "# --- ASSUMPTIONS ---\n",
    "# STOCK_TRANSACTIONS_DF: Loaded with 'member_id', 'ticker', 'transaction_date_dt' (datetime)\n",
    "# id2name: Dictionary mapping member_id to member's full name.\n",
    "# member_to_committee: Dictionary mapping member_id to committee name (not used here for coloring).\n",
    "\n",
    "member_to_committee = COMMITTEE_MEMBERSHIP_MAP  # Still available, but we'll color by PageRank instead\n",
    "\n",
    "# --- Ensure required DataFrames/mappings are available ---\n",
    "if 'STOCK_TRANSACTIONS_DF' not in globals() or \\\n",
    "   'id2name' not in globals() or \\\n",
    "   'member_to_committee' not in globals():\n",
    "    print(\"Error: STOCK_TRANSACTIONS_DF, id2name, or member_to_committee is not defined.\")\n",
    "\n",
    "OUTPUT_DIR_LF = \"leader_follower_analysis\"\n",
    "os.makedirs(OUTPUT_DIR_LF, exist_ok=True)\n",
    "\n",
    "# --- Parameters ---\n",
    "MAX_LAG_DAYS = 7\n",
    "MIN_LF_COUNT_FOR_EDGE = 10\n",
    "TOP_N_PAIRS_TO_PRINT = 20\n",
    "TOP_N_CENTRAL_NODES_TO_PRINT = 10\n",
    "\n",
    "# --- Preprocessing ---\n",
    "if not pd.api.types.is_datetime64_any_dtype(STOCK_TRANSACTIONS_DF['transaction_date_dt']):\n",
    "    STOCK_TRANSACTIONS_DF['transaction_date_dt'] = pd.to_datetime(\n",
    "        STOCK_TRANSACTIONS_DF['transaction_date_dt'], errors='coerce'\n",
    "    )\n",
    "    STOCK_TRANSACTIONS_DF.dropna(subset=['transaction_date_dt'], inplace=True)\n",
    "\n",
    "STOCK_TRANSACTIONS_DF['member_id'] = STOCK_TRANSACTIONS_DF['member_id'].astype(str)\n",
    "STOCK_TRANSACTIONS_DF['ticker'] = STOCK_TRANSACTIONS_DF['ticker'].astype(str).str.upper()\n",
    "\n",
    "if 'type' in STOCK_TRANSACTIONS_DF.columns:\n",
    "    df_trades_lf = STOCK_TRANSACTIONS_DF[\n",
    "        STOCK_TRANSACTIONS_DF['type'].astype(str).str.contains('sale|purchase', case=False, na=False)\n",
    "    ].copy()\n",
    "else:\n",
    "    print(\"Warning: 'type' column not found. Using all transactions for leader-follower analysis.\")\n",
    "    df_trades_lf = STOCK_TRANSACTIONS_DF.copy()\n",
    "\n",
    "df_trades_lf.sort_values(by=['ticker', 'transaction_date_dt'], inplace=True)\n",
    "\n",
    "leader_follower_pairs_raw = defaultdict(int)\n",
    "\n",
    "for ticker_symbol, ticker_group_df in df_trades_lf.groupby('ticker'):\n",
    "    if len(ticker_group_df) < 2:\n",
    "        continue\n",
    "\n",
    "    trades_list = ticker_group_df.to_dict('records')\n",
    "    for i in range(len(trades_list)):\n",
    "        trade_A = trades_list[i]\n",
    "        member_A_id = trade_A['member_id']\n",
    "        date_A = trade_A['transaction_date_dt']\n",
    "\n",
    "        for j in range(i + 1, len(trades_list)):\n",
    "            trade_B = trades_list[j]\n",
    "            member_B_id = trade_B['member_id']\n",
    "            date_B = trade_B['transaction_date_dt']\n",
    "\n",
    "            if member_A_id == member_B_id:\n",
    "                continue\n",
    "\n",
    "            time_lag = (date_B - date_A).days\n",
    "            if 0 < time_lag <= MAX_LAG_DAYS:\n",
    "                leader_follower_pairs_raw[(member_A_id, member_B_id)] += 1\n",
    "            elif time_lag > MAX_LAG_DAYS:\n",
    "                break\n",
    "\n",
    "if not leader_follower_pairs_raw:\n",
    "    print(\"No leader-follower patterns found with the specified lag.\")\n",
    "else:\n",
    "    df_leader_follower_with_ids = pd.DataFrame([\n",
    "        {\n",
    "            \"leader_id\": k[0],\n",
    "            \"follower_id\": k[1],\n",
    "            \"leader_name\": id2name.get(k[0], k[0]),\n",
    "            \"follower_name\": id2name.get(k[1], k[1]),\n",
    "            \"count\": v\n",
    "        }\n",
    "        for k, v in leader_follower_pairs_raw.items()\n",
    "    ]).sort_values(by=\"count\", ascending=False)\n",
    "\n",
    "    print(f\"\\nTop {TOP_N_PAIRS_TO_PRINT} Leader-Follower Pairs (lag ≤ {MAX_LAG_DAYS} days):\")\n",
    "    print(\n",
    "        df_leader_follower_with_ids[['leader_name', 'follower_name', 'count']]\n",
    "        .head(TOP_N_PAIRS_TO_PRINT)\n",
    "        .to_string()\n",
    "    )\n",
    "    df_leader_follower_with_ids.to_csv(\n",
    "        os.path.join(OUTPUT_DIR_LF, \"leader_follower_all_pairs.csv\"),\n",
    "        index=False\n",
    "    )\n",
    "    print(f\"[Saved] All leader-follower pairs to {os.path.join(OUTPUT_DIR_LF, 'leader_follower_all_pairs.csv')}\")\n",
    "\n",
    "    # --- Build Directed Graph ---\n",
    "    G_lf = nx.DiGraph()\n",
    "    all_members_in_lf_pairs = set()\n",
    "\n",
    "    for _, row in df_leader_follower_with_ids.iterrows():\n",
    "        if row['count'] >= MIN_LF_COUNT_FOR_EDGE:\n",
    "            leader_id = row['leader_id']\n",
    "            follower_id = row['follower_id']\n",
    "            G_lf.add_edge(leader_id, follower_id, weight=row['count'])\n",
    "            all_members_in_lf_pairs.add(leader_id)\n",
    "            all_members_in_lf_pairs.add(follower_id)\n",
    "\n",
    "    for mid in all_members_in_lf_pairs:\n",
    "        if not G_lf.has_node(mid):\n",
    "            G_lf.add_node(mid)\n",
    "\n",
    "    if G_lf.number_of_nodes() == 0:\n",
    "        print(\"Leader-follower graph is empty (no edges met the threshold).\")\n",
    "    else:\n",
    "        print(f\"\\nLeader-Follower Graph: {G_lf.number_of_nodes()} members, {G_lf.number_of_edges()} edges.\")\n",
    "\n",
    "        # --- Centrality Analysis ---\n",
    "        out_degree_centrality = {n: G_lf.out_degree(n, weight='weight') for n in G_lf.nodes()}\n",
    "        df_out_degree = pd.DataFrame.from_dict(\n",
    "            out_degree_centrality, orient='index', columns=['weighted_out_degree']\n",
    "        )\n",
    "        df_out_degree['member_name'] = df_out_degree.index.map(lambda x: id2name.get(x, x))\n",
    "        df_out_degree.sort_values(by='weighted_out_degree', ascending=False, inplace=True)\n",
    "        print(f\"\\nTop {TOP_N_CENTRAL_NODES_TO_PRINT} Leaders by Weighted Out-Degree:\")\n",
    "        print(df_out_degree[['member_name', 'weighted_out_degree']].head(TOP_N_CENTRAL_NODES_TO_PRINT).to_string())\n",
    "        df_out_degree.to_csv(\n",
    "            os.path.join(OUTPUT_DIR_LF, \"leader_follower_out_degree_centrality.csv\"),\n",
    "            index=True\n",
    "        )\n",
    "\n",
    "        in_degree_centrality = {n: G_lf.in_degree(n, weight='weight') for n in G_lf.nodes()}\n",
    "        df_in_degree = pd.DataFrame.from_dict(\n",
    "            in_degree_centrality, orient='index', columns=['weighted_in_degree']\n",
    "        )\n",
    "        df_in_degree['member_name'] = df_in_degree.index.map(lambda x: id2name.get(x, x))\n",
    "        df_in_degree.sort_values(by='weighted_in_degree', ascending=False, inplace=True)\n",
    "        print(f\"\\nTop {TOP_N_CENTRAL_NODES_TO_PRINT} Followers by Weighted In-Degree:\")\n",
    "        print(df_in_degree[['member_name', 'weighted_in_degree']].head(TOP_N_CENTRAL_NODES_TO_PRINT).to_string())\n",
    "        df_in_degree.to_csv(\n",
    "            os.path.join(OUTPUT_DIR_LF, \"leader_follower_in_degree_centrality.csv\"),\n",
    "            index=True\n",
    "        )\n",
    "\n",
    "        # Compute PageRank once for coloring\n",
    "        try:\n",
    "            pagerank_dict = nx.pagerank(G_lf, weight='weight')\n",
    "        except Exception as e:\n",
    "            print(f\"Could not calculate PageRank: {e}\")\n",
    "            pagerank_dict = {n: 0.0 for n in G_lf.nodes()}\n",
    "\n",
    "        # --- Visualization of the Leader-Follower Graph (Colored by PageRank) ---\n",
    "        plt.figure(figsize=(18, 15))\n",
    "        pos_lf = nx.spring_layout(G_lf, k=0.5, iterations=50, seed=42)\n",
    "\n",
    "        # Normalize PageRank values for colormap\n",
    "        pr_values = pd.Series(pagerank_dict)\n",
    "        if not pr_values.empty:\n",
    "            pr_min, pr_max = pr_values.min(), pr_values.max()\n",
    "            norm = mcolors.Normalize(vmin=pr_min, vmax=pr_max)\n",
    "            cmap = cm.viridis\n",
    "            node_colors_lf = [cmap(norm(pagerank_dict[n])) for n in G_lf.nodes()]\n",
    "        else:\n",
    "            node_colors_lf = [\"silver\" for _ in G_lf.nodes()]\n",
    "\n",
    "        # Node sizes by out-degree (normalized between [50, 1000])\n",
    "        out_degrees_for_size = pd.Series({n: G_lf.out_degree(n, weight='weight') for n in G_lf.nodes()})\n",
    "        if not out_degrees_for_size.empty and out_degrees_for_size.max() > 0:\n",
    "            normalized = (\n",
    "                out_degrees_for_size.values - out_degrees_for_size.min()\n",
    "            ) / (out_degrees_for_size.max() - out_degrees_for_size.min())\n",
    "            node_sizes_lf = 50 + (normalized * 950)\n",
    "        else:\n",
    "            node_sizes_lf = [200] * G_lf.number_of_nodes()\n",
    "\n",
    "        # Edge widths by weight (normalized between [0.2, 3.2])\n",
    "        edge_weights_lf = [d['weight'] for _, _, d in G_lf.edges(data=True)]\n",
    "        if edge_weights_lf:\n",
    "            max_edge_w_lf = max(edge_weights_lf) or 1.0\n",
    "            edge_widths_lf_plot = [(w / max_edge_w_lf) * 3.0 + 0.2 for w in edge_weights_lf]\n",
    "        else:\n",
    "            edge_widths_lf_plot = [0.2 for _ in G_lf.edges()]\n",
    "\n",
    "        nx.draw_networkx_edges(\n",
    "            G_lf,\n",
    "            pos_lf,\n",
    "            width=edge_widths_lf_plot,\n",
    "            alpha=0.15,\n",
    "            edge_color=\"#888888\"\n",
    "        )\n",
    "        nx.draw_networkx_nodes(\n",
    "            G_lf,\n",
    "            pos_lf,\n",
    "            nodelist=list(G_lf.nodes()),\n",
    "            node_color=node_colors_lf,\n",
    "            node_size=node_sizes_lf,\n",
    "            alpha=0.85,\n",
    "            edgecolors='black',\n",
    "            linewidths=0.3\n",
    "        )\n",
    "\n",
    "        # Labels for top 5 highest PageRank values\n",
    "        labels_lf = {}\n",
    "        if pagerank_dict:\n",
    "            # Sort members by PageRank descending and take top 5\n",
    "            sorted_pr = sorted(pagerank_dict.items(), key=lambda kv: kv[1], reverse=True)\n",
    "            top5_nodes = [n for n, pr in sorted_pr[:5]]\n",
    "            labels_lf = {n: id2name.get(n, n) for n in top5_nodes}\n",
    "\n",
    "        if labels_lf:\n",
    "            nx.draw_networkx_labels(\n",
    "                G_lf,\n",
    "                pos_lf,\n",
    "                labels=labels_lf,\n",
    "                font_size=10,\n",
    "                font_weight='bold',\n",
    "                bbox=dict(facecolor='white', alpha=0.4, edgecolor='none', boxstyle='round,pad=0.1')\n",
    "            )\n",
    "\n",
    "        plt.title(\n",
    "            f\"Leader-Follower Network (Min Edge Count: {MIN_LF_COUNT_FOR_EDGE}, Lag: {MAX_LAG_DAYS} days)\\n\"\n",
    "            \"Node Color: PageRank Centrality | Node Size: Weighted Out-Degree\",\n",
    "            pad=20,\n",
    "            fontsize=16\n",
    "        )\n",
    "\n",
    "        # Create colorbar for PageRank (provide ax= to avoid ValueError)\n",
    "        if not pr_values.empty:\n",
    "            sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "            sm.set_array([])\n",
    "            ax = plt.gca()\n",
    "            cbar = plt.colorbar(sm, ax=ax, fraction=0.046, pad=0.04)\n",
    "            cbar.set_label(\"PageRank Value\", rotation=270, labelpad=15)\n",
    "\n",
    "        plt.axis(\"off\")\n",
    "        plt.tight_layout(rect=[0, 0, 0.85, 1])\n",
    "\n",
    "        lf_graph_path = os.path.join(\n",
    "            OUTPUT_DIR_LF,\n",
    "            f\"leader_follower_network_min{MIN_LF_COUNT_FOR_EDGE}_lag{MAX_LAG_DAYS}_pagerank_top5labels.png\"\n",
    "        )\n",
    "        plt.savefig(lf_graph_path, dpi=250, bbox_inches='tight')\n",
    "        plt.close()\n",
    "        print(f\"[Saved] Leader-follower network plot (PageRank-colored, top 5 labeled) to {lf_graph_path}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ef4a09b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 20 Leader-Follower Pairs (A's trade precedes B's trade in same ticker within 7 days):\n",
      "                leader_name      follower_name  count\n",
      "293         Josh Gottheimer   Gilbert Cisneros     86\n",
      "82           Kelly Loeffler       David Perdue     80\n",
      "72            Dean Phillips     Kelly Loeffler     46\n",
      "286               Ro Khanna       Peter Meijer     43\n",
      "310         Josh Gottheimer     Greg Gianforte     41\n",
      "100           Kurt Schrader    Josh Gottheimer     36\n",
      "296           Dean Phillips  Michael T. McCaul     36\n",
      "49              Pat Roberts    Josh Gottheimer     28\n",
      "420         Josh Gottheimer   Tommy Tuberville     26\n",
      "583            David Perdue     Kelly Loeffler     25\n",
      "41          Josh Gottheimer        Pat Roberts     24\n",
      "65              Zoe Lofgren        John Curtis     22\n",
      "273         Josh Gottheimer       Nancy Pelosi     22\n",
      "891         Josh Gottheimer     Mikie Sherrill     20\n",
      "521           Kurt Schrader          Ro Khanna     19\n",
      "343         Josh Gottheimer     Kelly Loeffler     18\n",
      "61          Josh Gottheimer        John Curtis     18\n",
      "159       David B. McKinley    Josh Gottheimer     18\n",
      "1184  Michael Patrick Guest       David Perdue     18\n",
      "412         Josh Gottheimer       Peter Meijer     18\n",
      "[Saved] All leader-follower pairs to leader_follower_analysis_louvain_spaced/leader_follower_all_pairs.csv\n",
      "\n",
      "Leader-Follower Graph: 69 members, 213 directed links (min count: 5).\n",
      "\n",
      "--- Louvain Community Details (Leader-Follower Network) ---\n",
      "\n",
      "Community ID: 0 (Size: 3)\n",
      "  - Billy Long (L000576) - Committee: House Energy and Commerce\n",
      "  - Mark E. Green (G000590) - Committee: House Oversight and Reform\n",
      "  - Virginia Foxx (F000450) - Committee: House Oversight and Reform\n",
      "\n",
      "Community ID: 1 (Size: 13)\n",
      "  - Alan S. Lowenthal (L000579) - Committee: Unspecified\n",
      "  - Blake Moore (M001213) - Committee: Unspecified\n",
      "  - Dean Phillips (P000616) - Committee: House Financial Services\n",
      "  - Debbie Dingell (D000624) - Committee: House Energy and Commerce\n",
      "  - Gilbert Cisneros (C001123) - Committee: Unspecified\n",
      "  - Katherine M. Clark (C001101) - Committee: House Appropriations\n",
      "  - Kathy Castor (C001066) - Committee: House Energy and Commerce\n",
      "  - Kelly Loeffler (S405) - Committee: Unspecified\n",
      "  - Michael T. McCaul (M001157) - Committee: Unspecified\n",
      "  - Ron Wyden (S247) - Committee: Senate Finance\n",
      "  - Susan W. Brooks (B001284) - Committee: House Energy and Commerce\n",
      "  - Susie Lee (L000590) - Committee: House Appropriations\n",
      "  - William R. Timmons (T000480) - Committee: House Financial Services\n",
      "\n",
      "Community ID: 2 (Size: 7)\n",
      "  - David Perdue (S379) - Committee: Unspecified\n",
      "  - Dwight Evans (E000296) - Committee: House Ways and Means\n",
      "  - Jerry Moran (S347) - Committee: Senate Banking, Housing, and Urban Affairs\n",
      "  - K. Michael Conaway (C001062) - Committee: Unspecified\n",
      "  - Michael Patrick Guest (G000591) - Committee: Unspecified\n",
      "  - Sheldon Whitehouse (S316) - Committee: Senate Finance\n",
      "  - Steve Cohen (C001068) - Committee: Unspecified\n",
      "\n",
      "Community ID: 3 (Size: 9)\n",
      "  - James R. Langevin (L000559) - Committee: Unspecified\n",
      "  - John Rutherford (R000609) - Committee: House Appropriations\n",
      "  - Kathy Manning (M001135) - Committee: Unspecified\n",
      "  - Marie Newman (N000192) - Committee: Unspecified\n",
      "  - Marjorie Taylor Greene (G000596) - Committee: Unspecified\n",
      "  - Peter Meijer (M001186) - Committee: Unspecified\n",
      "  - Richard W. Allen (A000372) - Committee: Unspecified\n",
      "  - Ro Khanna (K000389) - Committee: House Oversight and Reform\n",
      "  - Thomas Suozzi (S001201) - Committee: House Ways and Means\n",
      "\n",
      "Community ID: 4 (Size: 10)\n",
      "  - Brian Mast (M001199) - Committee: Unspecified\n",
      "  - David B. McKinley (M001180) - Committee: House Energy and Commerce\n",
      "  - David Trone (T000483) - Committee: House Appropriations\n",
      "  - Ed Perlmutter (P000593) - Committee: House Financial Services\n",
      "  - John A. Yarmuth (Y000062) - Committee: Unspecified\n",
      "  - John Curtis (C001114) - Committee: House Energy and Commerce\n",
      "  - Kim Schrier (S001216) - Committee: House Energy and Commerce\n",
      "  - Mike Garcia (G000061) - Committee: House Appropriations\n",
      "  - Pat Roberts (S260) - Committee: Unspecified\n",
      "  - Zoe Lofgren (L000397) - Committee: Unspecified\n",
      "\n",
      "Community ID: 5 (Size: 7)\n",
      "  - Cynthia Axne (A000378) - Committee: House Financial Services\n",
      "  - Dan Sullivan (S383) - Committee: Unspecified\n",
      "  - Doug Lamborn (L000564) - Committee: Unspecified\n",
      "  - Michael C. Burgess (B001248) - Committee: House Energy and Commerce\n",
      "  - Patrick Fallon (F000246) - Committee: House Oversight and Reform\n",
      "  - Thomas R. Carper (S277) - Committee: Senate Finance\n",
      "  - Tommy Tuberville (S412) - Committee: Senate Health, Education, Labor, and Pensions\n",
      "\n",
      "Community ID: 6 (Size: 20)\n",
      "  - Bill Cassidy (S373) - Committee: Senate Health, Education, Labor, and Pensions\n",
      "  - David Kustoff (K000392) - Committee: House Financial Services\n",
      "  - Earl Blumenauer (B000574) - Committee: House Ways and Means\n",
      "  - Francis Rooney (R000607) - Committee: Unspecified\n",
      "  - Greg Gianforte (G000584) - Committee: House Energy and Commerce\n",
      "  - John W. Hickenlooper (S408) - Committee: Senate Health, Education, Labor, and Pensions\n",
      "  - Joseph D. Morelle (M001206) - Committee: Unspecified\n",
      "  - Josh Gottheimer (G000583) - Committee: House Financial Services\n",
      "  - Kenny Marchant (M001158) - Committee: House Ways and Means\n",
      "  - Kevin Hern (H001082) - Committee: House Ways and Means\n",
      "  - Kurt Schrader (S001180) - Committee: House Energy and Commerce\n",
      "  - Lloyd Doggett (D000399) - Committee: House Ways and Means\n",
      "  - Lois Frankel (F000462) - Committee: House Appropriations\n",
      "  - Mikie Sherrill (S001207) - Committee: Unspecified\n",
      "  - Nancy Pelosi (P000197) - Committee: Unspecified\n",
      "  - Pete Sessions (S000250) - Committee: House Oversight and Reform\n",
      "  - Robert J. Wittman (W000804) - Committee: Unspecified\n",
      "  - Roger W. Marshall (M001198) - Committee: Senate Health, Education, Labor, and Pensions\n",
      "  - Shelley Moore Capito (S372) - Committee: Senate Appropriations\n",
      "  - Suzan K. DelBene (D000617) - Committee: House Ways and Means\n",
      "\n",
      "[Saved] Detailed Louvain communities to leader_follower_analysis_louvain_spaced/leader_follower_louvain_communities_detailed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jx/7k6544kn35n00wd0kcwf51300000gn/T/ipykernel_14504/2259721463.py:166: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "  louvain_palette = cm.get_cmap(louvain_cmap_name, max(2, num_communities_viz))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Saved] Spaced leader-follower network with Louvain communities to leader_follower_analysis_louvain_spaced/leader_follower_louvain_min5_lag7_spaced.png\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm # For colormaps\n",
    "import matplotlib.patches as mpatches # For legends\n",
    "import os\n",
    "import numpy as np\n",
    "import community as community_louvain # For Louvain: pip install python-louvain\n",
    "\n",
    "# --- ASSUMPTIONS ---\n",
    "# STOCK_TRANSACTIONS_DF: Loaded\n",
    "# id2name: Dictionary\n",
    "# COMMITTEE_MEMBERSHIP_MAP: Your main map (used to create member_to_committee)\n",
    "\n",
    "# --- Create member_to_committee from COMMITTEE_MEMBERSHIP_MAP ---\n",
    "if 'COMMITTEE_MEMBERSHIP_MAP' not in globals():\n",
    "    print(\"CRITICAL: COMMITTEE_MEMBERSHIP_MAP not defined.\")\n",
    "    exit()\n",
    "member_to_committee = {} \n",
    "for comm, period_map in COMMITTEE_MEMBERSHIP_MAP.items():\n",
    "    for members_set in period_map.values():\n",
    "        for m_id in members_set:\n",
    "            member_to_committee.setdefault(str(m_id), comm)\n",
    "\n",
    "# --- Ensure required DataFrames/mappings are available ---\n",
    "if 'STOCK_TRANSACTIONS_DF' not in globals() or \\\n",
    "   'id2name' not in globals() or \\\n",
    "   'member_to_committee' not in globals():\n",
    "    print(\"Error: STOCK_TRANSACTIONS_DF, id2name, or member_to_committee is not defined.\")\n",
    "    exit() \n",
    "\n",
    "OUTPUT_DIR_LF = \"leader_follower_analysis_louvain_spaced\" # New output dir\n",
    "os.makedirs(OUTPUT_DIR_LF, exist_ok=True)\n",
    "\n",
    "# --- Parameters ---\n",
    "MAX_LAG_DAYS = 7\n",
    "MIN_LF_COUNT_FOR_EDGE = 5 # Adjusted as per your last run for more edges\n",
    "TOP_N_PAIRS_TO_PRINT = 20\n",
    "TOP_N_CENTRAL_NODES_TO_PRINT = 10\n",
    "LABEL_PERCENTILE_THRESHOLD = 90 \n",
    "\n",
    "# --- Preprocessing --- (Same as your provided code)\n",
    "if not pd.api.types.is_datetime64_any_dtype(STOCK_TRANSACTIONS_DF['transaction_date_dt']):\n",
    "    STOCK_TRANSACTIONS_DF['transaction_date_dt'] = pd.to_datetime(STOCK_TRANSACTIONS_DF['transaction_date_dt'], errors='coerce')\n",
    "    STOCK_TRANSACTIONS_DF.dropna(subset=['transaction_date_dt'], inplace=True)\n",
    "STOCK_TRANSACTIONS_DF['member_id'] = STOCK_TRANSACTIONS_DF['member_id'].astype(str)\n",
    "STOCK_TRANSACTIONS_DF['ticker'] = STOCK_TRANSACTIONS_DF['ticker'].astype(str).str.upper()\n",
    "if 'type' in STOCK_TRANSACTIONS_DF.columns:\n",
    "    df_trades_lf = STOCK_TRANSACTIONS_DF[\n",
    "        STOCK_TRANSACTIONS_DF['type'].astype(str).str.contains('sale|purchase', case=False, na=False)\n",
    "    ].copy()\n",
    "else:\n",
    "    df_trades_lf = STOCK_TRANSACTIONS_DF.copy()\n",
    "df_trades_lf.sort_values(by=['ticker', 'transaction_date_dt'], inplace=True)\n",
    "leader_follower_pairs_raw = defaultdict(int)\n",
    "for ticker_symbol, ticker_group_df in df_trades_lf.groupby('ticker'):\n",
    "    if len(ticker_group_df) < 2: continue\n",
    "    trades_list = ticker_group_df.to_dict('records') \n",
    "    for i in range(len(trades_list)):\n",
    "        trade_A = trades_list[i]; member_A_id = trade_A['member_id']; date_A = trade_A['transaction_date_dt']\n",
    "        for j in range(i + 1, len(trades_list)):\n",
    "            trade_B = trades_list[j]; member_B_id = trade_B['member_id']; date_B = trade_B['transaction_date_dt']\n",
    "            if member_A_id == member_B_id: continue\n",
    "            time_lag = (date_B - date_A).days\n",
    "            if 0 < time_lag <= MAX_LAG_DAYS: leader_follower_pairs_raw[(member_A_id, member_B_id)] += 1\n",
    "            elif time_lag > MAX_LAG_DAYS: break \n",
    "\n",
    "if not leader_follower_pairs_raw:\n",
    "    print(\"No leader-follower patterns found with the specified lag.\")\n",
    "else:\n",
    "    df_leader_follower_with_ids = pd.DataFrame([\n",
    "        {\"leader_id\": k[0], \"follower_id\": k[1], \n",
    "         \"leader_name\": id2name.get(k[0], k[0]), \"follower_name\": id2name.get(k[1], k[1]),\n",
    "         \"count\": v}\n",
    "        for k, v in leader_follower_pairs_raw.items()\n",
    "    ]).sort_values(by=\"count\", ascending=False)\n",
    "    \n",
    "    print(f\"\\nTop {TOP_N_PAIRS_TO_PRINT} Leader-Follower Pairs (A's trade precedes B's trade in same ticker within {MAX_LAG_DAYS} days):\")\n",
    "    print(df_leader_follower_with_ids[['leader_name', 'follower_name', 'count']].head(TOP_N_PAIRS_TO_PRINT).to_string())\n",
    "    # (Save df_leader_follower_with_ids)\n",
    "    df_leader_follower_with_ids.to_csv(os.path.join(OUTPUT_DIR_LF, \"leader_follower_all_pairs.csv\"), index=False)\n",
    "    print(f\"[Saved] All leader-follower pairs to {os.path.join(OUTPUT_DIR_LF, 'leader_follower_all_pairs.csv')}\")\n",
    "\n",
    "\n",
    "    G_lf = nx.DiGraph()\n",
    "    all_members_in_graph_nodes = set()\n",
    "    for _, row in df_leader_follower_with_ids.iterrows():\n",
    "        if row['count'] >= MIN_LF_COUNT_FOR_EDGE:\n",
    "            G_lf.add_edge(row['leader_id'], row['follower_id'], weight=row['count'])\n",
    "            all_members_in_graph_nodes.add(row['leader_id']); all_members_in_graph_nodes.add(row['follower_id'])\n",
    "    for mid in all_members_in_graph_nodes:\n",
    "        if not G_lf.has_node(mid): G_lf.add_node(mid)\n",
    "\n",
    "    if G_lf.number_of_nodes() == 0:\n",
    "        print(\"Leader-follower graph is empty (no edges met the MIN_LF_COUNT_FOR_EDGE threshold).\")\n",
    "    else:\n",
    "        print(f\"\\nLeader-Follower Graph: {G_lf.number_of_nodes()} members, {G_lf.number_of_edges()} directed links (min count: {MIN_LF_COUNT_FOR_EDGE}).\")\n",
    "\n",
    "        # --- Centrality Analysis ---\n",
    "        out_degree_centrality_weighted = {n: G_lf.out_degree(n, weight='weight') for n in G_lf.nodes()}\n",
    "        # (Save centrality DFs - same as before)\n",
    "        # ...\n",
    "\n",
    "        # --- Louvain Community Detection ---\n",
    "        partition_lf = {} \n",
    "        if G_lf.number_of_edges() > 0:\n",
    "            try:\n",
    "                G_lf_undirected = G_lf.to_undirected() \n",
    "                partition_lf = community_louvain.best_partition(G_lf_undirected, weight='weight', random_state=42)\n",
    "                nx.set_node_attributes(G_lf, partition_lf, 'louvain_community_lf')\n",
    "                \n",
    "                # --- Print Community Details ---\n",
    "                print(f\"\\n--- Louvain Community Details (Leader-Follower Network) ---\")\n",
    "                communities_summary = defaultdict(list)\n",
    "                for member_id, comm_id in partition_lf.items():\n",
    "                    communities_summary[comm_id].append(member_id)\n",
    "                \n",
    "                df_louvain_list = []\n",
    "                for comm_id, members_in_comm in sorted(communities_summary.items()):\n",
    "                    print(f\"\\nCommunity ID: {comm_id} (Size: {len(members_in_comm)})\")\n",
    "                    member_details_list = []\n",
    "                    for member_id in sorted(members_in_comm, key=lambda m: id2name.get(m,m)): # Sort members by name within community\n",
    "                        member_name = id2name.get(member_id, member_id)\n",
    "                        committee = member_to_committee.get(member_id, \"Unspecified\")\n",
    "                        print(f\"  - {member_name} ({member_id}) - Committee: {committee}\")\n",
    "                        member_details_list.append(f\"{member_name} [{committee}]\")\n",
    "                        df_louvain_list.append({\n",
    "                            'member_id': member_id, \n",
    "                            'member_name': member_name, \n",
    "                            'committee': committee, \n",
    "                            'louvain_community_lf': comm_id\n",
    "                        })\n",
    "                \n",
    "                if df_louvain_list:\n",
    "                    df_louvain_output = pd.DataFrame(df_louvain_list).sort_values(by=['louvain_community_lf', 'member_name'])\n",
    "                    df_louvain_output.to_csv(os.path.join(OUTPUT_DIR_LF, \"leader_follower_louvain_communities_detailed.csv\"), index=False)\n",
    "                    print(f\"\\n[Saved] Detailed Louvain communities to {os.path.join(OUTPUT_DIR_LF, 'leader_follower_louvain_communities_detailed.csv')}\")\n",
    "\n",
    "            except Exception as e_louvain:\n",
    "                print(f\"Could not perform Louvain community detection: {e_louvain}\")\n",
    "                partition_lf = {n: 0 for n in G_lf.nodes()} \n",
    "        else:\n",
    "            print(\"Graph has no edges, skipping Louvain community detection.\")\n",
    "            partition_lf = {n: 0 for n in G_lf.nodes()}\n",
    "\n",
    "\n",
    "        # --- Standard Matplotlib Visualization (Nodes colored by Louvain Community) ---\n",
    "        fig, ax = plt.subplots(figsize=(24, 20)) # Increased figure size for more space\n",
    "\n",
    "        # Increase k for more spread, and more iterations\n",
    "        k_val = 1.0 / np.sqrt(G_lf.number_of_nodes()) if G_lf.number_of_nodes() > 0 else 1.0\n",
    "        k_val = max(k_val, 0.3) # Ensure k is not too small for very dense small graphs\n",
    "        pos_lf = nx.spring_layout(G_lf, k=k_val * 1.5, # Multiplier for k to increase spacing\n",
    "                                  iterations=100, # Increased iterations\n",
    "                                  seed=42, weight='weight')\n",
    "\n",
    "        node_ids_list_viz_louvain = list(G_lf.nodes())\n",
    "        \n",
    "        # Node colors by Louvain community\n",
    "        num_communities_viz = len(set(partition_lf.values()))\n",
    "        louvain_cmap_name = \"tab20\" if num_communities_viz > 10 else \"tab10\"\n",
    "        if num_communities_viz <=1 : louvain_cmap_name = \"Set1\"\n",
    "        if num_communities_viz > 20: louvain_cmap_name = \"turbo\" # Or other perceptually uniform like 'viridis'\n",
    "        \n",
    "        louvain_palette = cm.get_cmap(louvain_cmap_name, max(2, num_communities_viz))\n",
    "        node_colors_louvain = [louvain_palette(partition_lf.get(n, 0) % louvain_palette.N) for n in node_ids_list_viz_louvain]\n",
    "\n",
    "        # Node sizes by weighted out-degree\n",
    "        out_degrees_values_louvain = np.array([out_degree_centrality_weighted.get(n, 0) for n in node_ids_list_viz_louvain])\n",
    "        if len(out_degrees_values_louvain) > 0 and out_degrees_values_louvain.max() > 0:\n",
    "            scaled_degrees_viz = np.log1p(out_degrees_values_louvain)\n",
    "            node_sizes_louvain = 200 + (scaled_degrees_viz / (scaled_degrees_viz.max() if scaled_degrees_viz.max() > 0 else 1)) * 2500 \n",
    "        else:\n",
    "            node_sizes_louvain = [300] * len(node_ids_list_viz_louvain)\n",
    "\n",
    "        # Identify Nodes to Label (Influencers)\n",
    "        labels_to_draw_louvain = {}\n",
    "        nodes_to_label_ids_louvain = set()\n",
    "        if len(out_degrees_values_louvain) > 0 :\n",
    "            degree_threshold_labeling_louvain = np.percentile(out_degrees_values_louvain, LABEL_PERCENTILE_THRESHOLD)\n",
    "            for i, node_id_val_louvain in enumerate(node_ids_list_viz_louvain):\n",
    "                if out_degrees_values_louvain[i] >= degree_threshold_labeling_louvain and out_degrees_values_louvain[i] > 0.01: \n",
    "                    labels_to_draw_louvain[node_id_val_louvain] = id2name.get(node_id_val_louvain, node_id_val_louvain)\n",
    "                    nodes_to_label_ids_louvain.add(node_id_val_louvain)\n",
    "        \n",
    "        # Node border for influencers (red), others standard\n",
    "        node_edge_colors_louvain = ['red' if n in nodes_to_label_ids_louvain else 'dimgrey' for n in node_ids_list_viz_louvain]\n",
    "        node_linewidths_louvain = [2.0 if n in nodes_to_label_ids_louvain else 0.6 for n in node_ids_list_viz_louvain] # Thicker border for influencers\n",
    "\n",
    "        # Edge properties\n",
    "        edge_weights_louvain_list = [G_lf[u][v]['weight'] for u,v in G_lf.edges()]\n",
    "        edge_widths_louvain_plot = [0.3] * G_lf.number_of_edges() # Thinner default for less clutter\n",
    "        edge_alpha_louvain = 0.25 \n",
    "        if edge_weights_louvain_list:\n",
    "            max_edge_w_louvain = float(max(edge_weights_louvain_list)) if edge_weights_louvain_list else 1.0\n",
    "            if max_edge_w_louvain == 0: max_edge_w_louvain = 1.0\n",
    "            edge_widths_louvain_plot = [0.3 + (w / max_edge_w_louvain)**0.7 * 3.5 for w in edge_weights_louvain_list]\n",
    "\n",
    "        # --- Drawing ---\n",
    "        nx.draw_networkx_nodes(G_lf, pos_lf, ax=ax, nodelist=node_ids_list_viz_louvain, \n",
    "                               node_color=node_colors_louvain, \n",
    "                               node_size=node_sizes_louvain, alpha=0.9, \n",
    "                               edgecolors=node_edge_colors_louvain, \n",
    "                               linewidths=node_linewidths_louvain) \n",
    "        \n",
    "        nx.draw_networkx_edges(G_lf, pos_lf, ax=ax, width=edge_widths_louvain_plot, \n",
    "                               alpha=edge_alpha_louvain, edge_color=\"darkgrey\", arrows=True, # Slightly darker arrows\n",
    "                               arrowstyle='-|>', arrowsize=9, # Smaller arrows\n",
    "                               connectionstyle='arc3,rad=0.08', # Less curve\n",
    "                               node_size=node_sizes_louvain)\n",
    "        \n",
    "        # Draw labels with background\n",
    "        label_font_size_louvain = 7.0 # Even smaller for more labels if needed\n",
    "        for node_viz_louvain, (x_viz_l, y_viz_l) in pos_lf.items():\n",
    "            if node_viz_louvain in labels_to_draw_louvain:\n",
    "                ax.text(x_viz_l, y_viz_l, labels_to_draw_louvain[node_viz_louvain], \n",
    "                        fontsize=label_font_size_louvain, ha='center', va='center', # Centered on node\n",
    "                        fontweight='bold', # Make labeled names bold\n",
    "                        color='black', # Ensure label text is black for contrast on white bg\n",
    "                        bbox=dict(boxstyle='round,pad=0.25', fc='white', alpha=0.8, ec='red', lw=0.7)) # White bg, red border for influencer labels\n",
    "        \n",
    "        ax.set_title(f\"Leader-Follower Network & Louvain Communities\\n(Min. {MIN_LF_COUNT_FOR_EDGE} links/ticker | Lag ≤ {MAX_LAG_DAYS} days | Node Color: Louvain Community)\", \n",
    "                     fontsize=15, pad=15, weight='semibold')\n",
    "        \n",
    "        # Create legend\n",
    "        legend_handles_louvain = []\n",
    "        if labels_to_draw_louvain: \n",
    "            legend_handles_louvain.append(mpatches.Patch(edgecolor='red', facecolor='white', lw=1.5,\n",
    "                                                  label=f\"Top {100-LABEL_PERCENTILE_THRESHOLD}% Influencers (Red Border & Label BG)\"))\n",
    "        \n",
    "        # Add a note about community colors if not making a full legend for each color\n",
    "        unique_comm_ids_in_plot = sorted(list(set(partition_lf.get(n,0) for n in node_ids_list_viz_louvain)))\n",
    "        if len(unique_comm_ids_in_plot) > 1 : \n",
    "            # Generic community legend entry if too many, or create one for each if few\n",
    "            if len(unique_comm_ids_in_plot) <= 10: # Make legend for up to 10 distinct communities\n",
    "                 for comm_id_leg in unique_comm_ids_in_plot:\n",
    "                     legend_handles_louvain.append(mpatches.Patch(color=louvain_palette(comm_id_leg % louvain_palette.N), label=f\"Community {comm_id_leg}\"))\n",
    "            else:\n",
    "                 legend_handles_louvain.append(mpatches.Patch(color='grey', label=f\"Louvain Communities ({len(unique_comm_ids_in_plot)} groups)\"))\n",
    "\n",
    "\n",
    "        if legend_handles_louvain:\n",
    "            ax.legend(handles=legend_handles_louvain, \n",
    "                      title=\"Legend\", \n",
    "                      bbox_to_anchor=(1.01, 0.98), loc=\"upper left\", fontsize=8.5, title_fontsize=9.5,\n",
    "                      frameon=True, facecolor='#f7f7f7', framealpha=0.9, edgecolor='darkgrey')\n",
    "        \n",
    "        ax.set_axis_off() \n",
    "        fig.tight_layout(rect=[0, 0, 0.87, 0.96]) # Adjust rect for title and legend\n",
    "        \n",
    "        lf_graph_path_louvain = os.path.join(OUTPUT_DIR_LF, f\"leader_follower_louvain_min{MIN_LF_COUNT_FOR_EDGE}_lag{MAX_LAG_DAYS}_spaced.png\")\n",
    "        plt.savefig(lf_graph_path_louvain, dpi=200) \n",
    "        plt.close(fig)\n",
    "        print(f\"[Saved] Spaced leader-follower network with Louvain communities to {lf_graph_path_louvain}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
